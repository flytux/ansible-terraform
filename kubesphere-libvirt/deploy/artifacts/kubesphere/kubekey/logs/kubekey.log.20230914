00:03:44 UTC [GreetingsModule] Greetings
00:03:44 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
00:03:44 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
00:03:44 UTC success: [kubesphere-worker-1]
00:03:44 UTC success: [kubesphere-master-1]
00:03:44 UTC [NodePreCheckModule] A pre-check on nodes
00:03:45 UTC success: [kubesphere-master-1]
00:03:45 UTC success: [kubesphere-worker-1]
00:03:45 UTC [ConfirmModule] Display confirmation form
00:03:45 UTC success: [LocalHost]
00:03:45 UTC [NodeBinariesModule] Download installation binaries
00:03:45 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
00:03:45 UTC message: [localhost]
kubeadm is existed
00:03:45 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
00:03:45 UTC message: [localhost]
kubelet is existed
00:03:45 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
00:03:45 UTC message: [localhost]
kubectl is existed
00:03:45 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
00:03:45 UTC message: [localhost]
helm is existed
00:03:45 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
00:03:45 UTC message: [localhost]
kubecni is existed
00:03:45 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
00:03:45 UTC message: [localhost]
crictl is existed
00:03:45 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
00:03:45 UTC message: [localhost]
etcd is existed
00:03:45 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
00:03:47 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
00:03:47 UTC message: [localhost]
runc is existed
00:03:47 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
00:03:48 UTC message: [localhost]
calicoctl is existed
00:03:48 UTC success: [LocalHost]
00:03:48 UTC [ConfigureOSModule] Get OS release
00:03:48 UTC success: [kubesphere-master-1]
00:03:48 UTC success: [kubesphere-worker-1]
00:03:48 UTC [ConfigureOSModule] Prepare to init OS
00:03:48 UTC success: [kubesphere-worker-1]
00:03:48 UTC success: [kubesphere-master-1]
00:03:48 UTC [ConfigureOSModule] Generate init os script
00:03:48 UTC success: [kubesphere-master-1]
00:03:48 UTC success: [kubesphere-worker-1]
00:03:48 UTC [ConfigureOSModule] Exec init os script
00:03:49 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
00:03:49 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
00:03:49 UTC success: [kubesphere-master-1]
00:03:49 UTC success: [kubesphere-worker-1]
00:03:49 UTC [ConfigureOSModule] configure the ntp server for each node
00:03:49 UTC skipped: [kubesphere-worker-1]
00:03:49 UTC skipped: [kubesphere-master-1]
00:03:49 UTC [KubernetesStatusModule] Get kubernetes cluster status
00:03:49 UTC success: [kubesphere-master-1]
00:03:49 UTC [InstallContainerModule] Sync containerd binaries
00:03:49 UTC skipped: [kubesphere-master-1]
00:03:49 UTC skipped: [kubesphere-worker-1]
00:03:49 UTC [InstallContainerModule] Sync crictl binaries
00:03:50 UTC success: [kubesphere-worker-1]
00:03:50 UTC success: [kubesphere-master-1]
00:03:50 UTC [InstallContainerModule] Generate containerd service
00:03:50 UTC skipped: [kubesphere-master-1]
00:03:50 UTC skipped: [kubesphere-worker-1]
00:03:50 UTC [InstallContainerModule] Generate containerd config
00:03:50 UTC skipped: [kubesphere-worker-1]
00:03:50 UTC skipped: [kubesphere-master-1]
00:03:50 UTC [InstallContainerModule] Generate crictl config
00:03:50 UTC skipped: [kubesphere-worker-1]
00:03:50 UTC skipped: [kubesphere-master-1]
00:03:50 UTC [InstallContainerModule] Enable containerd
00:03:50 UTC skipped: [kubesphere-worker-1]
00:03:50 UTC skipped: [kubesphere-master-1]
00:03:50 UTC [PullModule] Start to pull images on all nodes
00:03:50 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
00:03:50 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
00:03:51 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
00:03:51 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
00:03:53 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
00:03:53 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
00:03:54 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
00:03:55 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
00:03:56 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
00:03:56 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
00:03:57 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
00:03:58 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
00:03:59 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
00:03:59 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
00:04:01 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
00:04:01 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
00:04:02 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
00:04:04 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
00:04:05 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
00:04:07 UTC success: [kubesphere-worker-1]
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [ETCDPreCheckModule] Get etcd status
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [CertsModule] Fetch etcd certs
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [CertsModule] Generate etcd Certs
00:04:07 UTC success: [LocalHost]
00:04:07 UTC [CertsModule] Synchronize certs file
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [CertsModule] Synchronize certs file to master
00:04:07 UTC skipped: [kubesphere-master-1]
00:04:07 UTC [InstallETCDBinaryModule] Install etcd using binary
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [InstallETCDBinaryModule] Generate etcd service
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [InstallETCDBinaryModule] Generate access address
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [ETCDConfigureModule] Health check on exist etcd
00:04:07 UTC skipped: [kubesphere-master-1]
00:04:07 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
00:04:07 UTC success: [kubesphere-master-1]
00:04:07 UTC [ETCDConfigureModule] Restart etcd
00:04:09 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDConfigureModule] Health check on all etcd
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDConfigureModule] Health check on all etcd
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDBackupModule] Backup etcd data regularly
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDBackupModule] Generate backup ETCD service
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDBackupModule] Generate backup ETCD timer
00:04:09 UTC success: [kubesphere-master-1]
00:04:09 UTC [ETCDBackupModule] Enable backup etcd service
00:04:10 UTC success: [kubesphere-master-1]
00:04:10 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
00:04:13 UTC success: [kubesphere-worker-1]
00:04:13 UTC success: [kubesphere-master-1]
00:04:13 UTC [InstallKubeBinariesModule] Change kubelet mode
00:04:13 UTC success: [kubesphere-master-1]
00:04:13 UTC success: [kubesphere-worker-1]
00:04:13 UTC [InstallKubeBinariesModule] Generate kubelet service
00:04:13 UTC success: [kubesphere-master-1]
00:04:13 UTC success: [kubesphere-worker-1]
00:04:13 UTC [InstallKubeBinariesModule] Enable kubelet service
00:04:14 UTC success: [kubesphere-master-1]
00:04:14 UTC success: [kubesphere-worker-1]
00:04:14 UTC [InstallKubeBinariesModule] Generate kubelet env
00:04:14 UTC success: [kubesphere-master-1]
00:04:14 UTC success: [kubesphere-worker-1]
00:04:14 UTC [InitKubernetesModule] Generate kubeadm config
00:04:14 UTC success: [kubesphere-master-1]
00:04:14 UTC [InitKubernetesModule] Generate audit policy
00:04:14 UTC skipped: [kubesphere-master-1]
00:04:14 UTC [InitKubernetesModule] Generate audit webhook
00:04:14 UTC skipped: [kubesphere-master-1]
00:04:14 UTC [InitKubernetesModule] Init cluster using kubeadm
00:04:28 UTC stdout: [kubesphere-master-1]
W0914 00:04:14.457803    3602 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.006311 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: ia8zox.3axcw9ibb638k7js
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token ia8zox.3axcw9ibb638k7js \
	--discovery-token-ca-cert-hash sha256:e9f20bae7fc9bba7533f748ff54dfa0b9ca8ee7cb0045c94de97ef739038fc98 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token ia8zox.3axcw9ibb638k7js \
	--discovery-token-ca-cert-hash sha256:e9f20bae7fc9bba7533f748ff54dfa0b9ca8ee7cb0045c94de97ef739038fc98
00:04:28 UTC success: [kubesphere-master-1]
00:04:28 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
00:04:28 UTC success: [kubesphere-master-1]
00:04:28 UTC [InitKubernetesModule] Remove master taint
00:04:29 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
00:04:29 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
00:04:29 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
00:04:29 UTC success: [kubesphere-master-1]
00:04:29 UTC [ClusterDNSModule] Generate coredns service
00:04:29 UTC success: [kubesphere-master-1]
00:04:29 UTC [ClusterDNSModule] Override coredns service
00:04:29 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
00:04:30 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [ClusterDNSModule] Generate nodelocaldns
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [ClusterDNSModule] Deploy nodelocaldns
00:04:30 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [ClusterDNSModule] Generate nodelocaldns configmap
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [ClusterDNSModule] Apply nodelocaldns configmap
00:04:30 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [KubernetesStatusModule] Get kubernetes cluster status
00:04:30 UTC stdout: [kubesphere-master-1]
v1.23.10
00:04:30 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
00:04:30 UTC stdout: [kubesphere-master-1]
W0914 00:04:30.826956    4266 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
9ee5bf0a14c6ef8a44114d6435dd27cb886f88fad2932231eeab4a3a13cc146d
00:04:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:04:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:04:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:04:30 UTC stdout: [kubesphere-master-1]
18wlk9.h8uoupccnb6bsp31
00:04:30 UTC success: [kubesphere-master-1]
00:04:30 UTC [JoinNodesModule] Generate kubeadm config
00:04:31 UTC skipped: [kubesphere-master-1]
00:04:31 UTC success: [kubesphere-worker-1]
00:04:31 UTC [JoinNodesModule] Generate audit policy
00:04:31 UTC skipped: [kubesphere-master-1]
00:04:31 UTC [JoinNodesModule] Generate audit webhook
00:04:31 UTC skipped: [kubesphere-master-1]
00:04:31 UTC [JoinNodesModule] Join control-plane node
00:04:31 UTC skipped: [kubesphere-master-1]
00:04:31 UTC [JoinNodesModule] Join worker node
00:04:49 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 00:04:43.821114    3142 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 00:04:43.824457    3142 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
00:04:49 UTC skipped: [kubesphere-master-1]
00:04:49 UTC success: [kubesphere-worker-1]
00:04:49 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
00:04:49 UTC skipped: [kubesphere-master-1]
00:04:49 UTC [JoinNodesModule] Remove master taint
00:04:49 UTC skipped: [kubesphere-master-1]
00:04:49 UTC [JoinNodesModule] Add worker label to all nodes
00:04:49 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
00:04:49 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
00:04:49 UTC success: [kubesphere-master-1]
00:04:49 UTC [DeployNetworkPluginModule] Generate calico
00:04:49 UTC success: [kubesphere-master-1]
00:04:49 UTC [DeployNetworkPluginModule] Deploy calico
00:04:50 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [ConfigureKubernetesModule] Configure kubernetes
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [ChownModule] Chown user $HOME/.kube dir
00:04:50 UTC success: [kubesphere-worker-1]
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [AutoRenewCertsModule] Generate k8s certs renew script
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [AutoRenewCertsModule] Generate k8s certs renew service
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [AutoRenewCertsModule] Enable k8s certs renew service
00:04:50 UTC success: [kubesphere-master-1]
00:04:50 UTC [SaveKubeConfigModule] Save kube config as a configmap
00:04:50 UTC success: [LocalHost]
00:04:50 UTC [AddonsModule] Install addons
00:04:50 UTC success: [LocalHost]
00:04:50 UTC Pipeline[CreateClusterPipeline] execute successfully
00:36:36 UTC [GreetingsModule] Greetings
00:36:36 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
00:36:37 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
00:36:37 UTC success: [kubesphere-worker-1]
00:36:37 UTC success: [kubesphere-master-1]
00:36:37 UTC [NodePreCheckModule] A pre-check on nodes
00:36:37 UTC success: [kubesphere-worker-1]
00:36:37 UTC success: [kubesphere-master-1]
00:36:37 UTC [ConfirmModule] Display confirmation form
00:36:37 UTC success: [LocalHost]
00:36:37 UTC [NodeBinariesModule] Download installation binaries
00:36:37 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
00:36:37 UTC message: [localhost]
kubeadm is existed
00:36:37 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
00:36:37 UTC message: [localhost]
kubelet is existed
00:36:37 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
00:36:37 UTC message: [localhost]
kubectl is existed
00:36:37 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
00:36:37 UTC message: [localhost]
helm is existed
00:36:37 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
00:36:37 UTC message: [localhost]
kubecni is existed
00:36:37 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
00:36:37 UTC message: [localhost]
crictl is existed
00:36:37 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
00:36:37 UTC message: [localhost]
etcd is existed
00:36:37 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
00:36:38 UTC message: [localhost]
containerd is existed
00:36:38 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
00:36:38 UTC message: [localhost]
runc is existed
00:36:38 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
00:36:38 UTC message: [localhost]
calicoctl is existed
00:36:38 UTC success: [LocalHost]
00:36:38 UTC [ConfigureOSModule] Get OS release
00:36:38 UTC success: [kubesphere-master-1]
00:36:38 UTC success: [kubesphere-worker-1]
00:36:38 UTC [ConfigureOSModule] Prepare to init OS
00:36:38 UTC success: [kubesphere-worker-1]
00:36:38 UTC success: [kubesphere-master-1]
00:36:38 UTC [ConfigureOSModule] Generate init os script
00:36:38 UTC success: [kubesphere-worker-1]
00:36:38 UTC success: [kubesphere-master-1]
00:36:38 UTC [ConfigureOSModule] Exec init os script
00:36:40 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
00:36:40 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
00:36:40 UTC success: [kubesphere-master-1]
00:36:40 UTC success: [kubesphere-worker-1]
00:36:40 UTC [ConfigureOSModule] configure the ntp server for each node
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC [KubernetesStatusModule] Get kubernetes cluster status
00:36:40 UTC success: [kubesphere-master-1]
00:36:40 UTC [InstallContainerModule] Sync containerd binaries
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC [InstallContainerModule] Sync crictl binaries
00:36:40 UTC success: [kubesphere-master-1]
00:36:40 UTC success: [kubesphere-worker-1]
00:36:40 UTC [InstallContainerModule] Generate containerd service
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC [InstallContainerModule] Generate containerd config
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC [InstallContainerModule] Generate crictl config
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC [InstallContainerModule] Enable containerd
00:36:40 UTC skipped: [kubesphere-worker-1]
00:36:40 UTC skipped: [kubesphere-master-1]
00:36:40 UTC [PullModule] Start to pull images on all nodes
00:36:40 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
00:36:40 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
00:36:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
00:36:42 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
00:36:43 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
00:36:43 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
00:36:45 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
00:36:45 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
00:36:46 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
00:36:46 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
00:36:48 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
00:36:48 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
00:36:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
00:36:49 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
00:36:51 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
00:36:51 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
00:36:52 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
00:36:54 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
00:36:55 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
00:36:57 UTC success: [kubesphere-worker-1]
00:36:57 UTC success: [kubesphere-master-1]
00:36:57 UTC [ETCDPreCheckModule] Get etcd status
00:36:57 UTC success: [kubesphere-master-1]
00:36:57 UTC [CertsModule] Fetch etcd certs
00:36:57 UTC success: [kubesphere-master-1]
00:36:57 UTC [CertsModule] Generate etcd Certs
00:36:57 UTC success: [LocalHost]
00:36:57 UTC [CertsModule] Synchronize certs file
00:36:57 UTC success: [kubesphere-master-1]
00:36:57 UTC [CertsModule] Synchronize certs file to master
00:36:57 UTC skipped: [kubesphere-master-1]
00:36:57 UTC [InstallETCDBinaryModule] Install etcd using binary
00:36:58 UTC success: [kubesphere-master-1]
00:36:58 UTC [InstallETCDBinaryModule] Generate etcd service
00:36:58 UTC success: [kubesphere-master-1]
00:36:58 UTC [InstallETCDBinaryModule] Generate access address
00:36:58 UTC success: [kubesphere-master-1]
00:36:58 UTC [ETCDConfigureModule] Health check on exist etcd
00:36:58 UTC skipped: [kubesphere-master-1]
00:36:58 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
00:36:58 UTC success: [kubesphere-master-1]
00:36:58 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
00:36:58 UTC success: [kubesphere-master-1]
00:36:58 UTC [ETCDConfigureModule] Restart etcd
00:37:02 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDConfigureModule] Health check on all etcd
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDConfigureModule] Health check on all etcd
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDBackupModule] Backup etcd data regularly
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDBackupModule] Generate backup ETCD service
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDBackupModule] Generate backup ETCD timer
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [ETCDBackupModule] Enable backup etcd service
00:37:02 UTC success: [kubesphere-master-1]
00:37:02 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
00:37:06 UTC success: [kubesphere-worker-1]
00:37:06 UTC success: [kubesphere-master-1]
00:37:06 UTC [InstallKubeBinariesModule] Change kubelet mode
00:37:06 UTC success: [kubesphere-master-1]
00:37:06 UTC success: [kubesphere-worker-1]
00:37:06 UTC [InstallKubeBinariesModule] Generate kubelet service
00:37:06 UTC success: [kubesphere-worker-1]
00:37:06 UTC success: [kubesphere-master-1]
00:37:06 UTC [InstallKubeBinariesModule] Enable kubelet service
00:37:07 UTC success: [kubesphere-master-1]
00:37:07 UTC success: [kubesphere-worker-1]
00:37:07 UTC [InstallKubeBinariesModule] Generate kubelet env
00:37:07 UTC success: [kubesphere-worker-1]
00:37:07 UTC success: [kubesphere-master-1]
00:37:07 UTC [InitKubernetesModule] Generate kubeadm config
00:37:07 UTC success: [kubesphere-master-1]
00:37:07 UTC [InitKubernetesModule] Generate audit policy
00:37:07 UTC skipped: [kubesphere-master-1]
00:37:07 UTC [InitKubernetesModule] Generate audit webhook
00:37:07 UTC skipped: [kubesphere-master-1]
00:37:07 UTC [InitKubernetesModule] Init cluster using kubeadm
00:37:21 UTC stdout: [kubesphere-master-1]
W0914 00:37:07.268901    3558 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.002652 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: tlv35f.j0orp3ui2zp3815x
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token tlv35f.j0orp3ui2zp3815x \
	--discovery-token-ca-cert-hash sha256:2d216d829d2b489a85215b1914105afb09f82652447372e603d581237dc0f787 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token tlv35f.j0orp3ui2zp3815x \
	--discovery-token-ca-cert-hash sha256:2d216d829d2b489a85215b1914105afb09f82652447372e603d581237dc0f787
00:37:21 UTC success: [kubesphere-master-1]
00:37:21 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
00:37:21 UTC success: [kubesphere-master-1]
00:37:21 UTC [InitKubernetesModule] Remove master taint
00:37:22 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
00:37:22 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
00:37:22 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
00:37:22 UTC success: [kubesphere-master-1]
00:37:22 UTC [ClusterDNSModule] Generate coredns service
00:37:22 UTC success: [kubesphere-master-1]
00:37:22 UTC [ClusterDNSModule] Override coredns service
00:37:22 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
00:37:23 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [ClusterDNSModule] Generate nodelocaldns
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [ClusterDNSModule] Deploy nodelocaldns
00:37:23 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [ClusterDNSModule] Generate nodelocaldns configmap
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [ClusterDNSModule] Apply nodelocaldns configmap
00:37:23 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [KubernetesStatusModule] Get kubernetes cluster status
00:37:23 UTC stdout: [kubesphere-master-1]
v1.23.10
00:37:23 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
00:37:23 UTC stdout: [kubesphere-master-1]
W0914 00:37:23.547936    4249 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
8f53a10739e9c0b58739e3439eb6138d65e9e77a4e372e1e275a9505677c4579
00:37:23 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:37:23 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:37:23 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
00:37:23 UTC stdout: [kubesphere-master-1]
nqfvfe.j1vy2326kmd9gafp
00:37:23 UTC success: [kubesphere-master-1]
00:37:23 UTC [JoinNodesModule] Generate kubeadm config
00:37:23 UTC skipped: [kubesphere-master-1]
00:37:23 UTC success: [kubesphere-worker-1]
00:37:23 UTC [JoinNodesModule] Generate audit policy
00:37:23 UTC skipped: [kubesphere-master-1]
00:37:23 UTC [JoinNodesModule] Generate audit webhook
00:37:23 UTC skipped: [kubesphere-master-1]
00:37:23 UTC [JoinNodesModule] Join control-plane node
00:37:23 UTC skipped: [kubesphere-master-1]
00:37:23 UTC [JoinNodesModule] Join worker node
00:37:48 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 00:37:42.562723    3124 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 00:37:42.565451    3124 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
00:37:48 UTC skipped: [kubesphere-master-1]
00:37:48 UTC success: [kubesphere-worker-1]
00:37:48 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
00:37:48 UTC skipped: [kubesphere-master-1]
00:37:48 UTC [JoinNodesModule] Remove master taint
00:37:48 UTC skipped: [kubesphere-master-1]
00:37:48 UTC [JoinNodesModule] Add worker label to all nodes
00:37:48 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
00:37:48 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
00:37:48 UTC success: [kubesphere-master-1]
00:37:48 UTC [DeployNetworkPluginModule] Generate calico
00:37:48 UTC success: [kubesphere-master-1]
00:37:48 UTC [DeployNetworkPluginModule] Deploy calico
00:37:49 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [ConfigureKubernetesModule] Configure kubernetes
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [ChownModule] Chown user $HOME/.kube dir
00:37:49 UTC success: [kubesphere-worker-1]
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [AutoRenewCertsModule] Generate k8s certs renew script
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [AutoRenewCertsModule] Generate k8s certs renew service
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [AutoRenewCertsModule] Enable k8s certs renew service
00:37:49 UTC success: [kubesphere-master-1]
00:37:49 UTC [SaveKubeConfigModule] Save kube config as a configmap
00:37:49 UTC success: [LocalHost]
00:37:49 UTC [AddonsModule] Install addons
00:37:49 UTC success: [LocalHost]
00:37:49 UTC Pipeline[CreateClusterPipeline] execute successfully
02:26:06 UTC [FATA] [kubesphere_worker_1] is in [worker] group, but not in hosts list
02:28:13 UTC [FATA] [kubesphere_worker_1] is in [worker] group, but not in hosts list
02:29:29 UTC [FATA] [kubesphere-worker-1] is in [worker] group, but not in hosts list
02:35:24 UTC [GreetingsModule] Greetings
02:35:24 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
02:35:25 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
02:35:25 UTC success: [kubesphere-worker-1]
02:35:25 UTC success: [kubesphere-master-1]
02:35:25 UTC [NodePreCheckModule] A pre-check on nodes
02:35:25 UTC success: [kubesphere-worker-1]
02:35:25 UTC success: [kubesphere-master-1]
02:35:25 UTC [ConfirmModule] Display confirmation form
02:35:25 UTC success: [LocalHost]
02:35:25 UTC [NodeBinariesModule] Download installation binaries
02:35:25 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
02:35:25 UTC message: [localhost]
kubeadm is existed
02:35:25 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
02:35:25 UTC message: [localhost]
kubelet is existed
02:35:25 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
02:35:25 UTC message: [localhost]
kubectl is existed
02:35:25 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
02:35:25 UTC message: [localhost]
helm is existed
02:35:25 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
02:35:25 UTC message: [localhost]
kubecni is existed
02:35:25 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
02:35:25 UTC message: [localhost]
crictl is existed
02:35:25 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
02:35:25 UTC message: [localhost]
etcd is existed
02:35:25 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
02:35:25 UTC message: [localhost]
containerd is existed
02:35:25 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
02:35:25 UTC message: [localhost]
runc is existed
02:35:25 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
02:35:26 UTC message: [localhost]
calicoctl is existed
02:35:26 UTC success: [LocalHost]
02:35:26 UTC [ConfigureOSModule] Get OS release
02:35:26 UTC success: [kubesphere-master-1]
02:35:26 UTC success: [kubesphere-worker-1]
02:35:26 UTC [ConfigureOSModule] Prepare to init OS
02:35:26 UTC success: [kubesphere-worker-1]
02:35:26 UTC success: [kubesphere-master-1]
02:35:26 UTC [ConfigureOSModule] Generate init os script
02:35:26 UTC success: [kubesphere-master-1]
02:35:26 UTC success: [kubesphere-worker-1]
02:35:26 UTC [ConfigureOSModule] Exec init os script
02:35:27 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:35:27 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:35:27 UTC success: [kubesphere-worker-1]
02:35:27 UTC success: [kubesphere-master-1]
02:35:27 UTC [ConfigureOSModule] configure the ntp server for each node
02:35:27 UTC skipped: [kubesphere-worker-1]
02:35:27 UTC skipped: [kubesphere-master-1]
02:35:27 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:35:27 UTC success: [kubesphere-master-1]
02:35:27 UTC [InstallContainerModule] Sync containerd binaries
02:35:27 UTC skipped: [kubesphere-master-1]
02:35:27 UTC skipped: [kubesphere-worker-1]
02:35:27 UTC [InstallContainerModule] Sync crictl binaries
02:35:28 UTC success: [kubesphere-master-1]
02:35:28 UTC success: [kubesphere-worker-1]
02:35:28 UTC [InstallContainerModule] Generate containerd service
02:35:28 UTC skipped: [kubesphere-master-1]
02:35:28 UTC skipped: [kubesphere-worker-1]
02:35:28 UTC [InstallContainerModule] Generate containerd config
02:35:28 UTC skipped: [kubesphere-worker-1]
02:35:28 UTC skipped: [kubesphere-master-1]
02:35:28 UTC [InstallContainerModule] Generate crictl config
02:35:28 UTC skipped: [kubesphere-worker-1]
02:35:28 UTC skipped: [kubesphere-master-1]
02:35:28 UTC [InstallContainerModule] Enable containerd
02:35:28 UTC skipped: [kubesphere-worker-1]
02:35:28 UTC skipped: [kubesphere-master-1]
02:35:28 UTC [PullModule] Start to pull images on all nodes
02:35:28 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
02:35:28 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
02:35:29 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:35:29 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:35:31 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
02:35:31 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:35:32 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:35:32 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:35:34 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
02:35:34 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:35:35 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
02:35:36 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
02:35:37 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
02:35:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:35:38 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:35:39 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
02:35:40 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
02:35:42 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
02:35:43 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:35:45 UTC success: [kubesphere-worker-1]
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [ETCDPreCheckModule] Get etcd status
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [CertsModule] Fetch etcd certs
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [CertsModule] Generate etcd Certs
02:35:45 UTC success: [LocalHost]
02:35:45 UTC [CertsModule] Synchronize certs file
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [CertsModule] Synchronize certs file to master
02:35:45 UTC skipped: [kubesphere-master-1]
02:35:45 UTC [InstallETCDBinaryModule] Install etcd using binary
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [InstallETCDBinaryModule] Generate etcd service
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [InstallETCDBinaryModule] Generate access address
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [ETCDConfigureModule] Health check on exist etcd
02:35:45 UTC skipped: [kubesphere-master-1]
02:35:45 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
02:35:45 UTC success: [kubesphere-master-1]
02:35:45 UTC [ETCDConfigureModule] Restart etcd
02:35:47 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:35:47 UTC success: [kubesphere-master-1]
02:35:47 UTC [ETCDConfigureModule] Health check on all etcd
02:35:47 UTC success: [kubesphere-master-1]
02:35:47 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
02:35:47 UTC success: [kubesphere-master-1]
02:35:47 UTC [ETCDConfigureModule] Health check on all etcd
02:35:47 UTC success: [kubesphere-master-1]
02:35:47 UTC [ETCDBackupModule] Backup etcd data regularly
02:35:48 UTC success: [kubesphere-master-1]
02:35:48 UTC [ETCDBackupModule] Generate backup ETCD service
02:35:48 UTC success: [kubesphere-master-1]
02:35:48 UTC [ETCDBackupModule] Generate backup ETCD timer
02:35:48 UTC success: [kubesphere-master-1]
02:35:48 UTC [ETCDBackupModule] Enable backup etcd service
02:35:48 UTC success: [kubesphere-master-1]
02:35:48 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC success: [kubesphere-worker-1]
02:35:52 UTC [InstallKubeBinariesModule] Change kubelet mode
02:35:52 UTC success: [kubesphere-worker-1]
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC [InstallKubeBinariesModule] Generate kubelet service
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC success: [kubesphere-worker-1]
02:35:52 UTC [InstallKubeBinariesModule] Enable kubelet service
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC success: [kubesphere-worker-1]
02:35:52 UTC [InstallKubeBinariesModule] Generate kubelet env
02:35:52 UTC success: [kubesphere-worker-1]
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC [InitKubernetesModule] Generate kubeadm config
02:35:52 UTC success: [kubesphere-master-1]
02:35:52 UTC [InitKubernetesModule] Generate audit policy
02:35:52 UTC skipped: [kubesphere-master-1]
02:35:52 UTC [InitKubernetesModule] Generate audit webhook
02:35:52 UTC skipped: [kubesphere-master-1]
02:35:52 UTC [InitKubernetesModule] Init cluster using kubeadm
02:36:06 UTC stdout: [kubesphere-master-1]
W0914 02:35:52.609239    3557 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.002654 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: xjraqg.0rgcannf1h3gr5k0
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token xjraqg.0rgcannf1h3gr5k0 \
	--discovery-token-ca-cert-hash sha256:1e76b69c79a2522b8a32cb7e755cbd14aa0ba15d6868dc635f674066d2b23e7e \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token xjraqg.0rgcannf1h3gr5k0 \
	--discovery-token-ca-cert-hash sha256:1e76b69c79a2522b8a32cb7e755cbd14aa0ba15d6868dc635f674066d2b23e7e
02:36:06 UTC success: [kubesphere-master-1]
02:36:06 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
02:36:06 UTC success: [kubesphere-master-1]
02:36:06 UTC [InitKubernetesModule] Remove master taint
02:36:06 UTC skipped: [kubesphere-master-1]
02:36:06 UTC [ClusterDNSModule] Generate coredns service
02:36:07 UTC success: [kubesphere-master-1]
02:36:07 UTC [ClusterDNSModule] Override coredns service
02:36:07 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
02:36:08 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [ClusterDNSModule] Generate nodelocaldns
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [ClusterDNSModule] Deploy nodelocaldns
02:36:08 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [ClusterDNSModule] Generate nodelocaldns configmap
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [ClusterDNSModule] Apply nodelocaldns configmap
02:36:08 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:36:08 UTC stdout: [kubesphere-master-1]
v1.23.10
02:36:08 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
02:36:08 UTC stdout: [kubesphere-master-1]
W0914 02:36:08.785284    4215 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
85b477579168547e70a92fb37c050686feb8860cb357ebf92558288a18b8accd
02:36:08 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:36:08 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:36:08 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:36:08 UTC stdout: [kubesphere-master-1]
ez4bxq.g8xc5gd94jwzt7fu
02:36:08 UTC success: [kubesphere-master-1]
02:36:08 UTC [JoinNodesModule] Generate kubeadm config
02:36:09 UTC skipped: [kubesphere-master-1]
02:36:09 UTC success: [kubesphere-worker-1]
02:36:09 UTC [JoinNodesModule] Generate audit policy
02:36:09 UTC skipped: [kubesphere-master-1]
02:36:09 UTC [JoinNodesModule] Generate audit webhook
02:36:09 UTC skipped: [kubesphere-master-1]
02:36:09 UTC [JoinNodesModule] Join control-plane node
02:36:09 UTC skipped: [kubesphere-master-1]
02:36:09 UTC [JoinNodesModule] Join worker node
02:36:27 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 02:36:21.785343    3145 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 02:36:21.789112    3145 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
02:36:27 UTC success: [kubesphere-worker-1]
02:36:27 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
02:36:27 UTC skipped: [kubesphere-master-1]
02:36:27 UTC [JoinNodesModule] Remove master taint
02:36:27 UTC skipped: [kubesphere-master-1]
02:36:27 UTC [JoinNodesModule] Add worker label to all nodes
02:36:28 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [DeployNetworkPluginModule] Generate calico
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [DeployNetworkPluginModule] Deploy calico
02:36:28 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [ConfigureKubernetesModule] Configure kubernetes
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [ChownModule] Chown user $HOME/.kube dir
02:36:28 UTC success: [kubesphere-worker-1]
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [AutoRenewCertsModule] Generate k8s certs renew script
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [AutoRenewCertsModule] Generate k8s certs renew service
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
02:36:28 UTC success: [kubesphere-master-1]
02:36:28 UTC [AutoRenewCertsModule] Enable k8s certs renew service
02:36:29 UTC success: [kubesphere-master-1]
02:36:29 UTC [SaveKubeConfigModule] Save kube config as a configmap
02:36:29 UTC success: [LocalHost]
02:36:29 UTC [AddonsModule] Install addons
02:36:29 UTC success: [LocalHost]
02:36:29 UTC Pipeline[CreateClusterPipeline] execute successfully
03:25:33 UTC [GreetingsModule] Greetings
03:25:34 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
03:25:34 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
03:25:34 UTC success: [kubesphere-worker-1]
03:25:34 UTC success: [kubesphere-master-1]
03:25:34 UTC [NodePreCheckModule] A pre-check on nodes
03:25:34 UTC success: [kubesphere-master-1]
03:25:34 UTC success: [kubesphere-worker-1]
03:25:34 UTC [ConfirmModule] Display confirmation form
03:25:34 UTC success: [LocalHost]
03:25:34 UTC [NodeBinariesModule] Download installation binaries
03:25:34 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
03:25:34 UTC message: [localhost]
kubeadm is existed
03:25:34 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
03:25:34 UTC message: [localhost]
kubelet is existed
03:25:34 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
03:25:34 UTC message: [localhost]
kubectl is existed
03:25:34 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
03:25:34 UTC message: [localhost]
helm is existed
03:25:34 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
03:25:35 UTC message: [localhost]
kubecni is existed
03:25:35 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
03:25:35 UTC message: [localhost]
crictl is existed
03:25:35 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
03:25:35 UTC message: [localhost]
etcd is existed
03:25:35 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
03:25:35 UTC message: [localhost]
containerd is existed
03:25:35 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
03:25:35 UTC message: [localhost]
runc is existed
03:25:35 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
03:25:35 UTC message: [localhost]
calicoctl is existed
03:25:35 UTC success: [LocalHost]
03:25:35 UTC [ConfigureOSModule] Get OS release
03:25:35 UTC success: [kubesphere-worker-1]
03:25:35 UTC success: [kubesphere-master-1]
03:25:35 UTC [ConfigureOSModule] Prepare to init OS
03:25:35 UTC success: [kubesphere-worker-1]
03:25:35 UTC success: [kubesphere-master-1]
03:25:35 UTC [ConfigureOSModule] Generate init os script
03:25:35 UTC success: [kubesphere-master-1]
03:25:35 UTC success: [kubesphere-worker-1]
03:25:35 UTC [ConfigureOSModule] Exec init os script
03:25:37 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:25:37 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:25:37 UTC success: [kubesphere-worker-1]
03:25:37 UTC success: [kubesphere-master-1]
03:25:37 UTC [ConfigureOSModule] configure the ntp server for each node
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:25:37 UTC success: [kubesphere-master-1]
03:25:37 UTC [InstallContainerModule] Sync containerd binaries
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC [InstallContainerModule] Sync crictl binaries
03:25:37 UTC success: [kubesphere-master-1]
03:25:37 UTC success: [kubesphere-worker-1]
03:25:37 UTC [InstallContainerModule] Generate containerd service
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC [InstallContainerModule] Generate containerd config
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC [InstallContainerModule] Generate crictl config
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC [InstallContainerModule] Enable containerd
03:25:37 UTC skipped: [kubesphere-master-1]
03:25:37 UTC skipped: [kubesphere-worker-1]
03:25:37 UTC [PullModule] Start to pull images on all nodes
03:25:37 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
03:25:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
03:25:39 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
03:25:39 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:25:40 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
03:25:40 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
03:25:42 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:25:42 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
03:25:43 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
03:25:44 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:25:45 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
03:25:45 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
03:25:46 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
03:25:47 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:25:48 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:25:48 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
03:25:50 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
03:25:51 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
03:25:53 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:25:54 UTC success: [kubesphere-worker-1]
03:25:54 UTC success: [kubesphere-master-1]
03:25:54 UTC [ETCDPreCheckModule] Get etcd status
03:25:54 UTC success: [kubesphere-master-1]
03:25:54 UTC [CertsModule] Fetch etcd certs
03:25:54 UTC success: [kubesphere-master-1]
03:25:54 UTC [CertsModule] Generate etcd Certs
03:25:54 UTC success: [LocalHost]
03:25:54 UTC [CertsModule] Synchronize certs file
03:25:54 UTC success: [kubesphere-master-1]
03:25:54 UTC [CertsModule] Synchronize certs file to master
03:25:54 UTC skipped: [kubesphere-master-1]
03:25:54 UTC [InstallETCDBinaryModule] Install etcd using binary
03:25:55 UTC success: [kubesphere-master-1]
03:25:55 UTC [InstallETCDBinaryModule] Generate etcd service
03:25:55 UTC success: [kubesphere-master-1]
03:25:55 UTC [InstallETCDBinaryModule] Generate access address
03:25:55 UTC success: [kubesphere-master-1]
03:25:55 UTC [ETCDConfigureModule] Health check on exist etcd
03:25:55 UTC skipped: [kubesphere-master-1]
03:25:55 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
03:25:55 UTC success: [kubesphere-master-1]
03:25:55 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
03:25:55 UTC success: [kubesphere-master-1]
03:25:55 UTC [ETCDConfigureModule] Restart etcd
03:25:58 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDConfigureModule] Health check on all etcd
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDConfigureModule] Health check on all etcd
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDBackupModule] Backup etcd data regularly
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDBackupModule] Generate backup ETCD service
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDBackupModule] Generate backup ETCD timer
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [ETCDBackupModule] Enable backup etcd service
03:25:58 UTC success: [kubesphere-master-1]
03:25:58 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
03:26:02 UTC success: [kubesphere-worker-1]
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC [InstallKubeBinariesModule] Change kubelet mode
03:26:02 UTC success: [kubesphere-worker-1]
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC [InstallKubeBinariesModule] Generate kubelet service
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC success: [kubesphere-worker-1]
03:26:02 UTC [InstallKubeBinariesModule] Enable kubelet service
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC success: [kubesphere-worker-1]
03:26:02 UTC [InstallKubeBinariesModule] Generate kubelet env
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC success: [kubesphere-worker-1]
03:26:02 UTC [InitKubernetesModule] Generate kubeadm config
03:26:02 UTC success: [kubesphere-master-1]
03:26:02 UTC [InitKubernetesModule] Generate audit policy
03:26:02 UTC skipped: [kubesphere-master-1]
03:26:02 UTC [InitKubernetesModule] Generate audit webhook
03:26:02 UTC skipped: [kubesphere-master-1]
03:26:02 UTC [InitKubernetesModule] Init cluster using kubeadm
03:26:17 UTC stdout: [kubesphere-master-1]
W0914 03:26:02.767998    3565 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.501982 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 51s28t.jp2t6jrq3ngbp4jq
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 51s28t.jp2t6jrq3ngbp4jq \
	--discovery-token-ca-cert-hash sha256:7a1cad12cc6609daa11d639f70a210ada5592b7c85f8260c752d713c4b3d9c19 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 51s28t.jp2t6jrq3ngbp4jq \
	--discovery-token-ca-cert-hash sha256:7a1cad12cc6609daa11d639f70a210ada5592b7c85f8260c752d713c4b3d9c19
03:26:17 UTC success: [kubesphere-master-1]
03:26:17 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
03:26:17 UTC success: [kubesphere-master-1]
03:26:17 UTC [InitKubernetesModule] Remove master taint
03:26:17 UTC skipped: [kubesphere-master-1]
03:26:17 UTC [ClusterDNSModule] Generate coredns service
03:26:18 UTC success: [kubesphere-master-1]
03:26:18 UTC [ClusterDNSModule] Override coredns service
03:26:18 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
03:26:19 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [ClusterDNSModule] Generate nodelocaldns
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [ClusterDNSModule] Deploy nodelocaldns
03:26:19 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [ClusterDNSModule] Generate nodelocaldns configmap
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [ClusterDNSModule] Apply nodelocaldns configmap
03:26:19 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:26:19 UTC stdout: [kubesphere-master-1]
v1.23.10
03:26:19 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
03:26:19 UTC stdout: [kubesphere-master-1]
W0914 03:26:19.491040    4225 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
1ef33a3f7a3601203d56e9219131d8d4c6c847741ed36045951be2e6d4bde3e1
03:26:19 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:26:19 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:26:19 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:26:19 UTC stdout: [kubesphere-master-1]
ubheo5.r1s810zgvb1dqtlp
03:26:19 UTC success: [kubesphere-master-1]
03:26:19 UTC [JoinNodesModule] Generate kubeadm config
03:26:19 UTC skipped: [kubesphere-master-1]
03:26:19 UTC success: [kubesphere-worker-1]
03:26:19 UTC [JoinNodesModule] Generate audit policy
03:26:19 UTC skipped: [kubesphere-master-1]
03:26:19 UTC [JoinNodesModule] Generate audit webhook
03:26:19 UTC skipped: [kubesphere-master-1]
03:26:19 UTC [JoinNodesModule] Join control-plane node
03:26:19 UTC skipped: [kubesphere-master-1]
03:26:19 UTC [JoinNodesModule] Join worker node
03:26:38 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 03:26:32.494197    3133 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 03:26:32.496970    3133 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
03:26:38 UTC success: [kubesphere-worker-1]
03:26:38 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
03:26:38 UTC skipped: [kubesphere-master-1]
03:26:38 UTC [JoinNodesModule] Remove master taint
03:26:38 UTC skipped: [kubesphere-master-1]
03:26:38 UTC [JoinNodesModule] Add worker label to all nodes
03:26:38 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
03:26:38 UTC success: [kubesphere-master-1]
03:26:38 UTC [DeployNetworkPluginModule] Generate calico
03:26:38 UTC success: [kubesphere-master-1]
03:26:38 UTC [DeployNetworkPluginModule] Deploy calico
03:26:39 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [ConfigureKubernetesModule] Configure kubernetes
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [ChownModule] Chown user $HOME/.kube dir
03:26:39 UTC success: [kubesphere-worker-1]
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [AutoRenewCertsModule] Generate k8s certs renew script
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [AutoRenewCertsModule] Generate k8s certs renew service
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [AutoRenewCertsModule] Enable k8s certs renew service
03:26:39 UTC success: [kubesphere-master-1]
03:26:39 UTC [SaveKubeConfigModule] Save kube config as a configmap
03:26:39 UTC success: [LocalHost]
03:26:39 UTC [AddonsModule] Install addons
03:26:39 UTC success: [LocalHost]
03:26:39 UTC Pipeline[CreateClusterPipeline] execute successfully
03:29:54 UTC [GreetingsModule] Greetings
03:29:54 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
03:29:55 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
03:29:55 UTC success: [kubesphere-worker-1]
03:29:55 UTC success: [kubesphere-master-1]
03:29:55 UTC [NodePreCheckModule] A pre-check on nodes
03:29:55 UTC success: [kubesphere-master-1]
03:29:55 UTC success: [kubesphere-worker-1]
03:29:55 UTC [ConfirmModule] Display confirmation form
03:29:55 UTC success: [LocalHost]
03:29:55 UTC [NodeBinariesModule] Download installation binaries
03:29:55 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
03:29:55 UTC message: [localhost]
kubeadm is existed
03:29:55 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
03:29:55 UTC message: [localhost]
kubelet is existed
03:29:55 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
03:29:55 UTC message: [localhost]
kubectl is existed
03:29:55 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
03:29:55 UTC message: [localhost]
helm is existed
03:29:55 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
03:29:55 UTC message: [localhost]
kubecni is existed
03:29:55 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
03:29:55 UTC message: [localhost]
crictl is existed
03:29:55 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
03:29:55 UTC message: [localhost]
etcd is existed
03:29:55 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
03:29:56 UTC message: [localhost]
containerd is existed
03:29:56 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
03:29:56 UTC message: [localhost]
runc is existed
03:29:56 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
03:29:56 UTC message: [localhost]
calicoctl is existed
03:29:56 UTC success: [LocalHost]
03:29:56 UTC [ConfigureOSModule] Get OS release
03:29:56 UTC success: [kubesphere-master-1]
03:29:56 UTC success: [kubesphere-worker-1]
03:29:56 UTC [ConfigureOSModule] Prepare to init OS
03:29:56 UTC success: [kubesphere-worker-1]
03:29:56 UTC success: [kubesphere-master-1]
03:29:56 UTC [ConfigureOSModule] Generate init os script
03:29:56 UTC success: [kubesphere-master-1]
03:29:56 UTC success: [kubesphere-worker-1]
03:29:56 UTC [ConfigureOSModule] Exec init os script
03:29:58 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:29:58 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:29:58 UTC success: [kubesphere-master-1]
03:29:58 UTC success: [kubesphere-worker-1]
03:29:58 UTC [ConfigureOSModule] configure the ntp server for each node
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:29:58 UTC success: [kubesphere-master-1]
03:29:58 UTC [InstallContainerModule] Sync containerd binaries
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC [InstallContainerModule] Sync crictl binaries
03:29:58 UTC success: [kubesphere-worker-1]
03:29:58 UTC success: [kubesphere-master-1]
03:29:58 UTC [InstallContainerModule] Generate containerd service
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC [InstallContainerModule] Generate containerd config
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC [InstallContainerModule] Generate crictl config
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC [InstallContainerModule] Enable containerd
03:29:58 UTC skipped: [kubesphere-worker-1]
03:29:58 UTC skipped: [kubesphere-master-1]
03:29:58 UTC [PullModule] Start to pull images on all nodes
03:29:58 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
03:29:58 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
03:29:59 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
03:29:59 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:30:01 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
03:30:01 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
03:30:03 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
03:30:03 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:30:04 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:30:05 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
03:30:06 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
03:30:06 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
03:30:07 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:30:08 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
03:30:09 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
03:30:09 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:30:10 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
03:30:12 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
03:30:14 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:30:15 UTC success: [kubesphere-worker-1]
03:30:15 UTC success: [kubesphere-master-1]
03:30:15 UTC [ETCDPreCheckModule] Get etcd status
03:30:15 UTC success: [kubesphere-master-1]
03:30:15 UTC [CertsModule] Fetch etcd certs
03:30:15 UTC success: [kubesphere-master-1]
03:30:15 UTC [CertsModule] Generate etcd Certs
03:30:15 UTC success: [LocalHost]
03:30:15 UTC [CertsModule] Synchronize certs file
03:30:15 UTC success: [kubesphere-master-1]
03:30:15 UTC [CertsModule] Synchronize certs file to master
03:30:15 UTC skipped: [kubesphere-master-1]
03:30:15 UTC [InstallETCDBinaryModule] Install etcd using binary
03:30:16 UTC success: [kubesphere-master-1]
03:30:16 UTC [InstallETCDBinaryModule] Generate etcd service
03:30:16 UTC success: [kubesphere-master-1]
03:30:16 UTC [InstallETCDBinaryModule] Generate access address
03:30:16 UTC success: [kubesphere-master-1]
03:30:16 UTC [ETCDConfigureModule] Health check on exist etcd
03:30:16 UTC skipped: [kubesphere-master-1]
03:30:16 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
03:30:16 UTC success: [kubesphere-master-1]
03:30:16 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
03:30:16 UTC success: [kubesphere-master-1]
03:30:16 UTC [ETCDConfigureModule] Restart etcd
03:30:20 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
03:30:20 UTC success: [kubesphere-master-1]
03:30:20 UTC [ETCDConfigureModule] Health check on all etcd
03:30:20 UTC success: [kubesphere-master-1]
03:30:20 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
03:30:20 UTC success: [kubesphere-master-1]
03:30:20 UTC [ETCDConfigureModule] Health check on all etcd
03:30:20 UTC success: [kubesphere-master-1]
03:30:20 UTC [ETCDBackupModule] Backup etcd data regularly
03:30:20 UTC success: [kubesphere-master-1]
03:30:20 UTC [ETCDBackupModule] Generate backup ETCD service
03:30:21 UTC success: [kubesphere-master-1]
03:30:21 UTC [ETCDBackupModule] Generate backup ETCD timer
03:30:21 UTC success: [kubesphere-master-1]
03:30:21 UTC [ETCDBackupModule] Enable backup etcd service
03:30:21 UTC success: [kubesphere-master-1]
03:30:21 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC success: [kubesphere-worker-1]
03:30:25 UTC [InstallKubeBinariesModule] Change kubelet mode
03:30:25 UTC success: [kubesphere-worker-1]
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC [InstallKubeBinariesModule] Generate kubelet service
03:30:25 UTC success: [kubesphere-worker-1]
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC [InstallKubeBinariesModule] Enable kubelet service
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC success: [kubesphere-worker-1]
03:30:25 UTC [InstallKubeBinariesModule] Generate kubelet env
03:30:25 UTC success: [kubesphere-worker-1]
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC [InitKubernetesModule] Generate kubeadm config
03:30:25 UTC success: [kubesphere-master-1]
03:30:25 UTC [InitKubernetesModule] Generate audit policy
03:30:25 UTC skipped: [kubesphere-master-1]
03:30:25 UTC [InitKubernetesModule] Generate audit webhook
03:30:25 UTC skipped: [kubesphere-master-1]
03:30:25 UTC [InitKubernetesModule] Init cluster using kubeadm
03:30:40 UTC stdout: [kubesphere-master-1]
W0914 03:30:25.630249    3643 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.001922 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: x4bw7f.fljlfkes0k8g8q8x
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token x4bw7f.fljlfkes0k8g8q8x \
	--discovery-token-ca-cert-hash sha256:def338b17288187e38252a036c3e8f924aaa160c941d6ada85ec5cb672658cca \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token x4bw7f.fljlfkes0k8g8q8x \
	--discovery-token-ca-cert-hash sha256:def338b17288187e38252a036c3e8f924aaa160c941d6ada85ec5cb672658cca
03:30:40 UTC success: [kubesphere-master-1]
03:30:40 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
03:30:40 UTC success: [kubesphere-master-1]
03:30:40 UTC [InitKubernetesModule] Remove master taint
03:30:40 UTC skipped: [kubesphere-master-1]
03:30:40 UTC [ClusterDNSModule] Generate coredns service
03:30:41 UTC success: [kubesphere-master-1]
03:30:41 UTC [ClusterDNSModule] Override coredns service
03:30:41 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
03:30:41 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
03:30:41 UTC success: [kubesphere-master-1]
03:30:41 UTC [ClusterDNSModule] Generate nodelocaldns
03:30:41 UTC success: [kubesphere-master-1]
03:30:41 UTC [ClusterDNSModule] Deploy nodelocaldns
03:30:42 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
03:30:42 UTC success: [kubesphere-master-1]
03:30:42 UTC [ClusterDNSModule] Generate nodelocaldns configmap
03:30:42 UTC success: [kubesphere-master-1]
03:30:42 UTC [ClusterDNSModule] Apply nodelocaldns configmap
03:30:42 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
03:30:42 UTC success: [kubesphere-master-1]
03:30:42 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:30:42 UTC stdout: [kubesphere-master-1]
v1.23.10
03:30:42 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
03:30:42 UTC stdout: [kubesphere-master-1]
W0914 03:30:42.329383    4329 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
0286283ad65d1c1a00832d4caef378ba9ccd713dd784bbc065736df80b540178
03:30:42 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:30:42 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:30:42 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:30:42 UTC stdout: [kubesphere-master-1]
7tgv3b.atz8kmms6xyf7u0m
03:30:42 UTC success: [kubesphere-master-1]
03:30:42 UTC [JoinNodesModule] Generate kubeadm config
03:30:42 UTC skipped: [kubesphere-master-1]
03:30:42 UTC success: [kubesphere-worker-1]
03:30:42 UTC [JoinNodesModule] Generate audit policy
03:30:42 UTC skipped: [kubesphere-master-1]
03:30:42 UTC [JoinNodesModule] Generate audit webhook
03:30:42 UTC skipped: [kubesphere-master-1]
03:30:42 UTC [JoinNodesModule] Join control-plane node
03:30:42 UTC skipped: [kubesphere-master-1]
03:30:42 UTC [JoinNodesModule] Join worker node
03:31:01 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 03:30:55.332668    3138 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 03:30:55.335992    3138 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
03:31:01 UTC success: [kubesphere-worker-1]
03:31:01 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
03:31:01 UTC skipped: [kubesphere-master-1]
03:31:01 UTC [JoinNodesModule] Remove master taint
03:31:01 UTC skipped: [kubesphere-master-1]
03:31:01 UTC [JoinNodesModule] Add worker label to all nodes
03:31:01 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [DeployNetworkPluginModule] Generate calico
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [DeployNetworkPluginModule] Deploy calico
03:31:01 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [ConfigureKubernetesModule] Configure kubernetes
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [ChownModule] Chown user $HOME/.kube dir
03:31:01 UTC success: [kubesphere-worker-1]
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [AutoRenewCertsModule] Generate k8s certs renew script
03:31:01 UTC success: [kubesphere-master-1]
03:31:01 UTC [AutoRenewCertsModule] Generate k8s certs renew service
03:31:02 UTC success: [kubesphere-master-1]
03:31:02 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
03:31:02 UTC success: [kubesphere-master-1]
03:31:02 UTC [AutoRenewCertsModule] Enable k8s certs renew service
03:31:02 UTC success: [kubesphere-master-1]
03:31:02 UTC [SaveKubeConfigModule] Save kube config as a configmap
03:31:02 UTC success: [LocalHost]
03:31:02 UTC [AddonsModule] Install addons
03:31:02 UTC success: [LocalHost]
03:31:02 UTC Pipeline[CreateClusterPipeline] execute successfully
03:49:39 UTC [GreetingsModule] Greetings
03:49:39 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
03:49:39 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
03:49:40 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
03:49:40 UTC success: [kubesphere-master-1]
03:49:40 UTC success: [kubesphere-worker-1]
03:49:40 UTC success: [kubesphere-master-2]
03:49:40 UTC [NodePreCheckModule] A pre-check on nodes
03:49:40 UTC success: [kubesphere-master-1]
03:49:40 UTC success: [kubesphere-worker-1]
03:49:40 UTC success: [kubesphere-master-2]
03:49:40 UTC [ConfirmModule] Display confirmation form
03:49:40 UTC success: [LocalHost]
03:49:40 UTC [NodeBinariesModule] Download installation binaries
03:49:40 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
03:49:40 UTC message: [localhost]
kubeadm is existed
03:49:40 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
03:49:40 UTC message: [localhost]
kubelet is existed
03:49:40 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
03:49:40 UTC message: [localhost]
kubectl is existed
03:49:40 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
03:49:40 UTC message: [localhost]
helm is existed
03:49:40 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
03:49:40 UTC message: [localhost]
kubecni is existed
03:49:40 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
03:49:40 UTC message: [localhost]
crictl is existed
03:49:40 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
03:49:40 UTC message: [localhost]
etcd is existed
03:49:40 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
03:49:40 UTC message: [localhost]
containerd is existed
03:49:40 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
03:49:40 UTC message: [localhost]
runc is existed
03:49:40 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
03:49:41 UTC message: [localhost]
calicoctl is existed
03:49:41 UTC success: [LocalHost]
03:49:41 UTC [ConfigureOSModule] Get OS release
03:49:41 UTC success: [kubesphere-master-2]
03:49:41 UTC success: [kubesphere-master-1]
03:49:41 UTC success: [kubesphere-worker-1]
03:49:41 UTC [ConfigureOSModule] Prepare to init OS
03:49:41 UTC success: [kubesphere-worker-1]
03:49:41 UTC success: [kubesphere-master-1]
03:49:41 UTC success: [kubesphere-master-2]
03:49:41 UTC [ConfigureOSModule] Generate init os script
03:49:41 UTC success: [kubesphere-worker-1]
03:49:41 UTC success: [kubesphere-master-1]
03:49:41 UTC success: [kubesphere-master-2]
03:49:41 UTC [ConfigureOSModule] Exec init os script
03:49:43 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:49:43 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:49:43 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
03:49:43 UTC success: [kubesphere-worker-1]
03:49:43 UTC success: [kubesphere-master-1]
03:49:43 UTC success: [kubesphere-master-2]
03:49:43 UTC [ConfigureOSModule] configure the ntp server for each node
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:49:43 UTC success: [kubesphere-master-1]
03:49:43 UTC success: [kubesphere-master-2]
03:49:43 UTC [InstallContainerModule] Sync containerd binaries
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC [InstallContainerModule] Sync crictl binaries
03:49:43 UTC success: [kubesphere-master-2]
03:49:43 UTC success: [kubesphere-master-1]
03:49:43 UTC success: [kubesphere-worker-1]
03:49:43 UTC [InstallContainerModule] Generate containerd service
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC [InstallContainerModule] Generate containerd config
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC [InstallContainerModule] Generate crictl config
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC [InstallContainerModule] Enable containerd
03:49:43 UTC skipped: [kubesphere-worker-1]
03:49:43 UTC skipped: [kubesphere-master-1]
03:49:43 UTC skipped: [kubesphere-master-2]
03:49:43 UTC [PullModule] Start to pull images on all nodes
03:49:43 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
03:49:43 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
03:49:43 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
03:49:45 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
03:49:45 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:49:45 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
03:49:46 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
03:49:46 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
03:49:47 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
03:49:48 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
03:49:48 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:49:48 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
03:49:49 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
03:49:50 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
03:49:50 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
03:49:51 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
03:49:51 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
03:49:51 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
03:49:53 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:49:53 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
03:49:53 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
03:49:54 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
03:49:54 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
03:49:54 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:49:56 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
03:49:56 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
03:49:57 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
03:49:57 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
03:49:58 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:49:59 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
03:50:00 UTC success: [kubesphere-worker-1]
03:50:00 UTC success: [kubesphere-master-2]
03:50:00 UTC success: [kubesphere-master-1]
03:50:00 UTC [ETCDPreCheckModule] Get etcd status
03:50:00 UTC success: [kubesphere-master-1]
03:50:00 UTC success: [kubesphere-master-2]
03:50:00 UTC [CertsModule] Fetch etcd certs
03:50:00 UTC success: [kubesphere-master-1]
03:50:00 UTC skipped: [kubesphere-master-2]
03:50:00 UTC [CertsModule] Generate etcd Certs
03:50:00 UTC success: [LocalHost]
03:50:00 UTC [CertsModule] Synchronize certs file
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC success: [kubesphere-master-2]
03:50:01 UTC [CertsModule] Synchronize certs file to master
03:50:01 UTC skipped: [kubesphere-master-2]
03:50:01 UTC skipped: [kubesphere-master-1]
03:50:01 UTC [InstallETCDBinaryModule] Install etcd using binary
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC success: [kubesphere-master-2]
03:50:01 UTC [InstallETCDBinaryModule] Generate etcd service
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC success: [kubesphere-master-2]
03:50:01 UTC [InstallETCDBinaryModule] Generate access address
03:50:01 UTC skipped: [kubesphere-master-2]
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC [ETCDConfigureModule] Health check on exist etcd
03:50:01 UTC skipped: [kubesphere-master-2]
03:50:01 UTC skipped: [kubesphere-master-1]
03:50:01 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC success: [kubesphere-master-2]
03:50:01 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
03:50:01 UTC success: [kubesphere-master-1]
03:50:01 UTC success: [kubesphere-master-2]
03:50:01 UTC [ETCDConfigureModule] Restart etcd
03:50:08 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
03:50:08 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC [ETCDConfigureModule] Health check on all etcd
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC [ETCDConfigureModule] Health check on all etcd
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC [ETCDBackupModule] Backup etcd data regularly
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC [ETCDBackupModule] Generate backup ETCD service
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC [ETCDBackupModule] Generate backup ETCD timer
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC [ETCDBackupModule] Enable backup etcd service
03:50:08 UTC success: [kubesphere-master-2]
03:50:08 UTC success: [kubesphere-master-1]
03:50:08 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
03:50:13 UTC success: [kubesphere-worker-1]
03:50:13 UTC success: [kubesphere-master-2]
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC [InstallKubeBinariesModule] Change kubelet mode
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC success: [kubesphere-worker-1]
03:50:13 UTC success: [kubesphere-master-2]
03:50:13 UTC [InstallKubeBinariesModule] Generate kubelet service
03:50:13 UTC success: [kubesphere-master-2]
03:50:13 UTC success: [kubesphere-worker-1]
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC [InstallKubeBinariesModule] Enable kubelet service
03:50:13 UTC success: [kubesphere-master-2]
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC success: [kubesphere-worker-1]
03:50:13 UTC [InstallKubeBinariesModule] Generate kubelet env
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC success: [kubesphere-worker-1]
03:50:13 UTC success: [kubesphere-master-2]
03:50:13 UTC [InitKubernetesModule] Generate kubeadm config
03:50:13 UTC skipped: [kubesphere-master-2]
03:50:13 UTC success: [kubesphere-master-1]
03:50:13 UTC [InitKubernetesModule] Generate audit policy
03:50:13 UTC skipped: [kubesphere-master-2]
03:50:13 UTC skipped: [kubesphere-master-1]
03:50:13 UTC [InitKubernetesModule] Generate audit webhook
03:50:13 UTC skipped: [kubesphere-master-2]
03:50:13 UTC skipped: [kubesphere-master-1]
03:50:13 UTC [InitKubernetesModule] Init cluster using kubeadm
03:50:28 UTC stdout: [kubesphere-master-1]
W0914 03:50:13.579062    3693 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.010175 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: a472lh.mqrzkmdyr5jo7rbi
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token a472lh.mqrzkmdyr5jo7rbi \
	--discovery-token-ca-cert-hash sha256:61bef027ee0ba1a395e2aa17d1eaebc9e09d257dfc09dc9a6d34523b82b42827 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token a472lh.mqrzkmdyr5jo7rbi \
	--discovery-token-ca-cert-hash sha256:61bef027ee0ba1a395e2aa17d1eaebc9e09d257dfc09dc9a6d34523b82b42827
03:50:28 UTC skipped: [kubesphere-master-2]
03:50:28 UTC success: [kubesphere-master-1]
03:50:28 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
03:50:28 UTC skipped: [kubesphere-master-2]
03:50:28 UTC success: [kubesphere-master-1]
03:50:28 UTC [InitKubernetesModule] Remove master taint
03:50:28 UTC skipped: [kubesphere-master-1]
03:50:28 UTC skipped: [kubesphere-master-2]
03:50:28 UTC [ClusterDNSModule] Generate coredns service
03:50:29 UTC skipped: [kubesphere-master-2]
03:50:29 UTC success: [kubesphere-master-1]
03:50:29 UTC [ClusterDNSModule] Override coredns service
03:50:29 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
03:50:29 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
03:50:29 UTC skipped: [kubesphere-master-2]
03:50:29 UTC success: [kubesphere-master-1]
03:50:29 UTC [ClusterDNSModule] Generate nodelocaldns
03:50:29 UTC skipped: [kubesphere-master-2]
03:50:29 UTC success: [kubesphere-master-1]
03:50:29 UTC [ClusterDNSModule] Deploy nodelocaldns
03:50:30 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
03:50:30 UTC skipped: [kubesphere-master-2]
03:50:30 UTC success: [kubesphere-master-1]
03:50:30 UTC [ClusterDNSModule] Generate nodelocaldns configmap
03:50:30 UTC skipped: [kubesphere-master-2]
03:50:30 UTC success: [kubesphere-master-1]
03:50:30 UTC [ClusterDNSModule] Apply nodelocaldns configmap
03:50:30 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
03:50:30 UTC skipped: [kubesphere-master-2]
03:50:30 UTC success: [kubesphere-master-1]
03:50:30 UTC [KubernetesStatusModule] Get kubernetes cluster status
03:50:30 UTC stdout: [kubesphere-master-1]
v1.23.10
03:50:30 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
03:50:30 UTC stdout: [kubesphere-master-1]
W0914 03:50:30.433390    4361 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
829536b6117aba69be1783b4616cc6e9fbf3165875da9c51bfda4ce4e6aaeddf
03:50:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:50:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:50:30 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
03:50:30 UTC stdout: [kubesphere-master-1]
xxafvf.4hp6c28adzua4o50
03:50:30 UTC success: [kubesphere-master-1]
03:50:30 UTC success: [kubesphere-master-2]
03:50:30 UTC [JoinNodesModule] Generate kubeadm config
03:50:30 UTC skipped: [kubesphere-master-1]
03:50:30 UTC success: [kubesphere-master-2]
03:50:30 UTC success: [kubesphere-worker-1]
03:50:30 UTC [JoinNodesModule] Generate audit policy
03:50:30 UTC skipped: [kubesphere-master-2]
03:50:30 UTC skipped: [kubesphere-master-1]
03:50:30 UTC [JoinNodesModule] Generate audit webhook
03:50:30 UTC skipped: [kubesphere-master-2]
03:50:30 UTC skipped: [kubesphere-master-1]
03:50:30 UTC [JoinNodesModule] Join control-plane node
03:50:50 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 03:50:43.581057    3720 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 03:50:43.584707    3720 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
03:50:50 UTC skipped: [kubesphere-master-1]
03:50:50 UTC success: [kubesphere-master-2]
03:50:50 UTC [JoinNodesModule] Join worker node
03:50:57 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 03:50:51.355962    3143 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 03:50:51.358567    3143 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
03:50:57 UTC skipped: [kubesphere-master-2]
03:50:57 UTC success: [kubesphere-worker-1]
03:50:57 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
03:50:57 UTC skipped: [kubesphere-master-1]
03:50:57 UTC success: [kubesphere-master-2]
03:50:57 UTC [JoinNodesModule] Remove master taint
03:50:58 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
03:50:58 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
03:50:58 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
03:50:58 UTC skipped: [kubesphere-master-1]
03:50:58 UTC success: [kubesphere-master-2]
03:50:58 UTC [JoinNodesModule] Add worker label to all nodes
03:50:58 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
03:50:58 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
03:50:58 UTC success: [kubesphere-master-1]
03:50:58 UTC skipped: [kubesphere-master-2]
03:50:58 UTC [DeployNetworkPluginModule] Generate calico
03:50:58 UTC skipped: [kubesphere-master-2]
03:50:58 UTC success: [kubesphere-master-1]
03:50:58 UTC [DeployNetworkPluginModule] Deploy calico
03:51:00 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
03:51:00 UTC skipped: [kubesphere-master-2]
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC [ConfigureKubernetesModule] Configure kubernetes
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC skipped: [kubesphere-master-2]
03:51:00 UTC [ChownModule] Chown user $HOME/.kube dir
03:51:00 UTC success: [kubesphere-worker-1]
03:51:00 UTC success: [kubesphere-master-2]
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC [AutoRenewCertsModule] Generate k8s certs renew script
03:51:00 UTC success: [kubesphere-master-2]
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC [AutoRenewCertsModule] Generate k8s certs renew service
03:51:00 UTC success: [kubesphere-master-2]
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
03:51:00 UTC success: [kubesphere-master-2]
03:51:00 UTC success: [kubesphere-master-1]
03:51:00 UTC [AutoRenewCertsModule] Enable k8s certs renew service
03:51:01 UTC success: [kubesphere-master-2]
03:51:01 UTC success: [kubesphere-master-1]
03:51:01 UTC [SaveKubeConfigModule] Save kube config as a configmap
03:51:01 UTC success: [LocalHost]
03:51:01 UTC [AddonsModule] Install addons
03:51:01 UTC success: [LocalHost]
03:51:01 UTC Pipeline[CreateClusterPipeline] execute successfully
04:01:23 UTC [GreetingsModule] Greetings
04:01:23 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:01:24 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:01:24 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
04:01:24 UTC success: [kubesphere-master-1]
04:01:24 UTC success: [kubesphere-worker-1]
04:01:24 UTC success: [kubesphere-master-2]
04:01:24 UTC [NodePreCheckModule] A pre-check on nodes
04:01:24 UTC success: [kubesphere-master-2]
04:01:24 UTC success: [kubesphere-master-1]
04:01:24 UTC success: [kubesphere-worker-1]
04:01:24 UTC [ConfirmModule] Display confirmation form
04:01:24 UTC success: [LocalHost]
04:01:24 UTC [NodeBinariesModule] Download installation binaries
04:01:24 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
04:01:24 UTC message: [localhost]
kubeadm is existed
04:01:24 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
04:01:24 UTC message: [localhost]
kubelet is existed
04:01:24 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
04:01:25 UTC message: [localhost]
kubectl is existed
04:01:25 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
04:01:25 UTC message: [localhost]
helm is existed
04:01:25 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
04:01:25 UTC message: [localhost]
kubecni is existed
04:01:25 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
04:01:25 UTC message: [localhost]
crictl is existed
04:01:25 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
04:01:25 UTC message: [localhost]
etcd is existed
04:01:25 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
04:01:25 UTC message: [localhost]
containerd is existed
04:01:25 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
04:01:25 UTC message: [localhost]
runc is existed
04:01:25 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
04:01:25 UTC message: [localhost]
calicoctl is existed
04:01:25 UTC success: [LocalHost]
04:01:25 UTC [ConfigureOSModule] Get OS release
04:01:25 UTC success: [kubesphere-master-2]
04:01:25 UTC success: [kubesphere-worker-1]
04:01:25 UTC success: [kubesphere-master-1]
04:01:25 UTC [ConfigureOSModule] Prepare to init OS
04:01:26 UTC success: [kubesphere-worker-1]
04:01:26 UTC success: [kubesphere-master-1]
04:01:26 UTC success: [kubesphere-master-2]
04:01:26 UTC [ConfigureOSModule] Generate init os script
04:01:26 UTC success: [kubesphere-worker-1]
04:01:26 UTC success: [kubesphere-master-2]
04:01:26 UTC success: [kubesphere-master-1]
04:01:26 UTC [ConfigureOSModule] Exec init os script
04:01:27 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:01:27 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:01:27 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:01:27 UTC success: [kubesphere-master-1]
04:01:27 UTC success: [kubesphere-master-2]
04:01:27 UTC success: [kubesphere-worker-1]
04:01:27 UTC [ConfigureOSModule] configure the ntp server for each node
04:01:27 UTC skipped: [kubesphere-worker-1]
04:01:27 UTC skipped: [kubesphere-master-1]
04:01:27 UTC skipped: [kubesphere-master-2]
04:01:27 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:01:27 UTC success: [kubesphere-master-1]
04:01:27 UTC [InstallContainerModule] Sync containerd binaries
04:01:27 UTC skipped: [kubesphere-master-1]
04:01:27 UTC skipped: [kubesphere-master-2]
04:01:27 UTC skipped: [kubesphere-worker-1]
04:01:27 UTC [InstallContainerModule] Sync crictl binaries
04:01:28 UTC success: [kubesphere-master-1]
04:01:28 UTC success: [kubesphere-worker-1]
04:01:28 UTC success: [kubesphere-master-2]
04:01:28 UTC [InstallContainerModule] Generate containerd service
04:01:28 UTC skipped: [kubesphere-master-2]
04:01:28 UTC skipped: [kubesphere-master-1]
04:01:28 UTC skipped: [kubesphere-worker-1]
04:01:28 UTC [InstallContainerModule] Generate containerd config
04:01:28 UTC skipped: [kubesphere-worker-1]
04:01:28 UTC skipped: [kubesphere-master-2]
04:01:28 UTC skipped: [kubesphere-master-1]
04:01:28 UTC [InstallContainerModule] Generate crictl config
04:01:28 UTC skipped: [kubesphere-master-2]
04:01:28 UTC skipped: [kubesphere-master-1]
04:01:28 UTC skipped: [kubesphere-worker-1]
04:01:28 UTC [InstallContainerModule] Enable containerd
04:01:28 UTC skipped: [kubesphere-worker-1]
04:01:28 UTC skipped: [kubesphere-master-2]
04:01:28 UTC skipped: [kubesphere-master-1]
04:01:28 UTC [PullModule] Start to pull images on all nodes
04:01:28 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
04:01:28 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
04:01:28 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
04:01:29 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
04:01:29 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:01:29 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:01:31 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
04:01:31 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
04:01:31 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:01:32 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:01:32 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:01:32 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:01:34 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
04:01:34 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
04:01:34 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:01:35 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
04:01:35 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
04:01:36 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
04:01:37 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
04:01:37 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
04:01:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:01:39 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:01:39 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:01:39 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
04:01:40 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
04:01:42 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
04:01:43 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:01:45 UTC success: [kubesphere-master-2]
04:01:45 UTC success: [kubesphere-worker-1]
04:01:45 UTC success: [kubesphere-master-1]
04:01:45 UTC [ETCDPreCheckModule] Get etcd status
04:01:45 UTC success: [kubesphere-master-1]
04:01:45 UTC success: [kubesphere-master-2]
04:01:45 UTC [CertsModule] Fetch etcd certs
04:01:45 UTC success: [kubesphere-master-1]
04:01:45 UTC skipped: [kubesphere-master-2]
04:01:45 UTC [CertsModule] Generate etcd Certs
04:01:45 UTC success: [LocalHost]
04:01:45 UTC [CertsModule] Synchronize certs file
04:01:45 UTC success: [kubesphere-master-1]
04:01:45 UTC success: [kubesphere-master-2]
04:01:45 UTC [CertsModule] Synchronize certs file to master
04:01:45 UTC skipped: [kubesphere-master-1]
04:01:45 UTC [InstallETCDBinaryModule] Install etcd using binary
04:01:46 UTC success: [kubesphere-master-1]
04:01:46 UTC success: [kubesphere-master-2]
04:01:46 UTC [InstallETCDBinaryModule] Generate etcd service
04:01:46 UTC success: [kubesphere-master-1]
04:01:46 UTC success: [kubesphere-master-2]
04:01:46 UTC [InstallETCDBinaryModule] Generate access address
04:01:46 UTC skipped: [kubesphere-master-2]
04:01:46 UTC success: [kubesphere-master-1]
04:01:46 UTC [ETCDConfigureModule] Health check on exist etcd
04:01:46 UTC skipped: [kubesphere-master-2]
04:01:46 UTC skipped: [kubesphere-master-1]
04:01:46 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
04:01:46 UTC success: [kubesphere-master-1]
04:01:46 UTC success: [kubesphere-master-2]
04:01:46 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
04:01:46 UTC success: [kubesphere-master-1]
04:01:46 UTC success: [kubesphere-master-2]
04:01:46 UTC [ETCDConfigureModule] Restart etcd
04:01:52 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:01:52 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC [ETCDConfigureModule] Health check on all etcd
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC [ETCDConfigureModule] Health check on all etcd
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC [ETCDBackupModule] Backup etcd data regularly
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC [ETCDBackupModule] Generate backup ETCD service
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC [ETCDBackupModule] Generate backup ETCD timer
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC [ETCDBackupModule] Enable backup etcd service
04:01:52 UTC success: [kubesphere-master-2]
04:01:52 UTC success: [kubesphere-master-1]
04:01:52 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
04:01:56 UTC success: [kubesphere-worker-1]
04:01:56 UTC success: [kubesphere-master-2]
04:01:56 UTC success: [kubesphere-master-1]
04:01:56 UTC [InstallKubeBinariesModule] Change kubelet mode
04:01:56 UTC success: [kubesphere-worker-1]
04:01:56 UTC success: [kubesphere-master-1]
04:01:56 UTC success: [kubesphere-master-2]
04:01:56 UTC [InstallKubeBinariesModule] Generate kubelet service
04:01:56 UTC success: [kubesphere-worker-1]
04:01:56 UTC success: [kubesphere-master-2]
04:01:56 UTC success: [kubesphere-master-1]
04:01:56 UTC [InstallKubeBinariesModule] Enable kubelet service
04:01:57 UTC success: [kubesphere-master-2]
04:01:57 UTC success: [kubesphere-master-1]
04:01:57 UTC success: [kubesphere-worker-1]
04:01:57 UTC [InstallKubeBinariesModule] Generate kubelet env
04:01:57 UTC success: [kubesphere-master-2]
04:01:57 UTC success: [kubesphere-worker-1]
04:01:57 UTC success: [kubesphere-master-1]
04:01:57 UTC [InitKubernetesModule] Generate kubeadm config
04:01:57 UTC success: [kubesphere-master-1]
04:01:57 UTC [InitKubernetesModule] Generate audit policy
04:01:57 UTC skipped: [kubesphere-master-1]
04:01:57 UTC [InitKubernetesModule] Generate audit webhook
04:01:57 UTC skipped: [kubesphere-master-1]
04:01:57 UTC [InitKubernetesModule] Init cluster using kubeadm
04:02:12 UTC stdout: [kubesphere-master-1]
W0914 04:01:57.265741    3668 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.503354 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 9d1wxk.10v6ttehujbx5mnb
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 9d1wxk.10v6ttehujbx5mnb \
	--discovery-token-ca-cert-hash sha256:b3ea3740f748df92075ed15443cc88c93ea2501fb06d0aa75a3ed9d127fd2737 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 9d1wxk.10v6ttehujbx5mnb \
	--discovery-token-ca-cert-hash sha256:b3ea3740f748df92075ed15443cc88c93ea2501fb06d0aa75a3ed9d127fd2737
04:02:12 UTC success: [kubesphere-master-1]
04:02:12 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
04:02:12 UTC success: [kubesphere-master-1]
04:02:12 UTC [InitKubernetesModule] Remove master taint
04:02:12 UTC skipped: [kubesphere-master-1]
04:02:12 UTC [ClusterDNSModule] Generate coredns service
04:02:13 UTC success: [kubesphere-master-1]
04:02:13 UTC [ClusterDNSModule] Override coredns service
04:02:13 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
04:02:13 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
04:02:13 UTC success: [kubesphere-master-1]
04:02:13 UTC [ClusterDNSModule] Generate nodelocaldns
04:02:13 UTC success: [kubesphere-master-1]
04:02:13 UTC [ClusterDNSModule] Deploy nodelocaldns
04:02:13 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
04:02:13 UTC success: [kubesphere-master-1]
04:02:13 UTC [ClusterDNSModule] Generate nodelocaldns configmap
04:02:13 UTC success: [kubesphere-master-1]
04:02:13 UTC [ClusterDNSModule] Apply nodelocaldns configmap
04:02:14 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
04:02:14 UTC success: [kubesphere-master-1]
04:02:14 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:02:14 UTC stdout: [kubesphere-master-1]
v1.23.10
04:02:14 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
04:02:14 UTC stdout: [kubesphere-master-1]
W0914 04:02:14.086428    4321 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
e8413478c0a4d82e8f8fc13d7e2e8633e6a575d0a893cb166b4ad5832755dcec
04:02:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:02:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:02:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:02:14 UTC stdout: [kubesphere-master-1]
yr4bs2.v9xy76nt6cvhi9gd
04:02:14 UTC success: [kubesphere-master-1]
04:02:14 UTC [JoinNodesModule] Generate kubeadm config
04:02:14 UTC skipped: [kubesphere-master-1]
04:02:14 UTC success: [kubesphere-worker-1]
04:02:14 UTC success: [kubesphere-master-2]
04:02:14 UTC [JoinNodesModule] Generate audit policy
04:02:14 UTC skipped: [kubesphere-master-1]
04:02:14 UTC [JoinNodesModule] Generate audit webhook
04:02:14 UTC skipped: [kubesphere-master-1]
04:02:14 UTC [JoinNodesModule] Join control-plane node
04:02:14 UTC skipped: [kubesphere-master-1]
04:02:14 UTC [JoinNodesModule] Join worker node
04:02:32 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:02:27.179011    3132 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:02:27.181125    3132 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:02:33 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:02:27.184936    3658 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:02:27.186960    3658 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:02:33 UTC success: [kubesphere-worker-1]
04:02:33 UTC success: [kubesphere-master-2]
04:02:33 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
04:02:33 UTC skipped: [kubesphere-master-1]
04:02:33 UTC [JoinNodesModule] Remove master taint
04:02:33 UTC skipped: [kubesphere-master-1]
04:02:33 UTC [JoinNodesModule] Add worker label to all nodes
04:02:33 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
04:02:33 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
04:02:33 UTC success: [kubesphere-master-1]
04:02:33 UTC [DeployNetworkPluginModule] Generate calico
04:02:33 UTC success: [kubesphere-master-1]
04:02:33 UTC [DeployNetworkPluginModule] Deploy calico
04:02:34 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [ConfigureKubernetesModule] Configure kubernetes
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [ChownModule] Chown user $HOME/.kube dir
04:02:34 UTC success: [kubesphere-worker-1]
04:02:34 UTC success: [kubesphere-master-2]
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [AutoRenewCertsModule] Generate k8s certs renew script
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [AutoRenewCertsModule] Generate k8s certs renew service
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
04:02:34 UTC success: [kubesphere-master-1]
04:02:34 UTC [AutoRenewCertsModule] Enable k8s certs renew service
04:02:35 UTC success: [kubesphere-master-1]
04:02:35 UTC [SaveKubeConfigModule] Save kube config as a configmap
04:02:35 UTC success: [LocalHost]
04:02:35 UTC [AddonsModule] Install addons
04:02:35 UTC success: [LocalHost]
04:02:35 UTC Pipeline[CreateClusterPipeline] execute successfully
04:04:39 UTC [GreetingsModule] Greetings
04:04:40 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:04:40 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:04:40 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
04:04:40 UTC success: [kubesphere-worker-1]
04:04:40 UTC success: [kubesphere-master-1]
04:04:40 UTC success: [kubesphere-master-2]
04:04:40 UTC [NodePreCheckModule] A pre-check on nodes
04:04:40 UTC success: [kubesphere-master-2]
04:04:40 UTC success: [kubesphere-master-1]
04:04:40 UTC success: [kubesphere-worker-1]
04:04:40 UTC [ConfirmModule] Display confirmation form
04:04:40 UTC success: [LocalHost]
04:04:40 UTC [NodeBinariesModule] Download installation binaries
04:04:40 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
04:04:41 UTC message: [localhost]
kubeadm is existed
04:04:41 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
04:04:41 UTC message: [localhost]
kubelet is existed
04:04:41 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
04:04:41 UTC message: [localhost]
kubectl is existed
04:04:41 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
04:04:41 UTC message: [localhost]
helm is existed
04:04:41 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
04:04:41 UTC message: [localhost]
kubecni is existed
04:04:41 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
04:04:41 UTC message: [localhost]
crictl is existed
04:04:41 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
04:04:41 UTC message: [localhost]
etcd is existed
04:04:41 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
04:04:41 UTC message: [localhost]
containerd is existed
04:04:41 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
04:04:41 UTC message: [localhost]
runc is existed
04:04:41 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
04:04:41 UTC message: [localhost]
calicoctl is existed
04:04:41 UTC success: [LocalHost]
04:04:41 UTC [ConfigureOSModule] Get OS release
04:04:41 UTC success: [kubesphere-master-2]
04:04:41 UTC success: [kubesphere-master-1]
04:04:41 UTC success: [kubesphere-worker-1]
04:04:41 UTC [ConfigureOSModule] Prepare to init OS
04:04:42 UTC success: [kubesphere-worker-1]
04:04:42 UTC success: [kubesphere-master-1]
04:04:42 UTC success: [kubesphere-master-2]
04:04:42 UTC [ConfigureOSModule] Generate init os script
04:04:42 UTC success: [kubesphere-worker-1]
04:04:42 UTC success: [kubesphere-master-1]
04:04:42 UTC success: [kubesphere-master-2]
04:04:42 UTC [ConfigureOSModule] Exec init os script
04:04:44 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:04:44 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:04:44 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:04:44 UTC success: [kubesphere-worker-1]
04:04:44 UTC success: [kubesphere-master-2]
04:04:44 UTC success: [kubesphere-master-1]
04:04:44 UTC [ConfigureOSModule] configure the ntp server for each node
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:04:44 UTC success: [kubesphere-master-1]
04:04:44 UTC success: [kubesphere-master-2]
04:04:44 UTC [InstallContainerModule] Sync containerd binaries
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC [InstallContainerModule] Sync crictl binaries
04:04:44 UTC success: [kubesphere-master-1]
04:04:44 UTC success: [kubesphere-master-2]
04:04:44 UTC success: [kubesphere-worker-1]
04:04:44 UTC [InstallContainerModule] Generate containerd service
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC [InstallContainerModule] Generate containerd config
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC [InstallContainerModule] Generate crictl config
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC [InstallContainerModule] Enable containerd
04:04:44 UTC skipped: [kubesphere-master-2]
04:04:44 UTC skipped: [kubesphere-worker-1]
04:04:44 UTC skipped: [kubesphere-master-1]
04:04:44 UTC [PullModule] Start to pull images on all nodes
04:04:44 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
04:04:44 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
04:04:44 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
04:04:46 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:04:46 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:04:46 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:04:47 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
04:04:47 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:04:48 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:04:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:04:49 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:04:49 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:04:50 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
04:04:50 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:04:51 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
04:04:52 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
04:04:52 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
04:04:52 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
04:04:53 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
04:04:53 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:04:54 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:04:55 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:04:55 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
04:04:55 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
04:04:56 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
04:04:57 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
04:04:58 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
04:04:58 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
04:04:59 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:05:00 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:05:02 UTC success: [kubesphere-worker-1]
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC [ETCDPreCheckModule] Get etcd status
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC [CertsModule] Fetch etcd certs
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC skipped: [kubesphere-master-2]
04:05:02 UTC [CertsModule] Generate etcd Certs
04:05:02 UTC success: [LocalHost]
04:05:02 UTC [CertsModule] Synchronize certs file
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC [CertsModule] Synchronize certs file to master
04:05:02 UTC skipped: [kubesphere-master-2]
04:05:02 UTC skipped: [kubesphere-master-1]
04:05:02 UTC [InstallETCDBinaryModule] Install etcd using binary
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC [InstallETCDBinaryModule] Generate etcd service
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC [InstallETCDBinaryModule] Generate access address
04:05:02 UTC skipped: [kubesphere-master-2]
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC [ETCDConfigureModule] Health check on exist etcd
04:05:02 UTC skipped: [kubesphere-master-2]
04:05:02 UTC skipped: [kubesphere-master-1]
04:05:02 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
04:05:02 UTC success: [kubesphere-master-1]
04:05:02 UTC success: [kubesphere-master-2]
04:05:02 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
04:05:03 UTC success: [kubesphere-master-1]
04:05:03 UTC success: [kubesphere-master-2]
04:05:03 UTC [ETCDConfigureModule] Restart etcd
04:05:12 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:05:12 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC [ETCDConfigureModule] Health check on all etcd
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC [ETCDConfigureModule] Health check on all etcd
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC [ETCDBackupModule] Backup etcd data regularly
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC [ETCDBackupModule] Generate backup ETCD service
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC [ETCDBackupModule] Generate backup ETCD timer
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC [ETCDBackupModule] Enable backup etcd service
04:05:12 UTC success: [kubesphere-master-2]
04:05:12 UTC success: [kubesphere-master-1]
04:05:12 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC success: [kubesphere-worker-1]
04:05:17 UTC success: [kubesphere-master-2]
04:05:17 UTC [InstallKubeBinariesModule] Change kubelet mode
04:05:17 UTC success: [kubesphere-worker-1]
04:05:17 UTC success: [kubesphere-master-2]
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC [InstallKubeBinariesModule] Generate kubelet service
04:05:17 UTC success: [kubesphere-worker-1]
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC success: [kubesphere-master-2]
04:05:17 UTC [InstallKubeBinariesModule] Enable kubelet service
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC success: [kubesphere-master-2]
04:05:17 UTC success: [kubesphere-worker-1]
04:05:17 UTC [InstallKubeBinariesModule] Generate kubelet env
04:05:17 UTC success: [kubesphere-worker-1]
04:05:17 UTC success: [kubesphere-master-2]
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC [InitKubernetesModule] Generate kubeadm config
04:05:17 UTC skipped: [kubesphere-master-2]
04:05:17 UTC success: [kubesphere-master-1]
04:05:17 UTC [InitKubernetesModule] Generate audit policy
04:05:17 UTC skipped: [kubesphere-master-2]
04:05:17 UTC skipped: [kubesphere-master-1]
04:05:17 UTC [InitKubernetesModule] Generate audit webhook
04:05:17 UTC skipped: [kubesphere-master-2]
04:05:17 UTC skipped: [kubesphere-master-1]
04:05:17 UTC [InitKubernetesModule] Init cluster using kubeadm
04:05:32 UTC stdout: [kubesphere-master-1]
W0914 04:05:17.495907    3684 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.002072 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: tna3og.gm8esyqs35t2shl4
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token tna3og.gm8esyqs35t2shl4 \
	--discovery-token-ca-cert-hash sha256:2c77d7703739848bc1da5faeadd5e41c0ffff67a55b51d088754467f0c4693cc \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token tna3og.gm8esyqs35t2shl4 \
	--discovery-token-ca-cert-hash sha256:2c77d7703739848bc1da5faeadd5e41c0ffff67a55b51d088754467f0c4693cc
04:05:32 UTC skipped: [kubesphere-master-2]
04:05:32 UTC success: [kubesphere-master-1]
04:05:32 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
04:05:32 UTC skipped: [kubesphere-master-2]
04:05:32 UTC success: [kubesphere-master-1]
04:05:32 UTC [InitKubernetesModule] Remove master taint
04:05:32 UTC skipped: [kubesphere-master-2]
04:05:32 UTC skipped: [kubesphere-master-1]
04:05:32 UTC [ClusterDNSModule] Generate coredns service
04:05:33 UTC skipped: [kubesphere-master-2]
04:05:33 UTC success: [kubesphere-master-1]
04:05:33 UTC [ClusterDNSModule] Override coredns service
04:05:33 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
04:05:33 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
04:05:33 UTC skipped: [kubesphere-master-2]
04:05:33 UTC success: [kubesphere-master-1]
04:05:33 UTC [ClusterDNSModule] Generate nodelocaldns
04:05:33 UTC skipped: [kubesphere-master-2]
04:05:33 UTC success: [kubesphere-master-1]
04:05:33 UTC [ClusterDNSModule] Deploy nodelocaldns
04:05:33 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
04:05:33 UTC skipped: [kubesphere-master-2]
04:05:33 UTC success: [kubesphere-master-1]
04:05:33 UTC [ClusterDNSModule] Generate nodelocaldns configmap
04:05:34 UTC skipped: [kubesphere-master-2]
04:05:34 UTC success: [kubesphere-master-1]
04:05:34 UTC [ClusterDNSModule] Apply nodelocaldns configmap
04:05:34 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
04:05:34 UTC skipped: [kubesphere-master-2]
04:05:34 UTC success: [kubesphere-master-1]
04:05:34 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:05:34 UTC stdout: [kubesphere-master-1]
v1.23.10
04:05:34 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
04:05:34 UTC stdout: [kubesphere-master-1]
W0914 04:05:34.271242    4350 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
2c1a1d86065d1cf718baf310bddb45c06046236f430d45df7a6f8cd4bb9a3e93
04:05:34 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:05:34 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:05:34 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:05:34 UTC stdout: [kubesphere-master-1]
v5ll65.cw3rnhva65ft248u
04:05:34 UTC success: [kubesphere-master-1]
04:05:34 UTC success: [kubesphere-master-2]
04:05:34 UTC [JoinNodesModule] Generate kubeadm config
04:05:34 UTC skipped: [kubesphere-master-1]
04:05:34 UTC success: [kubesphere-master-2]
04:05:34 UTC success: [kubesphere-worker-1]
04:05:34 UTC [JoinNodesModule] Generate audit policy
04:05:34 UTC skipped: [kubesphere-master-1]
04:05:34 UTC skipped: [kubesphere-master-2]
04:05:34 UTC [JoinNodesModule] Generate audit webhook
04:05:34 UTC skipped: [kubesphere-master-2]
04:05:34 UTC skipped: [kubesphere-master-1]
04:05:34 UTC [JoinNodesModule] Join control-plane node
04:05:55 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:05:47.417107    3706 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:05:47.420394    3706 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
04:05:55 UTC skipped: [kubesphere-master-1]
04:05:55 UTC success: [kubesphere-master-2]
04:05:55 UTC [JoinNodesModule] Join worker node
04:06:02 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:05:56.020119    3129 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:05:56.022534    3129 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:06:02 UTC skipped: [kubesphere-master-2]
04:06:02 UTC success: [kubesphere-worker-1]
04:06:02 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
04:06:02 UTC skipped: [kubesphere-master-1]
04:06:02 UTC success: [kubesphere-master-2]
04:06:02 UTC [JoinNodesModule] Remove master taint
04:06:03 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
04:06:03 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
04:06:03 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
04:06:03 UTC skipped: [kubesphere-master-1]
04:06:03 UTC success: [kubesphere-master-2]
04:06:03 UTC [JoinNodesModule] Add worker label to all nodes
04:06:03 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
04:06:03 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
04:06:03 UTC success: [kubesphere-master-1]
04:06:03 UTC skipped: [kubesphere-master-2]
04:06:03 UTC [DeployNetworkPluginModule] Generate calico
04:06:03 UTC skipped: [kubesphere-master-2]
04:06:03 UTC success: [kubesphere-master-1]
04:06:03 UTC [DeployNetworkPluginModule] Deploy calico
04:06:05 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
04:06:05 UTC skipped: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [ConfigureKubernetesModule] Configure kubernetes
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC skipped: [kubesphere-master-2]
04:06:05 UTC [ChownModule] Chown user $HOME/.kube dir
04:06:05 UTC success: [kubesphere-worker-1]
04:06:05 UTC success: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [AutoRenewCertsModule] Generate k8s certs renew script
04:06:05 UTC success: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [AutoRenewCertsModule] Generate k8s certs renew service
04:06:05 UTC success: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
04:06:05 UTC success: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [AutoRenewCertsModule] Enable k8s certs renew service
04:06:05 UTC success: [kubesphere-master-2]
04:06:05 UTC success: [kubesphere-master-1]
04:06:05 UTC [SaveKubeConfigModule] Save kube config as a configmap
04:06:05 UTC success: [LocalHost]
04:06:05 UTC [AddonsModule] Install addons
04:06:05 UTC success: [LocalHost]
04:06:05 UTC Pipeline[CreateClusterPipeline] execute successfully
04:37:50 UTC [GreetingsModule] Greetings
04:37:51 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:37:51 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
04:37:51 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:37:51 UTC success: [kubesphere-worker-1]
04:37:51 UTC success: [kubesphere-master-2]
04:37:51 UTC success: [kubesphere-master-1]
04:37:51 UTC [NodePreCheckModule] A pre-check on nodes
04:37:52 UTC success: [kubesphere-master-2]
04:37:52 UTC success: [kubesphere-master-1]
04:37:52 UTC success: [kubesphere-worker-1]
04:37:52 UTC [ConfirmModule] Display confirmation form
04:37:52 UTC success: [LocalHost]
04:37:52 UTC [NodeBinariesModule] Download installation binaries
04:37:52 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
04:37:52 UTC message: [localhost]
kubeadm is existed
04:37:52 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
04:37:52 UTC message: [localhost]
kubelet is existed
04:37:52 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
04:37:52 UTC message: [localhost]
kubectl is existed
04:37:52 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
04:37:52 UTC message: [localhost]
helm is existed
04:37:52 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
04:37:52 UTC message: [localhost]
kubecni is existed
04:37:52 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
04:37:52 UTC message: [localhost]
crictl is existed
04:37:52 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
04:37:52 UTC message: [localhost]
etcd is existed
04:37:52 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
04:37:52 UTC message: [localhost]
containerd is existed
04:37:52 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
04:37:52 UTC message: [localhost]
runc is existed
04:37:52 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
04:37:53 UTC message: [localhost]
calicoctl is existed
04:37:53 UTC success: [LocalHost]
04:37:53 UTC [ConfigureOSModule] Get OS release
04:37:53 UTC success: [kubesphere-master-2]
04:37:53 UTC success: [kubesphere-master-1]
04:37:53 UTC success: [kubesphere-worker-1]
04:37:53 UTC [ConfigureOSModule] Prepare to init OS
04:37:53 UTC success: [kubesphere-worker-1]
04:37:53 UTC success: [kubesphere-master-2]
04:37:53 UTC success: [kubesphere-master-1]
04:37:53 UTC [ConfigureOSModule] Generate init os script
04:37:53 UTC success: [kubesphere-master-1]
04:37:53 UTC success: [kubesphere-worker-1]
04:37:53 UTC success: [kubesphere-master-2]
04:37:53 UTC [ConfigureOSModule] Exec init os script
04:37:55 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:37:55 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:37:55 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:37:55 UTC success: [kubesphere-master-2]
04:37:55 UTC success: [kubesphere-master-1]
04:37:55 UTC success: [kubesphere-worker-1]
04:37:55 UTC [ConfigureOSModule] configure the ntp server for each node
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:37:55 UTC success: [kubesphere-master-1]
04:37:55 UTC success: [kubesphere-master-2]
04:37:55 UTC [InstallContainerModule] Sync containerd binaries
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC [InstallContainerModule] Sync crictl binaries
04:37:55 UTC success: [kubesphere-master-2]
04:37:55 UTC success: [kubesphere-master-1]
04:37:55 UTC success: [kubesphere-worker-1]
04:37:55 UTC [InstallContainerModule] Generate containerd service
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC [InstallContainerModule] Generate containerd config
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC [InstallContainerModule] Generate crictl config
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC [InstallContainerModule] Enable containerd
04:37:55 UTC skipped: [kubesphere-master-2]
04:37:55 UTC skipped: [kubesphere-worker-1]
04:37:55 UTC skipped: [kubesphere-master-1]
04:37:55 UTC [PullModule] Start to pull images on all nodes
04:37:55 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
04:37:55 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
04:37:55 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
04:37:57 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:37:57 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:37:57 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:37:58 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:37:58 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:37:59 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
04:38:00 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:38:00 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:38:00 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:38:02 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:38:02 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
04:38:02 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
04:38:03 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
04:38:03 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
04:38:03 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
04:38:05 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:38:05 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
04:38:05 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:38:06 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
04:38:06 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:38:07 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
04:38:08 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
04:38:08 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
04:38:09 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
04:38:10 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
04:38:11 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:38:11 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:38:13 UTC success: [kubesphere-worker-1]
04:38:13 UTC success: [kubesphere-master-1]
04:38:13 UTC success: [kubesphere-master-2]
04:38:13 UTC [ETCDPreCheckModule] Get etcd status
04:38:13 UTC success: [kubesphere-master-1]
04:38:13 UTC success: [kubesphere-master-2]
04:38:13 UTC [CertsModule] Fetch etcd certs
04:38:13 UTC success: [kubesphere-master-1]
04:38:13 UTC skipped: [kubesphere-master-2]
04:38:13 UTC [CertsModule] Generate etcd Certs
04:38:13 UTC success: [LocalHost]
04:38:13 UTC [CertsModule] Synchronize certs file
04:38:13 UTC success: [kubesphere-master-2]
04:38:13 UTC success: [kubesphere-master-1]
04:38:13 UTC [CertsModule] Synchronize certs file to master
04:38:13 UTC skipped: [kubesphere-master-2]
04:38:13 UTC skipped: [kubesphere-master-1]
04:38:13 UTC [InstallETCDBinaryModule] Install etcd using binary
04:38:14 UTC success: [kubesphere-master-1]
04:38:14 UTC success: [kubesphere-master-2]
04:38:14 UTC [InstallETCDBinaryModule] Generate etcd service
04:38:14 UTC success: [kubesphere-master-2]
04:38:14 UTC success: [kubesphere-master-1]
04:38:14 UTC [InstallETCDBinaryModule] Generate access address
04:38:14 UTC skipped: [kubesphere-master-2]
04:38:14 UTC success: [kubesphere-master-1]
04:38:14 UTC [ETCDConfigureModule] Health check on exist etcd
04:38:14 UTC skipped: [kubesphere-master-2]
04:38:14 UTC skipped: [kubesphere-master-1]
04:38:14 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
04:38:14 UTC success: [kubesphere-master-1]
04:38:14 UTC success: [kubesphere-master-2]
04:38:14 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
04:38:14 UTC success: [kubesphere-master-1]
04:38:14 UTC success: [kubesphere-master-2]
04:38:14 UTC [ETCDConfigureModule] Restart etcd
04:38:23 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:38:23 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [ETCDConfigureModule] Health check on all etcd
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC [ETCDConfigureModule] Health check on all etcd
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [ETCDBackupModule] Backup etcd data regularly
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [ETCDBackupModule] Generate backup ETCD service
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [ETCDBackupModule] Generate backup ETCD timer
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC [ETCDBackupModule] Enable backup etcd service
04:38:23 UTC success: [kubesphere-master-2]
04:38:23 UTC success: [kubesphere-master-1]
04:38:23 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC success: [kubesphere-worker-1]
04:38:28 UTC success: [kubesphere-master-2]
04:38:28 UTC [InstallKubeBinariesModule] Change kubelet mode
04:38:28 UTC success: [kubesphere-worker-1]
04:38:28 UTC success: [kubesphere-master-2]
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC [InstallKubeBinariesModule] Generate kubelet service
04:38:28 UTC success: [kubesphere-worker-1]
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC success: [kubesphere-master-2]
04:38:28 UTC [InstallKubeBinariesModule] Enable kubelet service
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC success: [kubesphere-master-2]
04:38:28 UTC success: [kubesphere-worker-1]
04:38:28 UTC [InstallKubeBinariesModule] Generate kubelet env
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC success: [kubesphere-worker-1]
04:38:28 UTC success: [kubesphere-master-2]
04:38:28 UTC [InitKubernetesModule] Generate kubeadm config
04:38:28 UTC skipped: [kubesphere-master-2]
04:38:28 UTC success: [kubesphere-master-1]
04:38:28 UTC [InitKubernetesModule] Generate audit policy
04:38:28 UTC skipped: [kubesphere-master-2]
04:38:28 UTC skipped: [kubesphere-master-1]
04:38:28 UTC [InitKubernetesModule] Generate audit webhook
04:38:28 UTC skipped: [kubesphere-master-2]
04:38:28 UTC skipped: [kubesphere-master-1]
04:38:28 UTC [InitKubernetesModule] Init cluster using kubeadm
04:38:43 UTC stdout: [kubesphere-master-1]
W0914 04:38:28.620324    3745 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.502100 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 5hocxq.kga1adl5aajmkabz
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 5hocxq.kga1adl5aajmkabz \
	--discovery-token-ca-cert-hash sha256:a4b8395866cb68690f81af7b24eaf2b9d5088e0c7299b19ee8d0a522f447ef59 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 5hocxq.kga1adl5aajmkabz \
	--discovery-token-ca-cert-hash sha256:a4b8395866cb68690f81af7b24eaf2b9d5088e0c7299b19ee8d0a522f447ef59
04:38:43 UTC skipped: [kubesphere-master-2]
04:38:43 UTC success: [kubesphere-master-1]
04:38:43 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
04:38:43 UTC skipped: [kubesphere-master-2]
04:38:43 UTC success: [kubesphere-master-1]
04:38:43 UTC [InitKubernetesModule] Remove master taint
04:38:43 UTC skipped: [kubesphere-master-2]
04:38:43 UTC skipped: [kubesphere-master-1]
04:38:43 UTC [ClusterDNSModule] Generate coredns service
04:38:44 UTC skipped: [kubesphere-master-2]
04:38:44 UTC success: [kubesphere-master-1]
04:38:44 UTC [ClusterDNSModule] Override coredns service
04:38:44 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
04:38:44 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
04:38:44 UTC skipped: [kubesphere-master-2]
04:38:44 UTC success: [kubesphere-master-1]
04:38:44 UTC [ClusterDNSModule] Generate nodelocaldns
04:38:44 UTC skipped: [kubesphere-master-2]
04:38:44 UTC success: [kubesphere-master-1]
04:38:44 UTC [ClusterDNSModule] Deploy nodelocaldns
04:38:44 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
04:38:44 UTC skipped: [kubesphere-master-2]
04:38:44 UTC success: [kubesphere-master-1]
04:38:44 UTC [ClusterDNSModule] Generate nodelocaldns configmap
04:38:44 UTC skipped: [kubesphere-master-2]
04:38:44 UTC success: [kubesphere-master-1]
04:38:44 UTC [ClusterDNSModule] Apply nodelocaldns configmap
04:38:45 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
04:38:45 UTC skipped: [kubesphere-master-2]
04:38:45 UTC success: [kubesphere-master-1]
04:38:45 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:38:45 UTC stdout: [kubesphere-master-1]
v1.23.10
04:38:45 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
04:38:45 UTC stdout: [kubesphere-master-1]
W0914 04:38:45.084148    4406 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
f1c90956c67fe8f2339bf1f2e99eb6ed12f2cbcf4a4bbfb5c377f8227ebafa1f
04:38:45 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:38:45 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:38:45 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:38:45 UTC stdout: [kubesphere-master-1]
lajw3h.dxuhuwh4mzhs4q98
04:38:45 UTC success: [kubesphere-master-1]
04:38:45 UTC success: [kubesphere-master-2]
04:38:45 UTC [JoinNodesModule] Generate kubeadm config
04:38:45 UTC skipped: [kubesphere-master-1]
04:38:45 UTC success: [kubesphere-master-2]
04:38:45 UTC success: [kubesphere-worker-1]
04:38:45 UTC [JoinNodesModule] Generate audit policy
04:38:45 UTC skipped: [kubesphere-master-2]
04:38:45 UTC skipped: [kubesphere-master-1]
04:38:45 UTC [JoinNodesModule] Generate audit webhook
04:38:45 UTC skipped: [kubesphere-master-2]
04:38:45 UTC skipped: [kubesphere-master-1]
04:38:45 UTC [JoinNodesModule] Join control-plane node
04:39:05 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:38:58.243787    3751 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:38:58.246184    3751 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
04:39:05 UTC skipped: [kubesphere-master-1]
04:39:05 UTC success: [kubesphere-master-2]
04:39:05 UTC [JoinNodesModule] Join worker node
04:39:12 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 04:39:06.248888    3193 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 04:39:06.251114    3193 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:39:12 UTC skipped: [kubesphere-master-2]
04:39:12 UTC success: [kubesphere-worker-1]
04:39:12 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
04:39:12 UTC skipped: [kubesphere-master-1]
04:39:12 UTC success: [kubesphere-master-2]
04:39:12 UTC [JoinNodesModule] Remove master taint
04:39:12 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
04:39:12 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
04:39:12 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
04:39:12 UTC skipped: [kubesphere-master-1]
04:39:12 UTC success: [kubesphere-master-2]
04:39:12 UTC [JoinNodesModule] Add worker label to all nodes
04:39:12 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
04:39:12 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
04:39:12 UTC success: [kubesphere-master-1]
04:39:12 UTC skipped: [kubesphere-master-2]
04:39:12 UTC [DeployNetworkPluginModule] Generate calico
04:39:12 UTC skipped: [kubesphere-master-2]
04:39:12 UTC success: [kubesphere-master-1]
04:39:12 UTC [DeployNetworkPluginModule] Deploy calico
04:39:14 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
04:39:14 UTC skipped: [kubesphere-master-2]
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC [ConfigureKubernetesModule] Configure kubernetes
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC skipped: [kubesphere-master-2]
04:39:14 UTC [ChownModule] Chown user $HOME/.kube dir
04:39:14 UTC success: [kubesphere-worker-1]
04:39:14 UTC success: [kubesphere-master-2]
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC [AutoRenewCertsModule] Generate k8s certs renew script
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC success: [kubesphere-master-2]
04:39:14 UTC [AutoRenewCertsModule] Generate k8s certs renew service
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC success: [kubesphere-master-2]
04:39:14 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
04:39:14 UTC success: [kubesphere-master-1]
04:39:14 UTC success: [kubesphere-master-2]
04:39:14 UTC [AutoRenewCertsModule] Enable k8s certs renew service
04:39:15 UTC success: [kubesphere-master-2]
04:39:15 UTC success: [kubesphere-master-1]
04:39:15 UTC [SaveKubeConfigModule] Save kube config as a configmap
04:39:15 UTC success: [LocalHost]
04:39:15 UTC [AddonsModule] Install addons
04:39:15 UTC success: [LocalHost]
04:39:15 UTC Pipeline[CreateClusterPipeline] execute successfully
23:36:44 UTC [GreetingsModule] Greetings
23:36:44 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
23:36:45 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
23:36:45 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
23:36:45 UTC success: [kubesphere-master-1]
23:36:45 UTC success: [kubesphere-worker-1]
23:36:45 UTC success: [kubesphere-master-2]
23:36:45 UTC [NodePreCheckModule] A pre-check on nodes
23:36:45 UTC success: [kubesphere-master-2]
23:36:45 UTC success: [kubesphere-worker-1]
23:36:45 UTC success: [kubesphere-master-1]
23:36:45 UTC [ConfirmModule] Display confirmation form
23:36:45 UTC success: [LocalHost]
23:36:45 UTC [NodeBinariesModule] Download installation binaries
23:36:45 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
23:36:45 UTC message: [localhost]
kubeadm is existed
23:36:45 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
23:36:46 UTC message: [localhost]
kubelet is existed
23:36:46 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
23:36:46 UTC message: [localhost]
kubectl is existed
23:36:46 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
23:36:46 UTC message: [localhost]
helm is existed
23:36:46 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
23:36:46 UTC message: [localhost]
kubecni is existed
23:36:46 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
23:36:46 UTC message: [localhost]
crictl is existed
23:36:46 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
23:36:46 UTC message: [localhost]
etcd is existed
23:36:46 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
23:36:46 UTC message: [localhost]
containerd is existed
23:36:46 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
23:36:46 UTC message: [localhost]
runc is existed
23:36:46 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
23:36:46 UTC message: [localhost]
calicoctl is existed
23:36:46 UTC success: [LocalHost]
23:36:46 UTC [ConfigureOSModule] Get OS release
23:36:46 UTC success: [kubesphere-worker-1]
23:36:46 UTC success: [kubesphere-master-2]
23:36:46 UTC success: [kubesphere-master-1]
23:36:46 UTC [ConfigureOSModule] Prepare to init OS
23:36:47 UTC success: [kubesphere-worker-1]
23:36:47 UTC success: [kubesphere-master-1]
23:36:47 UTC success: [kubesphere-master-2]
23:36:47 UTC [ConfigureOSModule] Generate init os script
23:36:47 UTC success: [kubesphere-master-2]
23:36:47 UTC success: [kubesphere-master-1]
23:36:47 UTC success: [kubesphere-worker-1]
23:36:47 UTC [ConfigureOSModule] Exec init os script
23:36:49 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
23:36:49 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
23:36:49 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
23:36:49 UTC success: [kubesphere-master-2]
23:36:49 UTC success: [kubesphere-worker-1]
23:36:49 UTC success: [kubesphere-master-1]
23:36:49 UTC [ConfigureOSModule] configure the ntp server for each node
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC [KubernetesStatusModule] Get kubernetes cluster status
23:36:49 UTC success: [kubesphere-master-1]
23:36:49 UTC success: [kubesphere-master-2]
23:36:49 UTC [InstallContainerModule] Sync containerd binaries
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC [InstallContainerModule] Sync crictl binaries
23:36:49 UTC success: [kubesphere-master-1]
23:36:49 UTC success: [kubesphere-worker-1]
23:36:49 UTC success: [kubesphere-master-2]
23:36:49 UTC [InstallContainerModule] Generate containerd service
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC [InstallContainerModule] Generate containerd config
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC [InstallContainerModule] Generate crictl config
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC [InstallContainerModule] Enable containerd
23:36:49 UTC skipped: [kubesphere-master-2]
23:36:49 UTC skipped: [kubesphere-master-1]
23:36:49 UTC skipped: [kubesphere-worker-1]
23:36:49 UTC [PullModule] Start to pull images on all nodes
23:36:49 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
23:36:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
23:36:49 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
23:36:51 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
23:36:51 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
23:36:51 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
23:36:52 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
23:36:52 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
23:36:52 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
23:36:54 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
23:36:54 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
23:36:54 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
23:36:55 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
23:36:55 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
23:36:55 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
23:36:57 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
23:36:57 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
23:36:57 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
23:36:58 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
23:36:58 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
23:36:58 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
23:37:00 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
23:37:00 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
23:37:00 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
23:37:01 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
23:37:01 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
23:37:03 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
23:37:03 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
23:37:04 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
23:37:04 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
23:37:06 UTC success: [kubesphere-worker-1]
23:37:06 UTC success: [kubesphere-master-1]
23:37:06 UTC success: [kubesphere-master-2]
23:37:06 UTC [ETCDPreCheckModule] Get etcd status
23:37:06 UTC success: [kubesphere-master-1]
23:37:06 UTC success: [kubesphere-master-2]
23:37:06 UTC [CertsModule] Fetch etcd certs
23:37:06 UTC success: [kubesphere-master-1]
23:37:06 UTC skipped: [kubesphere-master-2]
23:37:06 UTC [CertsModule] Generate etcd Certs
23:37:06 UTC success: [LocalHost]
23:37:06 UTC [CertsModule] Synchronize certs file
23:37:06 UTC success: [kubesphere-master-1]
23:37:06 UTC success: [kubesphere-master-2]
23:37:06 UTC [CertsModule] Synchronize certs file to master
23:37:06 UTC skipped: [kubesphere-master-2]
23:37:06 UTC skipped: [kubesphere-master-1]
23:37:06 UTC [InstallETCDBinaryModule] Install etcd using binary
23:37:07 UTC success: [kubesphere-master-1]
23:37:07 UTC success: [kubesphere-master-2]
23:37:07 UTC [InstallETCDBinaryModule] Generate etcd service
23:37:07 UTC success: [kubesphere-master-2]
23:37:07 UTC success: [kubesphere-master-1]
23:37:07 UTC [InstallETCDBinaryModule] Generate access address
23:37:07 UTC skipped: [kubesphere-master-2]
23:37:07 UTC success: [kubesphere-master-1]
23:37:07 UTC [ETCDConfigureModule] Health check on exist etcd
23:37:07 UTC skipped: [kubesphere-master-2]
23:37:07 UTC skipped: [kubesphere-master-1]
23:37:07 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
23:37:07 UTC success: [kubesphere-master-1]
23:37:07 UTC success: [kubesphere-master-2]
23:37:07 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
23:37:07 UTC success: [kubesphere-master-1]
23:37:07 UTC success: [kubesphere-master-2]
23:37:07 UTC [ETCDConfigureModule] Restart etcd
23:37:14 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
23:37:14 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDConfigureModule] Health check on all etcd
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDConfigureModule] Health check on all etcd
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDBackupModule] Backup etcd data regularly
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDBackupModule] Generate backup ETCD service
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC [ETCDBackupModule] Generate backup ETCD timer
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [ETCDBackupModule] Enable backup etcd service
23:37:14 UTC success: [kubesphere-master-1]
23:37:14 UTC success: [kubesphere-master-2]
23:37:14 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
23:37:18 UTC success: [kubesphere-master-2]
23:37:18 UTC success: [kubesphere-master-1]
23:37:18 UTC success: [kubesphere-worker-1]
23:37:18 UTC [InstallKubeBinariesModule] Change kubelet mode
23:37:18 UTC success: [kubesphere-worker-1]
23:37:18 UTC success: [kubesphere-master-2]
23:37:18 UTC success: [kubesphere-master-1]
23:37:18 UTC [InstallKubeBinariesModule] Generate kubelet service
23:37:18 UTC success: [kubesphere-master-1]
23:37:18 UTC success: [kubesphere-master-2]
23:37:18 UTC success: [kubesphere-worker-1]
23:37:18 UTC [InstallKubeBinariesModule] Enable kubelet service
23:37:19 UTC success: [kubesphere-master-2]
23:37:19 UTC success: [kubesphere-master-1]
23:37:19 UTC success: [kubesphere-worker-1]
23:37:19 UTC [InstallKubeBinariesModule] Generate kubelet env
23:37:19 UTC success: [kubesphere-master-1]
23:37:19 UTC success: [kubesphere-worker-1]
23:37:19 UTC success: [kubesphere-master-2]
23:37:19 UTC [InitKubernetesModule] Generate kubeadm config
23:37:19 UTC skipped: [kubesphere-master-2]
23:37:19 UTC success: [kubesphere-master-1]
23:37:19 UTC [InitKubernetesModule] Generate audit policy
23:37:19 UTC skipped: [kubesphere-master-2]
23:37:19 UTC skipped: [kubesphere-master-1]
23:37:19 UTC [InitKubernetesModule] Generate audit webhook
23:37:19 UTC skipped: [kubesphere-master-2]
23:37:19 UTC skipped: [kubesphere-master-1]
23:37:19 UTC [InitKubernetesModule] Init cluster using kubeadm
23:37:34 UTC stdout: [kubesphere-master-1]
W0914 23:37:19.407209    3704 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.501992 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 26yh7q.8zdiqyk7x2qm34j7
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 26yh7q.8zdiqyk7x2qm34j7 \
	--discovery-token-ca-cert-hash sha256:a42e2ea02a89290a5108bcf29d218fde190e7775e6c5547da20eac2e1bdda781 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 26yh7q.8zdiqyk7x2qm34j7 \
	--discovery-token-ca-cert-hash sha256:a42e2ea02a89290a5108bcf29d218fde190e7775e6c5547da20eac2e1bdda781
23:37:34 UTC skipped: [kubesphere-master-2]
23:37:34 UTC success: [kubesphere-master-1]
23:37:34 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
23:37:34 UTC skipped: [kubesphere-master-2]
23:37:34 UTC success: [kubesphere-master-1]
23:37:34 UTC [InitKubernetesModule] Remove master taint
23:37:34 UTC skipped: [kubesphere-master-2]
23:37:34 UTC skipped: [kubesphere-master-1]
23:37:34 UTC [ClusterDNSModule] Generate coredns service
23:37:34 UTC skipped: [kubesphere-master-2]
23:37:34 UTC success: [kubesphere-master-1]
23:37:34 UTC [ClusterDNSModule] Override coredns service
23:37:34 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
23:37:35 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
23:37:35 UTC skipped: [kubesphere-master-2]
23:37:35 UTC success: [kubesphere-master-1]
23:37:35 UTC [ClusterDNSModule] Generate nodelocaldns
23:37:35 UTC skipped: [kubesphere-master-2]
23:37:35 UTC success: [kubesphere-master-1]
23:37:35 UTC [ClusterDNSModule] Deploy nodelocaldns
23:37:35 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
23:37:35 UTC skipped: [kubesphere-master-2]
23:37:35 UTC success: [kubesphere-master-1]
23:37:35 UTC [ClusterDNSModule] Generate nodelocaldns configmap
23:37:35 UTC skipped: [kubesphere-master-2]
23:37:35 UTC success: [kubesphere-master-1]
23:37:35 UTC [ClusterDNSModule] Apply nodelocaldns configmap
23:37:36 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
23:37:36 UTC skipped: [kubesphere-master-2]
23:37:36 UTC success: [kubesphere-master-1]
23:37:36 UTC [KubernetesStatusModule] Get kubernetes cluster status
23:37:36 UTC stdout: [kubesphere-master-1]
v1.23.10
23:37:36 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
23:37:36 UTC stdout: [kubesphere-master-1]
W0914 23:37:36.082371    4370 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
62234ca9335509eebfa58bc92d1c0fe495ea30793f12dbc4e3feb4d24578de52
23:37:36 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
23:37:36 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
23:37:36 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
23:37:36 UTC stdout: [kubesphere-master-1]
886f7h.qlvqomaocsau2ggg
23:37:36 UTC success: [kubesphere-master-1]
23:37:36 UTC success: [kubesphere-master-2]
23:37:36 UTC [JoinNodesModule] Generate kubeadm config
23:37:36 UTC skipped: [kubesphere-master-1]
23:37:36 UTC success: [kubesphere-master-2]
23:37:36 UTC success: [kubesphere-worker-1]
23:37:36 UTC [JoinNodesModule] Generate audit policy
23:37:36 UTC skipped: [kubesphere-master-2]
23:37:36 UTC skipped: [kubesphere-master-1]
23:37:36 UTC [JoinNodesModule] Generate audit webhook
23:37:36 UTC skipped: [kubesphere-master-2]
23:37:36 UTC skipped: [kubesphere-master-1]
23:37:36 UTC [JoinNodesModule] Join control-plane node
23:37:57 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 23:37:49.347051    3716 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 23:37:49.349180    3716 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
23:37:57 UTC skipped: [kubesphere-master-1]
23:37:57 UTC success: [kubesphere-master-2]
23:37:57 UTC [JoinNodesModule] Join worker node
23:38:04 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0914 23:37:57.775865    3143 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0914 23:37:57.779339    3143 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
23:38:04 UTC skipped: [kubesphere-master-2]
23:38:04 UTC success: [kubesphere-worker-1]
23:38:04 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
23:38:04 UTC skipped: [kubesphere-master-1]
23:38:04 UTC success: [kubesphere-master-2]
23:38:04 UTC [JoinNodesModule] Remove master taint
23:38:04 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
23:38:04 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
23:38:04 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
23:38:04 UTC skipped: [kubesphere-master-1]
23:38:04 UTC success: [kubesphere-master-2]
23:38:04 UTC [JoinNodesModule] Add worker label to all nodes
23:38:04 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
23:38:05 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
23:38:05 UTC success: [kubesphere-master-1]
23:38:05 UTC skipped: [kubesphere-master-2]
23:38:05 UTC [DeployNetworkPluginModule] Generate calico
23:38:05 UTC skipped: [kubesphere-master-2]
23:38:05 UTC success: [kubesphere-master-1]
23:38:05 UTC [DeployNetworkPluginModule] Deploy calico
23:38:06 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
23:38:06 UTC skipped: [kubesphere-master-2]
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC [ConfigureKubernetesModule] Configure kubernetes
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC skipped: [kubesphere-master-2]
23:38:06 UTC [ChownModule] Chown user $HOME/.kube dir
23:38:06 UTC success: [kubesphere-worker-1]
23:38:06 UTC success: [kubesphere-master-2]
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC [AutoRenewCertsModule] Generate k8s certs renew script
23:38:06 UTC success: [kubesphere-master-2]
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC [AutoRenewCertsModule] Generate k8s certs renew service
23:38:06 UTC success: [kubesphere-master-2]
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
23:38:06 UTC success: [kubesphere-master-2]
23:38:06 UTC success: [kubesphere-master-1]
23:38:06 UTC [AutoRenewCertsModule] Enable k8s certs renew service
23:38:07 UTC success: [kubesphere-master-2]
23:38:07 UTC success: [kubesphere-master-1]
23:38:07 UTC [SaveKubeConfigModule] Save kube config as a configmap
23:38:07 UTC success: [LocalHost]
23:38:07 UTC [AddonsModule] Install addons
23:38:07 UTC success: [LocalHost]
23:38:07 UTC Pipeline[CreateClusterPipeline] execute successfully
