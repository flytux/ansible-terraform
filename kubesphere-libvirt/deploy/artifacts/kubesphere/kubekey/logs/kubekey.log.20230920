02:14:54 UTC [GreetingsModule] Greetings
02:14:55 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
02:14:55 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
02:14:56 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
02:14:56 UTC success: [kubesphere-worker-1]
02:14:56 UTC success: [kubesphere-master-1]
02:14:56 UTC success: [kubesphere-master-2]
02:14:56 UTC [NodePreCheckModule] A pre-check on nodes
02:14:56 UTC success: [kubesphere-master-2]
02:14:56 UTC success: [kubesphere-worker-1]
02:14:56 UTC success: [kubesphere-master-1]
02:14:56 UTC [ConfirmModule] Display confirmation form
02:14:56 UTC success: [LocalHost]
02:14:56 UTC [NodeBinariesModule] Download installation binaries
02:14:56 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
02:14:58 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
02:15:03 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
02:15:06 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
02:15:07 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
02:15:10 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
02:15:11 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
02:15:13 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
02:15:15 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
02:15:16 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
02:15:19 UTC success: [LocalHost]
02:15:19 UTC [ConfigureOSModule] Get OS release
02:15:19 UTC success: [kubesphere-master-1]
02:15:19 UTC success: [kubesphere-worker-1]
02:15:19 UTC success: [kubesphere-master-2]
02:15:19 UTC [ConfigureOSModule] Prepare to init OS
02:15:19 UTC success: [kubesphere-worker-1]
02:15:19 UTC success: [kubesphere-master-2]
02:15:19 UTC success: [kubesphere-master-1]
02:15:19 UTC [ConfigureOSModule] Generate init os script
02:15:19 UTC success: [kubesphere-master-1]
02:15:19 UTC success: [kubesphere-master-2]
02:15:19 UTC success: [kubesphere-worker-1]
02:15:19 UTC [ConfigureOSModule] Exec init os script
02:15:21 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:15:21 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:15:21 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:15:21 UTC success: [kubesphere-worker-1]
02:15:21 UTC success: [kubesphere-master-2]
02:15:21 UTC success: [kubesphere-master-1]
02:15:21 UTC [ConfigureOSModule] configure the ntp server for each node
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:15:21 UTC success: [kubesphere-master-1]
02:15:21 UTC success: [kubesphere-master-2]
02:15:21 UTC [InstallContainerModule] Sync containerd binaries
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC [InstallContainerModule] Sync crictl binaries
02:15:21 UTC success: [kubesphere-worker-1]
02:15:21 UTC success: [kubesphere-master-2]
02:15:21 UTC success: [kubesphere-master-1]
02:15:21 UTC [InstallContainerModule] Generate containerd service
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC [InstallContainerModule] Generate containerd config
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC [InstallContainerModule] Generate crictl config
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC [InstallContainerModule] Enable containerd
02:15:21 UTC skipped: [kubesphere-master-2]
02:15:21 UTC skipped: [kubesphere-worker-1]
02:15:21 UTC skipped: [kubesphere-master-1]
02:15:21 UTC [PullModule] Start to pull images on all nodes
02:15:21 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
02:15:21 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
02:15:21 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
02:15:23 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:15:23 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:15:23 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:15:26 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:15:26 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:15:26 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
02:15:28 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:15:28 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:15:29 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:15:30 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
02:15:31 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:15:31 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
02:15:33 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
02:15:33 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
02:15:34 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
02:15:35 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:15:35 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:15:38 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
02:15:38 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
02:15:38 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
02:15:41 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
02:15:41 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
02:15:42 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:15:45 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
02:15:46 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
02:15:49 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:15:49 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:15:51 UTC success: [kubesphere-worker-1]
02:15:51 UTC success: [kubesphere-master-2]
02:15:51 UTC success: [kubesphere-master-1]
02:15:51 UTC [ETCDPreCheckModule] Get etcd status
02:15:51 UTC success: [kubesphere-master-1]
02:15:51 UTC success: [kubesphere-master-2]
02:15:51 UTC [CertsModule] Fetch etcd certs
02:15:51 UTC success: [kubesphere-master-1]
02:15:51 UTC skipped: [kubesphere-master-2]
02:15:51 UTC [CertsModule] Generate etcd Certs
02:15:52 UTC success: [LocalHost]
02:15:52 UTC [CertsModule] Synchronize certs file
02:15:52 UTC success: [kubesphere-master-2]
02:15:52 UTC success: [kubesphere-master-1]
02:15:52 UTC [CertsModule] Synchronize certs file to master
02:15:52 UTC skipped: [kubesphere-master-2]
02:15:52 UTC skipped: [kubesphere-master-1]
02:15:52 UTC [InstallETCDBinaryModule] Install etcd using binary
02:15:53 UTC success: [kubesphere-master-1]
02:15:53 UTC success: [kubesphere-master-2]
02:15:53 UTC [InstallETCDBinaryModule] Generate etcd service
02:15:53 UTC success: [kubesphere-master-2]
02:15:53 UTC success: [kubesphere-master-1]
02:15:53 UTC [InstallETCDBinaryModule] Generate access address
02:15:53 UTC skipped: [kubesphere-master-2]
02:15:53 UTC success: [kubesphere-master-1]
02:15:53 UTC [ETCDConfigureModule] Health check on exist etcd
02:15:53 UTC skipped: [kubesphere-master-2]
02:15:53 UTC skipped: [kubesphere-master-1]
02:15:53 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
02:15:53 UTC success: [kubesphere-master-1]
02:15:53 UTC success: [kubesphere-master-2]
02:15:53 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
02:15:53 UTC success: [kubesphere-master-1]
02:15:53 UTC success: [kubesphere-master-2]
02:15:53 UTC [ETCDConfigureModule] Restart etcd
02:16:02 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:16:02 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:16:02 UTC success: [kubesphere-master-1]
02:16:02 UTC success: [kubesphere-master-2]
02:16:02 UTC [ETCDConfigureModule] Health check on all etcd
02:16:02 UTC success: [kubesphere-master-2]
02:16:02 UTC success: [kubesphere-master-1]
02:16:02 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
02:16:02 UTC success: [kubesphere-master-1]
02:16:02 UTC success: [kubesphere-master-2]
02:16:02 UTC [ETCDConfigureModule] Health check on all etcd
02:16:03 UTC success: [kubesphere-master-1]
02:16:03 UTC success: [kubesphere-master-2]
02:16:03 UTC [ETCDBackupModule] Backup etcd data regularly
02:16:03 UTC success: [kubesphere-master-2]
02:16:03 UTC success: [kubesphere-master-1]
02:16:03 UTC [ETCDBackupModule] Generate backup ETCD service
02:16:03 UTC success: [kubesphere-master-2]
02:16:03 UTC success: [kubesphere-master-1]
02:16:03 UTC [ETCDBackupModule] Generate backup ETCD timer
02:16:03 UTC success: [kubesphere-master-1]
02:16:03 UTC success: [kubesphere-master-2]
02:16:03 UTC [ETCDBackupModule] Enable backup etcd service
02:16:03 UTC success: [kubesphere-master-1]
02:16:03 UTC success: [kubesphere-master-2]
02:16:03 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
02:16:07 UTC success: [kubesphere-master-1]
02:16:07 UTC success: [kubesphere-worker-1]
02:16:07 UTC success: [kubesphere-master-2]
02:16:07 UTC [InstallKubeBinariesModule] Change kubelet mode
02:16:07 UTC success: [kubesphere-master-2]
02:16:07 UTC success: [kubesphere-worker-1]
02:16:07 UTC success: [kubesphere-master-1]
02:16:07 UTC [InstallKubeBinariesModule] Generate kubelet service
02:16:07 UTC success: [kubesphere-master-2]
02:16:07 UTC success: [kubesphere-worker-1]
02:16:07 UTC success: [kubesphere-master-1]
02:16:07 UTC [InstallKubeBinariesModule] Enable kubelet service
02:16:07 UTC success: [kubesphere-master-1]
02:16:07 UTC success: [kubesphere-master-2]
02:16:07 UTC success: [kubesphere-worker-1]
02:16:07 UTC [InstallKubeBinariesModule] Generate kubelet env
02:16:08 UTC success: [kubesphere-worker-1]
02:16:08 UTC success: [kubesphere-master-1]
02:16:08 UTC success: [kubesphere-master-2]
02:16:08 UTC [InitKubernetesModule] Generate kubeadm config
02:16:08 UTC skipped: [kubesphere-master-2]
02:16:08 UTC success: [kubesphere-master-1]
02:16:08 UTC [InitKubernetesModule] Generate audit policy
02:16:08 UTC skipped: [kubesphere-master-2]
02:16:08 UTC skipped: [kubesphere-master-1]
02:16:08 UTC [InitKubernetesModule] Generate audit webhook
02:16:08 UTC skipped: [kubesphere-master-2]
02:16:08 UTC skipped: [kubesphere-master-1]
02:16:08 UTC [InitKubernetesModule] Init cluster using kubeadm
02:16:23 UTC stdout: [kubesphere-master-1]
W0920 02:16:08.099804    3890 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.151 127.0.0.1 192.168.100.152 192.168.100.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 12.501868 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 7ylqrb.kbxmdfb21np2lwfz
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 7ylqrb.kbxmdfb21np2lwfz \
	--discovery-token-ca-cert-hash sha256:ff15cbd1acdf49ccf42b1363f6e6dd57d21d8cb49c4f876eb6d0496b7671da2f \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 7ylqrb.kbxmdfb21np2lwfz \
	--discovery-token-ca-cert-hash sha256:ff15cbd1acdf49ccf42b1363f6e6dd57d21d8cb49c4f876eb6d0496b7671da2f
02:16:23 UTC skipped: [kubesphere-master-2]
02:16:23 UTC success: [kubesphere-master-1]
02:16:23 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
02:16:23 UTC skipped: [kubesphere-master-2]
02:16:23 UTC success: [kubesphere-master-1]
02:16:23 UTC [InitKubernetesModule] Remove master taint
02:16:23 UTC skipped: [kubesphere-master-2]
02:16:23 UTC skipped: [kubesphere-master-1]
02:16:23 UTC [ClusterDNSModule] Generate coredns service
02:16:24 UTC skipped: [kubesphere-master-2]
02:16:24 UTC success: [kubesphere-master-1]
02:16:24 UTC [ClusterDNSModule] Override coredns service
02:16:24 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
02:16:25 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC [ClusterDNSModule] Generate nodelocaldns
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC [ClusterDNSModule] Deploy nodelocaldns
02:16:25 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC [ClusterDNSModule] Generate nodelocaldns configmap
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC [ClusterDNSModule] Apply nodelocaldns configmap
02:16:25 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:16:25 UTC stdout: [kubesphere-master-1]
v1.23.10
02:16:25 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.100.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
02:16:25 UTC stdout: [kubesphere-master-1]
W0920 02:16:25.525587    4557 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
0410e819f7e272d3585f3570bd7f4a027aafee95e85606b232359f3a27bd0018
02:16:25 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:16:25 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:16:25 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:16:25 UTC stdout: [kubesphere-master-1]
q3po9g.13zduiscm16n640a
02:16:25 UTC success: [kubesphere-master-1]
02:16:25 UTC success: [kubesphere-master-2]
02:16:25 UTC [JoinNodesModule] Generate kubeadm config
02:16:25 UTC skipped: [kubesphere-master-1]
02:16:25 UTC success: [kubesphere-master-2]
02:16:25 UTC success: [kubesphere-worker-1]
02:16:25 UTC [JoinNodesModule] Generate audit policy
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC skipped: [kubesphere-master-1]
02:16:25 UTC [JoinNodesModule] Generate audit webhook
02:16:25 UTC skipped: [kubesphere-master-2]
02:16:25 UTC skipped: [kubesphere-master-1]
02:16:25 UTC [JoinNodesModule] Join control-plane node
02:16:45 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:16:38.636833    3901 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:16:38.639552    3901 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.152 127.0.0.1 192.168.100.151 192.168.100.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
02:16:45 UTC skipped: [kubesphere-master-1]
02:16:45 UTC success: [kubesphere-master-2]
02:16:45 UTC [JoinNodesModule] Join worker node
02:16:52 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:16:46.404940    3349 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:16:46.407932    3349 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
02:16:52 UTC skipped: [kubesphere-master-2]
02:16:52 UTC success: [kubesphere-worker-1]
02:16:52 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
02:16:52 UTC skipped: [kubesphere-master-1]
02:16:52 UTC success: [kubesphere-master-2]
02:16:52 UTC [JoinNodesModule] Remove master taint
02:16:52 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
02:16:52 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
02:16:52 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
02:16:52 UTC skipped: [kubesphere-master-1]
02:16:52 UTC success: [kubesphere-master-2]
02:16:52 UTC [JoinNodesModule] Add worker label to all nodes
02:16:52 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
02:16:52 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
02:16:52 UTC success: [kubesphere-master-1]
02:16:52 UTC skipped: [kubesphere-master-2]
02:16:52 UTC [DeployNetworkPluginModule] Generate calico
02:16:52 UTC skipped: [kubesphere-master-2]
02:16:52 UTC success: [kubesphere-master-1]
02:16:52 UTC [DeployNetworkPluginModule] Deploy calico
02:16:54 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
02:16:54 UTC skipped: [kubesphere-master-2]
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC [ConfigureKubernetesModule] Configure kubernetes
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC skipped: [kubesphere-master-2]
02:16:54 UTC [ChownModule] Chown user $HOME/.kube dir
02:16:54 UTC success: [kubesphere-worker-1]
02:16:54 UTC success: [kubesphere-master-2]
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC [AutoRenewCertsModule] Generate k8s certs renew script
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC success: [kubesphere-master-2]
02:16:54 UTC [AutoRenewCertsModule] Generate k8s certs renew service
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC success: [kubesphere-master-2]
02:16:54 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC success: [kubesphere-master-2]
02:16:54 UTC [AutoRenewCertsModule] Enable k8s certs renew service
02:16:54 UTC success: [kubesphere-master-1]
02:16:54 UTC success: [kubesphere-master-2]
02:16:54 UTC [SaveKubeConfigModule] Save kube config as a configmap
02:16:54 UTC success: [LocalHost]
02:16:54 UTC [AddonsModule] Install addons
02:16:54 UTC success: [LocalHost]
02:16:54 UTC Pipeline[CreateClusterPipeline] execute successfully
02:25:03 UTC [GreetingsModule] Greetings
02:25:03 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
02:25:04 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
02:25:04 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
02:25:04 UTC success: [kubesphere-master-2]
02:25:04 UTC success: [kubesphere-master-1]
02:25:04 UTC success: [kubesphere-worker-1]
02:25:04 UTC [NodePreCheckModule] A pre-check on nodes
02:25:04 UTC success: [kubesphere-worker-1]
02:25:04 UTC success: [kubesphere-master-1]
02:25:04 UTC success: [kubesphere-master-2]
02:25:04 UTC [ConfirmModule] Display confirmation form
02:25:04 UTC success: [LocalHost]
02:25:04 UTC [NodeBinariesModule] Download installation binaries
02:25:04 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
02:25:04 UTC message: [localhost]
kubeadm is existed
02:25:04 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
02:25:05 UTC message: [localhost]
kubelet is existed
02:25:05 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
02:25:05 UTC message: [localhost]
kubectl is existed
02:25:05 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
02:25:05 UTC message: [localhost]
helm is existed
02:25:05 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
02:25:05 UTC message: [localhost]
kubecni is existed
02:25:05 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
02:25:05 UTC message: [localhost]
crictl is existed
02:25:05 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
02:25:05 UTC message: [localhost]
etcd is existed
02:25:05 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
02:25:05 UTC message: [localhost]
containerd is existed
02:25:05 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
02:25:05 UTC message: [localhost]
runc is existed
02:25:05 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
02:25:05 UTC message: [localhost]
calicoctl is existed
02:25:05 UTC success: [LocalHost]
02:25:05 UTC [ConfigureOSModule] Get OS release
02:25:05 UTC success: [kubesphere-master-2]
02:25:05 UTC success: [kubesphere-worker-1]
02:25:05 UTC success: [kubesphere-master-1]
02:25:05 UTC [ConfigureOSModule] Prepare to init OS
02:25:06 UTC success: [kubesphere-worker-1]
02:25:06 UTC success: [kubesphere-master-1]
02:25:06 UTC success: [kubesphere-master-2]
02:25:06 UTC [ConfigureOSModule] Generate init os script
02:25:06 UTC success: [kubesphere-master-2]
02:25:06 UTC success: [kubesphere-master-1]
02:25:06 UTC success: [kubesphere-worker-1]
02:25:06 UTC [ConfigureOSModule] Exec init os script
02:25:08 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:25:08 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:25:08 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:25:08 UTC success: [kubesphere-master-2]
02:25:08 UTC success: [kubesphere-worker-1]
02:25:08 UTC success: [kubesphere-master-1]
02:25:08 UTC [ConfigureOSModule] configure the ntp server for each node
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:25:08 UTC success: [kubesphere-master-1]
02:25:08 UTC success: [kubesphere-master-2]
02:25:08 UTC [InstallContainerModule] Sync containerd binaries
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC [InstallContainerModule] Sync crictl binaries
02:25:08 UTC success: [kubesphere-worker-1]
02:25:08 UTC success: [kubesphere-master-2]
02:25:08 UTC success: [kubesphere-master-1]
02:25:08 UTC [InstallContainerModule] Generate containerd service
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC [InstallContainerModule] Generate containerd config
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC [InstallContainerModule] Generate crictl config
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC [InstallContainerModule] Enable containerd
02:25:08 UTC skipped: [kubesphere-worker-1]
02:25:08 UTC skipped: [kubesphere-master-1]
02:25:08 UTC skipped: [kubesphere-master-2]
02:25:08 UTC [PullModule] Start to pull images on all nodes
02:25:08 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
02:25:08 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
02:25:08 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
02:25:10 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:25:10 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:25:10 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:25:13 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:25:13 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
02:25:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:25:15 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:25:15 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:25:15 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:25:17 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:25:17 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
02:25:18 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
02:25:20 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
02:25:20 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
02:25:21 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
02:25:22 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:25:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:25:25 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
02:25:25 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
02:25:25 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
02:25:29 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
02:25:29 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
02:25:29 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:25:33 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
02:25:33 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
02:25:37 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:25:37 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:25:39 UTC success: [kubesphere-worker-1]
02:25:39 UTC success: [kubesphere-master-2]
02:25:39 UTC success: [kubesphere-master-1]
02:25:39 UTC [ETCDPreCheckModule] Get etcd status
02:25:39 UTC success: [kubesphere-master-1]
02:25:39 UTC success: [kubesphere-master-2]
02:25:39 UTC [CertsModule] Fetch etcd certs
02:25:39 UTC success: [kubesphere-master-1]
02:25:39 UTC skipped: [kubesphere-master-2]
02:25:39 UTC [CertsModule] Generate etcd Certs
02:25:39 UTC success: [LocalHost]
02:25:39 UTC [CertsModule] Synchronize certs file
02:25:39 UTC success: [kubesphere-master-1]
02:25:39 UTC success: [kubesphere-master-2]
02:25:39 UTC [CertsModule] Synchronize certs file to master
02:25:39 UTC skipped: [kubesphere-master-1]
02:25:39 UTC skipped: [kubesphere-master-2]
02:25:39 UTC [InstallETCDBinaryModule] Install etcd using binary
02:25:40 UTC success: [kubesphere-master-2]
02:25:40 UTC success: [kubesphere-master-1]
02:25:40 UTC [InstallETCDBinaryModule] Generate etcd service
02:25:40 UTC success: [kubesphere-master-2]
02:25:40 UTC success: [kubesphere-master-1]
02:25:40 UTC [InstallETCDBinaryModule] Generate access address
02:25:40 UTC skipped: [kubesphere-master-2]
02:25:40 UTC success: [kubesphere-master-1]
02:25:40 UTC [ETCDConfigureModule] Health check on exist etcd
02:25:40 UTC skipped: [kubesphere-master-2]
02:25:40 UTC skipped: [kubesphere-master-1]
02:25:40 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
02:25:40 UTC success: [kubesphere-master-1]
02:25:40 UTC success: [kubesphere-master-2]
02:25:40 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
02:25:40 UTC success: [kubesphere-master-1]
02:25:40 UTC success: [kubesphere-master-2]
02:25:40 UTC [ETCDConfigureModule] Restart etcd
02:25:50 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:25:50 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC [ETCDConfigureModule] Health check on all etcd
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC [ETCDConfigureModule] Health check on all etcd
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC [ETCDBackupModule] Backup etcd data regularly
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC [ETCDBackupModule] Generate backup ETCD service
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC [ETCDBackupModule] Generate backup ETCD timer
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC [ETCDBackupModule] Enable backup etcd service
02:25:50 UTC success: [kubesphere-master-1]
02:25:50 UTC success: [kubesphere-master-2]
02:25:50 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
02:25:54 UTC success: [kubesphere-master-2]
02:25:54 UTC success: [kubesphere-worker-1]
02:25:54 UTC success: [kubesphere-master-1]
02:25:54 UTC [InstallKubeBinariesModule] Change kubelet mode
02:25:54 UTC success: [kubesphere-master-1]
02:25:54 UTC success: [kubesphere-worker-1]
02:25:54 UTC success: [kubesphere-master-2]
02:25:54 UTC [InstallKubeBinariesModule] Generate kubelet service
02:25:54 UTC success: [kubesphere-worker-1]
02:25:54 UTC success: [kubesphere-master-1]
02:25:54 UTC success: [kubesphere-master-2]
02:25:54 UTC [InstallKubeBinariesModule] Enable kubelet service
02:25:55 UTC success: [kubesphere-master-1]
02:25:55 UTC success: [kubesphere-master-2]
02:25:55 UTC success: [kubesphere-worker-1]
02:25:55 UTC [InstallKubeBinariesModule] Generate kubelet env
02:25:55 UTC success: [kubesphere-master-2]
02:25:55 UTC success: [kubesphere-worker-1]
02:25:55 UTC success: [kubesphere-master-1]
02:25:55 UTC [InitKubernetesModule] Generate kubeadm config
02:25:55 UTC skipped: [kubesphere-master-2]
02:25:55 UTC success: [kubesphere-master-1]
02:25:55 UTC [InitKubernetesModule] Generate audit policy
02:25:55 UTC skipped: [kubesphere-master-2]
02:25:55 UTC skipped: [kubesphere-master-1]
02:25:55 UTC [InitKubernetesModule] Generate audit webhook
02:25:55 UTC skipped: [kubesphere-master-2]
02:25:55 UTC skipped: [kubesphere-master-1]
02:25:55 UTC [InitKubernetesModule] Init cluster using kubeadm
02:26:11 UTC stdout: [kubesphere-master-1]
W0920 02:25:55.307952    3863 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.151 127.0.0.1 192.168.100.152 192.168.100.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 12.502097 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: jpsfgs.tgji82cebke22y94
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token jpsfgs.tgji82cebke22y94 \
	--discovery-token-ca-cert-hash sha256:eb5e0320c55a5e21a366ad312d59a80991f75f2d9f0539a8e0956040f024cf15 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token jpsfgs.tgji82cebke22y94 \
	--discovery-token-ca-cert-hash sha256:eb5e0320c55a5e21a366ad312d59a80991f75f2d9f0539a8e0956040f024cf15
02:26:11 UTC skipped: [kubesphere-master-2]
02:26:11 UTC success: [kubesphere-master-1]
02:26:11 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
02:26:11 UTC skipped: [kubesphere-master-2]
02:26:11 UTC success: [kubesphere-master-1]
02:26:11 UTC [InitKubernetesModule] Remove master taint
02:26:11 UTC skipped: [kubesphere-master-2]
02:26:11 UTC skipped: [kubesphere-master-1]
02:26:11 UTC [ClusterDNSModule] Generate coredns service
02:26:11 UTC skipped: [kubesphere-master-2]
02:26:11 UTC success: [kubesphere-master-1]
02:26:11 UTC [ClusterDNSModule] Override coredns service
02:26:11 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
02:26:12 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
02:26:12 UTC skipped: [kubesphere-master-2]
02:26:12 UTC success: [kubesphere-master-1]
02:26:12 UTC [ClusterDNSModule] Generate nodelocaldns
02:26:12 UTC skipped: [kubesphere-master-2]
02:26:12 UTC success: [kubesphere-master-1]
02:26:12 UTC [ClusterDNSModule] Deploy nodelocaldns
02:26:12 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
02:26:12 UTC skipped: [kubesphere-master-2]
02:26:12 UTC success: [kubesphere-master-1]
02:26:12 UTC [ClusterDNSModule] Generate nodelocaldns configmap
02:26:12 UTC skipped: [kubesphere-master-2]
02:26:12 UTC success: [kubesphere-master-1]
02:26:12 UTC [ClusterDNSModule] Apply nodelocaldns configmap
02:26:12 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
02:26:12 UTC skipped: [kubesphere-master-2]
02:26:12 UTC success: [kubesphere-master-1]
02:26:12 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:26:12 UTC stdout: [kubesphere-master-1]
v1.23.10
02:26:12 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.100.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
02:26:12 UTC stdout: [kubesphere-master-1]
W0920 02:26:12.784786    4537 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
ad0ccf153ace621f374812d9f15cba0678146ee060ce75225b93a6686897bcb3
02:26:12 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:26:12 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:26:12 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:26:13 UTC stdout: [kubesphere-master-1]
874iur.w8z0084mvdx2cwdw
02:26:13 UTC success: [kubesphere-master-1]
02:26:13 UTC success: [kubesphere-master-2]
02:26:13 UTC [JoinNodesModule] Generate kubeadm config
02:26:13 UTC skipped: [kubesphere-master-1]
02:26:13 UTC success: [kubesphere-worker-1]
02:26:13 UTC success: [kubesphere-master-2]
02:26:13 UTC [JoinNodesModule] Generate audit policy
02:26:13 UTC skipped: [kubesphere-master-2]
02:26:13 UTC skipped: [kubesphere-master-1]
02:26:13 UTC [JoinNodesModule] Generate audit webhook
02:26:13 UTC skipped: [kubesphere-master-2]
02:26:13 UTC skipped: [kubesphere-master-1]
02:26:13 UTC [JoinNodesModule] Join control-plane node
02:26:33 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:26:25.872364    3885 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:26:25.874505    3885 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.152 127.0.0.1 192.168.100.151 192.168.100.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
02:26:33 UTC skipped: [kubesphere-master-1]
02:26:33 UTC success: [kubesphere-master-2]
02:26:33 UTC [JoinNodesModule] Join worker node
02:26:39 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:26:33.598078    3329 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:26:33.600918    3329 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
02:26:39 UTC skipped: [kubesphere-master-2]
02:26:39 UTC success: [kubesphere-worker-1]
02:26:39 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
02:26:39 UTC skipped: [kubesphere-master-1]
02:26:39 UTC success: [kubesphere-master-2]
02:26:39 UTC [JoinNodesModule] Remove master taint
02:26:40 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
02:26:40 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
02:26:40 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
02:26:40 UTC skipped: [kubesphere-master-1]
02:26:40 UTC success: [kubesphere-master-2]
02:26:40 UTC [JoinNodesModule] Add worker label to all nodes
02:26:40 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
02:26:40 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
02:26:40 UTC success: [kubesphere-master-1]
02:26:40 UTC skipped: [kubesphere-master-2]
02:26:40 UTC [DeployNetworkPluginModule] Generate calico
02:26:40 UTC skipped: [kubesphere-master-2]
02:26:40 UTC success: [kubesphere-master-1]
02:26:40 UTC [DeployNetworkPluginModule] Deploy calico
02:26:42 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
02:26:42 UTC skipped: [kubesphere-master-2]
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC [ConfigureKubernetesModule] Configure kubernetes
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC skipped: [kubesphere-master-2]
02:26:42 UTC [ChownModule] Chown user $HOME/.kube dir
02:26:42 UTC success: [kubesphere-worker-1]
02:26:42 UTC success: [kubesphere-master-2]
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC [AutoRenewCertsModule] Generate k8s certs renew script
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC success: [kubesphere-master-2]
02:26:42 UTC [AutoRenewCertsModule] Generate k8s certs renew service
02:26:42 UTC success: [kubesphere-master-2]
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
02:26:42 UTC success: [kubesphere-master-2]
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC [AutoRenewCertsModule] Enable k8s certs renew service
02:26:42 UTC success: [kubesphere-master-1]
02:26:42 UTC success: [kubesphere-master-2]
02:26:42 UTC [SaveKubeConfigModule] Save kube config as a configmap
02:26:42 UTC success: [LocalHost]
02:26:42 UTC [AddonsModule] Install addons
02:26:42 UTC success: [LocalHost]
02:26:42 UTC Pipeline[CreateClusterPipeline] execute successfully
02:56:20 UTC [GreetingsModule] Greetings
02:56:20 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
02:56:21 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
02:56:21 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
02:56:21 UTC success: [kubesphere-worker-1]
02:56:21 UTC success: [kubesphere-master-2]
02:56:21 UTC success: [kubesphere-master-1]
02:56:21 UTC [NodePreCheckModule] A pre-check on nodes
02:56:21 UTC success: [kubesphere-master-2]
02:56:21 UTC success: [kubesphere-worker-1]
02:56:21 UTC success: [kubesphere-master-1]
02:56:21 UTC [ConfirmModule] Display confirmation form
02:56:21 UTC success: [LocalHost]
02:56:21 UTC [NodeBinariesModule] Download installation binaries
02:56:21 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
02:56:21 UTC message: [localhost]
kubeadm is existed
02:56:21 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
02:56:22 UTC message: [localhost]
kubelet is existed
02:56:22 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
02:56:22 UTC message: [localhost]
kubectl is existed
02:56:22 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
02:56:22 UTC message: [localhost]
helm is existed
02:56:22 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
02:56:22 UTC message: [localhost]
kubecni is existed
02:56:22 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
02:56:22 UTC message: [localhost]
crictl is existed
02:56:22 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
02:56:22 UTC message: [localhost]
etcd is existed
02:56:22 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
02:56:22 UTC message: [localhost]
containerd is existed
02:56:22 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
02:56:22 UTC message: [localhost]
runc is existed
02:56:22 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
02:56:22 UTC message: [localhost]
calicoctl is existed
02:56:22 UTC success: [LocalHost]
02:56:22 UTC [ConfigureOSModule] Get OS release
02:56:22 UTC success: [kubesphere-master-1]
02:56:22 UTC success: [kubesphere-master-2]
02:56:22 UTC success: [kubesphere-worker-1]
02:56:22 UTC [ConfigureOSModule] Prepare to init OS
02:56:23 UTC success: [kubesphere-worker-1]
02:56:23 UTC success: [kubesphere-master-1]
02:56:23 UTC success: [kubesphere-master-2]
02:56:23 UTC [ConfigureOSModule] Generate init os script
02:56:23 UTC success: [kubesphere-master-1]
02:56:23 UTC success: [kubesphere-master-2]
02:56:23 UTC success: [kubesphere-worker-1]
02:56:23 UTC [ConfigureOSModule] Exec init os script
02:56:25 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:56:25 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:56:25 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
02:56:25 UTC success: [kubesphere-master-1]
02:56:25 UTC success: [kubesphere-worker-1]
02:56:25 UTC success: [kubesphere-master-2]
02:56:25 UTC [ConfigureOSModule] configure the ntp server for each node
02:56:25 UTC skipped: [kubesphere-worker-1]
02:56:25 UTC skipped: [kubesphere-master-1]
02:56:25 UTC skipped: [kubesphere-master-2]
02:56:25 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:56:25 UTC success: [kubesphere-master-1]
02:56:25 UTC success: [kubesphere-master-2]
02:56:25 UTC [InstallContainerModule] Sync containerd binaries
02:56:25 UTC skipped: [kubesphere-master-2]
02:56:25 UTC skipped: [kubesphere-master-1]
02:56:25 UTC skipped: [kubesphere-worker-1]
02:56:25 UTC [InstallContainerModule] Sync crictl binaries
02:56:26 UTC success: [kubesphere-master-2]
02:56:26 UTC success: [kubesphere-master-1]
02:56:26 UTC success: [kubesphere-worker-1]
02:56:26 UTC [InstallContainerModule] Generate containerd service
02:56:26 UTC skipped: [kubesphere-worker-1]
02:56:26 UTC skipped: [kubesphere-master-1]
02:56:26 UTC skipped: [kubesphere-master-2]
02:56:26 UTC [InstallContainerModule] Generate containerd config
02:56:26 UTC skipped: [kubesphere-master-2]
02:56:26 UTC skipped: [kubesphere-worker-1]
02:56:26 UTC skipped: [kubesphere-master-1]
02:56:26 UTC [InstallContainerModule] Generate crictl config
02:56:26 UTC skipped: [kubesphere-worker-1]
02:56:26 UTC skipped: [kubesphere-master-2]
02:56:26 UTC skipped: [kubesphere-master-1]
02:56:26 UTC [InstallContainerModule] Enable containerd
02:56:26 UTC skipped: [kubesphere-master-2]
02:56:26 UTC skipped: [kubesphere-master-1]
02:56:26 UTC skipped: [kubesphere-worker-1]
02:56:26 UTC [PullModule] Start to pull images on all nodes
02:56:26 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
02:56:26 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
02:56:26 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
02:56:27 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:56:27 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:56:27 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
02:56:30 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:56:30 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
02:56:30 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
02:56:32 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:56:33 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:56:33 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
02:56:35 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
02:56:35 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
02:56:35 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
02:56:37 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
02:56:38 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
02:56:38 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
02:56:39 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:56:40 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
02:56:42 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
02:56:42 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
02:56:43 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
02:56:45 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
02:56:46 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
02:56:46 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:56:50 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
02:56:50 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
02:56:54 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:56:54 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
02:56:56 UTC success: [kubesphere-worker-1]
02:56:56 UTC success: [kubesphere-master-2]
02:56:56 UTC success: [kubesphere-master-1]
02:56:56 UTC [ETCDPreCheckModule] Get etcd status
02:56:56 UTC success: [kubesphere-master-1]
02:56:56 UTC success: [kubesphere-master-2]
02:56:56 UTC [CertsModule] Fetch etcd certs
02:56:56 UTC success: [kubesphere-master-1]
02:56:56 UTC skipped: [kubesphere-master-2]
02:56:56 UTC [CertsModule] Generate etcd Certs
02:56:56 UTC success: [LocalHost]
02:56:56 UTC [CertsModule] Synchronize certs file
02:56:57 UTC success: [kubesphere-master-2]
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC [CertsModule] Synchronize certs file to master
02:56:57 UTC skipped: [kubesphere-master-2]
02:56:57 UTC skipped: [kubesphere-master-1]
02:56:57 UTC [InstallETCDBinaryModule] Install etcd using binary
02:56:57 UTC success: [kubesphere-master-2]
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC [InstallETCDBinaryModule] Generate etcd service
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC success: [kubesphere-master-2]
02:56:57 UTC [InstallETCDBinaryModule] Generate access address
02:56:57 UTC skipped: [kubesphere-master-2]
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC [ETCDConfigureModule] Health check on exist etcd
02:56:57 UTC skipped: [kubesphere-master-2]
02:56:57 UTC skipped: [kubesphere-master-1]
02:56:57 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC success: [kubesphere-master-2]
02:56:57 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
02:56:57 UTC success: [kubesphere-master-1]
02:56:57 UTC success: [kubesphere-master-2]
02:56:57 UTC [ETCDConfigureModule] Restart etcd
02:57:08 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:57:08 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC [ETCDConfigureModule] Health check on all etcd
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [ETCDConfigureModule] Health check on all etcd
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [ETCDBackupModule] Backup etcd data regularly
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [ETCDBackupModule] Generate backup ETCD service
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [ETCDBackupModule] Generate backup ETCD timer
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC [ETCDBackupModule] Enable backup etcd service
02:57:08 UTC success: [kubesphere-master-1]
02:57:08 UTC success: [kubesphere-master-2]
02:57:08 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
02:57:13 UTC success: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC success: [kubesphere-worker-1]
02:57:13 UTC [InstallKubeBinariesModule] Change kubelet mode
02:57:13 UTC success: [kubesphere-worker-1]
02:57:13 UTC success: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC [InstallKubeBinariesModule] Generate kubelet service
02:57:13 UTC success: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-worker-1]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC [InstallKubeBinariesModule] Enable kubelet service
02:57:13 UTC success: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC success: [kubesphere-worker-1]
02:57:13 UTC [InstallKubeBinariesModule] Generate kubelet env
02:57:13 UTC success: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC success: [kubesphere-worker-1]
02:57:13 UTC [InitKubernetesModule] Generate kubeadm config
02:57:13 UTC skipped: [kubesphere-master-2]
02:57:13 UTC success: [kubesphere-master-1]
02:57:13 UTC [InitKubernetesModule] Generate audit policy
02:57:13 UTC skipped: [kubesphere-master-1]
02:57:13 UTC skipped: [kubesphere-master-2]
02:57:13 UTC [InitKubernetesModule] Generate audit webhook
02:57:13 UTC skipped: [kubesphere-master-2]
02:57:13 UTC skipped: [kubesphere-master-1]
02:57:13 UTC [InitKubernetesModule] Init cluster using kubeadm
02:57:29 UTC stdout: [kubesphere-master-1]
W0920 02:57:13.556521    3860 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.151 127.0.0.1 192.168.100.152 192.168.100.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 13.039207 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 1vlnnv.g2uai1spgqk9le1p
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 1vlnnv.g2uai1spgqk9le1p \
	--discovery-token-ca-cert-hash sha256:bf254b6d2218d2b2539cfa87d2248fe9ff85d1a402f89155ccf90824cd74a9d1 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 1vlnnv.g2uai1spgqk9le1p \
	--discovery-token-ca-cert-hash sha256:bf254b6d2218d2b2539cfa87d2248fe9ff85d1a402f89155ccf90824cd74a9d1
02:57:29 UTC skipped: [kubesphere-master-2]
02:57:29 UTC success: [kubesphere-master-1]
02:57:29 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
02:57:29 UTC skipped: [kubesphere-master-2]
02:57:29 UTC success: [kubesphere-master-1]
02:57:29 UTC [InitKubernetesModule] Remove master taint
02:57:29 UTC skipped: [kubesphere-master-2]
02:57:29 UTC skipped: [kubesphere-master-1]
02:57:29 UTC [ClusterDNSModule] Generate coredns service
02:57:30 UTC skipped: [kubesphere-master-2]
02:57:30 UTC success: [kubesphere-master-1]
02:57:30 UTC [ClusterDNSModule] Override coredns service
02:57:30 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
02:57:31 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC [ClusterDNSModule] Generate nodelocaldns
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC [ClusterDNSModule] Deploy nodelocaldns
02:57:31 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC [ClusterDNSModule] Generate nodelocaldns configmap
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC [ClusterDNSModule] Apply nodelocaldns configmap
02:57:31 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC [KubernetesStatusModule] Get kubernetes cluster status
02:57:31 UTC stdout: [kubesphere-master-1]
v1.23.10
02:57:31 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.100.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
02:57:31 UTC stdout: [kubesphere-master-1]
W0920 02:57:31.479513    4535 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
b68cf183865d2bbb88eae7f686d7d3c4dd1bdfee32c79ac299f96b57d93ddcf7
02:57:31 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:57:31 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:57:31 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
02:57:31 UTC stdout: [kubesphere-master-1]
gwwys0.28i5sges2w25vtw0
02:57:31 UTC success: [kubesphere-master-1]
02:57:31 UTC success: [kubesphere-master-2]
02:57:31 UTC [JoinNodesModule] Generate kubeadm config
02:57:31 UTC skipped: [kubesphere-master-1]
02:57:31 UTC success: [kubesphere-master-2]
02:57:31 UTC success: [kubesphere-worker-1]
02:57:31 UTC [JoinNodesModule] Generate audit policy
02:57:31 UTC skipped: [kubesphere-master-1]
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC [JoinNodesModule] Generate audit webhook
02:57:31 UTC skipped: [kubesphere-master-2]
02:57:31 UTC skipped: [kubesphere-master-1]
02:57:31 UTC [JoinNodesModule] Join control-plane node
02:57:51 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:57:44.602786    3893 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:57:44.605804    3893 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.100.152 127.0.0.1 192.168.100.151 192.168.100.181]
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
02:57:51 UTC skipped: [kubesphere-master-1]
02:57:51 UTC success: [kubesphere-master-2]
02:57:51 UTC [JoinNodesModule] Join worker node
02:57:58 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0920 02:57:52.070879    3380 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0920 02:57:52.073357    3380 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
02:57:58 UTC skipped: [kubesphere-master-2]
02:57:58 UTC success: [kubesphere-worker-1]
02:57:58 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
02:57:58 UTC skipped: [kubesphere-master-1]
02:57:58 UTC success: [kubesphere-master-2]
02:57:58 UTC [JoinNodesModule] Remove master taint
02:57:58 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
02:57:58 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
02:57:58 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
02:57:58 UTC skipped: [kubesphere-master-1]
02:57:58 UTC success: [kubesphere-master-2]
02:57:58 UTC [JoinNodesModule] Add worker label to all nodes
02:57:59 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
02:57:59 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
02:57:59 UTC success: [kubesphere-master-1]
02:57:59 UTC skipped: [kubesphere-master-2]
02:57:59 UTC [DeployNetworkPluginModule] Generate calico
02:57:59 UTC skipped: [kubesphere-master-2]
02:57:59 UTC success: [kubesphere-master-1]
02:57:59 UTC [DeployNetworkPluginModule] Deploy calico
02:58:00 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
02:58:00 UTC skipped: [kubesphere-master-2]
02:58:00 UTC success: [kubesphere-master-1]
02:58:00 UTC [ConfigureKubernetesModule] Configure kubernetes
02:58:00 UTC success: [kubesphere-master-1]
02:58:00 UTC skipped: [kubesphere-master-2]
02:58:00 UTC [ChownModule] Chown user $HOME/.kube dir
02:58:00 UTC success: [kubesphere-worker-1]
02:58:00 UTC success: [kubesphere-master-2]
02:58:00 UTC success: [kubesphere-master-1]
02:58:00 UTC [AutoRenewCertsModule] Generate k8s certs renew script
02:58:01 UTC success: [kubesphere-master-2]
02:58:01 UTC success: [kubesphere-master-1]
02:58:01 UTC [AutoRenewCertsModule] Generate k8s certs renew service
02:58:01 UTC success: [kubesphere-master-1]
02:58:01 UTC success: [kubesphere-master-2]
02:58:01 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
02:58:01 UTC success: [kubesphere-master-1]
02:58:01 UTC success: [kubesphere-master-2]
02:58:01 UTC [AutoRenewCertsModule] Enable k8s certs renew service
02:58:01 UTC success: [kubesphere-master-1]
02:58:01 UTC success: [kubesphere-master-2]
02:58:01 UTC [SaveKubeConfigModule] Save kube config as a configmap
02:58:01 UTC success: [LocalHost]
02:58:01 UTC [AddonsModule] Install addons
02:58:01 UTC success: [LocalHost]
02:58:01 UTC Pipeline[CreateClusterPipeline] execute successfully
