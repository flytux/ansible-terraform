11:14:08 UTC [GreetingsModule] Greetings
11:14:09 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:14:09 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:14:10 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:14:10 UTC success: [kubesphere-worker-1]
11:14:10 UTC success: [kubesphere-master-1]
11:14:10 UTC success: [kubesphere-master-2]
11:14:10 UTC [NodePreCheckModule] A pre-check on nodes
11:14:10 UTC success: [kubesphere-worker-1]
11:14:10 UTC success: [kubesphere-master-2]
11:14:10 UTC success: [kubesphere-master-1]
11:14:10 UTC [ConfirmModule] Display confirmation form
11:14:10 UTC success: [LocalHost]
11:14:10 UTC [NodeBinariesModule] Download installation binaries
11:14:10 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
11:14:12 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
11:14:17 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
11:14:20 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
11:14:22 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
11:14:25 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
11:14:27 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
11:14:29 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
11:14:31 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
11:14:32 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
11:14:35 UTC success: [LocalHost]
11:14:35 UTC [ConfigureOSModule] Get OS release
11:14:35 UTC success: [kubesphere-master-1]
11:14:35 UTC success: [kubesphere-worker-1]
11:14:35 UTC success: [kubesphere-master-2]
11:14:35 UTC [ConfigureOSModule] Prepare to init OS
11:14:35 UTC success: [kubesphere-master-1]
11:14:35 UTC success: [kubesphere-master-2]
11:14:35 UTC success: [kubesphere-worker-1]
11:14:35 UTC [ConfigureOSModule] Generate init os script
11:14:35 UTC success: [kubesphere-master-1]
11:14:35 UTC success: [kubesphere-master-2]
11:14:35 UTC success: [kubesphere-worker-1]
11:14:35 UTC [ConfigureOSModule] Exec init os script
11:14:36 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:14:36 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:14:36 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:14:36 UTC success: [kubesphere-master-1]
11:14:36 UTC success: [kubesphere-master-2]
11:14:36 UTC success: [kubesphere-worker-1]
11:14:36 UTC [ConfigureOSModule] configure the ntp server for each node
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC [KubernetesStatusModule] Get kubernetes cluster status
11:14:36 UTC success: [kubesphere-master-1]
11:14:36 UTC success: [kubesphere-master-2]
11:14:36 UTC [InstallContainerModule] Sync containerd binaries
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC [InstallContainerModule] Sync crictl binaries
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC [InstallContainerModule] Generate containerd service
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC [InstallContainerModule] Generate containerd config
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC [InstallContainerModule] Generate crictl config
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC [InstallContainerModule] Enable containerd
11:14:36 UTC skipped: [kubesphere-master-2]
11:14:36 UTC skipped: [kubesphere-master-1]
11:14:36 UTC skipped: [kubesphere-worker-1]
11:14:36 UTC [PullModule] Start to pull images on all nodes
11:14:36 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
11:14:36 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
11:14:36 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
11:14:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:14:38 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:14:38 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:14:39 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:14:39 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:14:39 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
11:14:40 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:14:41 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:14:41 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:14:42 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:14:42 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
11:14:42 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
11:14:44 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
11:14:44 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
11:14:44 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
11:14:45 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:14:45 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:14:45 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
11:14:47 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
11:14:47 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
11:14:47 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:14:48 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
11:14:48 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
11:14:49 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
11:14:50 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
11:14:51 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:14:51 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:14:53 UTC success: [kubesphere-worker-1]
11:14:53 UTC success: [kubesphere-master-1]
11:14:53 UTC success: [kubesphere-master-2]
11:14:53 UTC [ETCDPreCheckModule] Get etcd status
11:14:53 UTC stdout: [kubesphere-master-1]
ETCD_NAME=etcd-kubesphere-master-1
11:14:53 UTC stdout: [kubesphere-master-2]
ETCD_NAME=etcd-kubesphere-master-2
11:14:53 UTC success: [kubesphere-master-1]
11:14:53 UTC success: [kubesphere-master-2]
11:14:53 UTC [CertsModule] Fetch etcd certs
11:14:53 UTC success: [kubesphere-master-1]
11:14:53 UTC skipped: [kubesphere-master-2]
11:14:53 UTC [CertsModule] Generate etcd Certs
11:14:53 UTC success: [LocalHost]
11:14:53 UTC [CertsModule] Synchronize certs file
11:14:54 UTC success: [kubesphere-master-1]
11:14:54 UTC success: [kubesphere-master-2]
11:14:54 UTC [CertsModule] Synchronize certs file to master
11:14:54 UTC skipped: [kubesphere-master-2]
11:14:54 UTC skipped: [kubesphere-master-1]
11:14:54 UTC [InstallETCDBinaryModule] Install etcd using binary
11:14:54 UTC success: [kubesphere-master-2]
11:14:54 UTC success: [kubesphere-master-1]
11:14:54 UTC [InstallETCDBinaryModule] Generate etcd service
11:14:54 UTC success: [kubesphere-master-2]
11:14:54 UTC success: [kubesphere-master-1]
11:14:54 UTC [InstallETCDBinaryModule] Generate access address
11:14:54 UTC skipped: [kubesphere-master-2]
11:14:54 UTC success: [kubesphere-master-1]
11:14:54 UTC [ETCDConfigureModule] Health check on exist etcd
11:14:58 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout: Process exited with status 1
11:14:58 UTC retry: [kubesphere-master-1]
11:14:58 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout: Process exited with status 1
11:14:58 UTC retry: [kubesphere-master-2]
11:15:05 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: read tcp 192.168.100.151:45904->192.168.100.151:2379: read: connection reset by peer
; error #1: read tcp 192.168.100.151:55810->192.168.100.152:2379: read: connection reset by peer

error #0: read tcp 192.168.100.151:45904->192.168.100.151:2379: read: connection reset by peer
error #1: read tcp 192.168.100.151:55810->192.168.100.152:2379: read: connection reset by peer: Process exited with status 1
11:15:05 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: read tcp 192.168.100.152:51958->192.168.100.152:2379: read: connection reset by peer
; error #1: dial tcp 192.168.100.151:2379: connect: connection refused

error #0: read tcp 192.168.100.152:51958->192.168.100.152:2379: read: connection reset by peer
error #1: dial tcp 192.168.100.151:2379: connect: connection refused: Process exited with status 1
11:15:05 UTC retry: [kubesphere-master-1]
11:15:05 UTC retry: [kubesphere-master-2]
11:15:10 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.100.151:2379: connect: connection refused
; error #1: dial tcp 192.168.100.152:2379: connect: connection refused

error #0: dial tcp 192.168.100.151:2379: connect: connection refused
error #1: dial tcp 192.168.100.152:2379: connect: connection refused: Process exited with status 1
11:15:10 UTC retry: [kubesphere-master-1]
11:15:10 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.100.151:2379: connect: connection refused
; error #1: dial tcp 192.168.100.152:2379: connect: connection refused

error #0: dial tcp 192.168.100.151:2379: connect: connection refused
error #1: dial tcp 192.168.100.152:2379: connect: connection refused: Process exited with status 1
11:15:10 UTC retry: [kubesphere-master-2]
11:15:15 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.100.152:2379: connect: connection refused
; error #1: dial tcp 192.168.100.151:2379: connect: connection refused

error #0: dial tcp 192.168.100.152:2379: connect: connection refused
error #1: dial tcp 192.168.100.151:2379: connect: connection refused: Process exited with status 1
11:15:15 UTC retry: [kubesphere-master-1]
11:15:15 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 192.168.100.151:2379: connect: connection refused
; error #1: dial tcp 192.168.100.152:2379: connect: connection refused

error #0: dial tcp 192.168.100.151:2379: connect: connection refused
error #1: dial tcp 192.168.100.152:2379: connect: connection refused: Process exited with status 1
11:15:15 UTC retry: [kubesphere-master-2]
11:15:24 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout: Process exited with status 1
11:15:24 UTC retry: [kubesphere-master-1]
11:15:24 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout: Process exited with status 1
11:15:24 UTC retry: [kubesphere-master-2]
11:15:33 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout: Process exited with status 1
11:15:33 UTC retry: [kubesphere-master-1]
11:15:33 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout: Process exited with status 1
11:15:33 UTC retry: [kubesphere-master-2]
11:15:53 UTC [GreetingsModule] Greetings
11:15:53 UTC message: [kw01-base]
Greetings, KubeKey!
11:15:53 UTC success: [kw01-base]
11:15:53 UTC [DeleteClusterConfirmModule] Display confirmation form
11:15:55 UTC success: [LocalHost]
11:15:55 UTC [ResetClusterModule] Reset the cluster using kubeadm
11:15:55 UTC stdout: [kw01-base]
/bin/bash: line 1: /usr/local/bin/kubeadm: No such file or directory
11:15:55 UTC success: [kw01-base]
11:15:55 UTC [ClearOSModule] Reset os network config
11:15:55 UTC stdout: [kw01-base]
/bin/bash: line 1: ipvsadm: command not found
11:15:55 UTC stdout: [kw01-base]
Cannot find device "kube-ipvs0"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "nodelocaldns"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "cni0"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "flannel.1"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "flannel-v6.1"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "flannel-wg"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "flannel-wg-v6"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "cilium_host"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "cilium_vxlan"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "vxlan.calico"
11:15:55 UTC stdout: [kw01-base]
Cannot find device "vxlan-v6.calico"
11:15:55 UTC success: [kw01-base]
11:15:55 UTC [ClearOSModule] Uninstall etcd
11:15:55 UTC success: [kw01-base]
11:15:55 UTC [ClearOSModule] Remove cluster files
11:15:56 UTC success: [kw01-base]
11:15:56 UTC [ClearOSModule] Systemd daemon reload
11:15:56 UTC success: [kw01-base]
11:15:56 UTC [UninstallAutoRenewCertsModule] UnInstall auto renew control-plane certs
11:15:56 UTC skipped: [kw01-base]
11:15:56 UTC Pipeline[DeleteClusterPipeline] execute successfully
11:16:06 UTC [GreetingsModule] Greetings
11:16:06 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:16:07 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:16:07 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:16:07 UTC success: [kubesphere-worker-1]
11:16:07 UTC success: [kubesphere-master-1]
11:16:07 UTC success: [kubesphere-master-2]
11:16:07 UTC [NodePreCheckModule] A pre-check on nodes
11:16:08 UTC success: [kubesphere-worker-1]
11:16:08 UTC success: [kubesphere-master-1]
11:16:08 UTC success: [kubesphere-master-2]
11:16:08 UTC [ConfirmModule] Display confirmation form
11:16:08 UTC success: [LocalHost]
11:16:08 UTC [NodeBinariesModule] Download installation binaries
11:16:08 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
11:16:08 UTC message: [localhost]
kubeadm is existed
11:16:08 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
11:16:08 UTC message: [localhost]
kubelet is existed
11:16:08 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
11:16:08 UTC message: [localhost]
kubectl is existed
11:16:08 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
11:16:08 UTC message: [localhost]
helm is existed
11:16:08 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
11:16:08 UTC message: [localhost]
kubecni is existed
11:16:08 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
11:16:08 UTC message: [localhost]
crictl is existed
11:16:08 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
11:16:08 UTC message: [localhost]
etcd is existed
11:16:08 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
11:16:08 UTC message: [localhost]
containerd is existed
11:16:08 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
11:16:08 UTC message: [localhost]
runc is existed
11:16:08 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
11:16:08 UTC message: [localhost]
calicoctl is existed
11:16:08 UTC success: [LocalHost]
11:16:08 UTC [ConfigureOSModule] Get OS release
11:16:08 UTC success: [kubesphere-worker-1]
11:16:08 UTC success: [kubesphere-master-2]
11:16:08 UTC success: [kubesphere-master-1]
11:16:08 UTC [ConfigureOSModule] Prepare to init OS
11:16:09 UTC success: [kubesphere-master-2]
11:16:09 UTC success: [kubesphere-worker-1]
11:16:09 UTC success: [kubesphere-master-1]
11:16:09 UTC [ConfigureOSModule] Generate init os script
11:16:09 UTC success: [kubesphere-master-2]
11:16:09 UTC success: [kubesphere-master-1]
11:16:09 UTC success: [kubesphere-worker-1]
11:16:09 UTC [ConfigureOSModule] Exec init os script
11:16:09 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:16:09 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:16:09 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:16:09 UTC success: [kubesphere-worker-1]
11:16:09 UTC success: [kubesphere-master-2]
11:16:09 UTC success: [kubesphere-master-1]
11:16:09 UTC [ConfigureOSModule] configure the ntp server for each node
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC [KubernetesStatusModule] Get kubernetes cluster status
11:16:09 UTC success: [kubesphere-master-1]
11:16:09 UTC success: [kubesphere-master-2]
11:16:09 UTC [InstallContainerModule] Sync containerd binaries
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC [InstallContainerModule] Sync crictl binaries
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC [InstallContainerModule] Generate containerd service
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC [InstallContainerModule] Generate containerd config
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC [InstallContainerModule] Generate crictl config
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC [InstallContainerModule] Enable containerd
11:16:09 UTC skipped: [kubesphere-master-2]
11:16:09 UTC skipped: [kubesphere-master-1]
11:16:09 UTC skipped: [kubesphere-worker-1]
11:16:09 UTC [PullModule] Start to pull images on all nodes
11:16:09 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
11:16:09 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
11:16:09 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
11:16:11 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:16:11 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:16:11 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:16:12 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:16:12 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:16:13 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
11:16:14 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:16:14 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:16:14 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:16:15 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:16:16 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
11:16:16 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
11:16:17 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
11:16:17 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
11:16:17 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
11:16:18 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:16:18 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
11:16:19 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:16:20 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
11:16:20 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:16:20 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
11:16:21 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
11:16:22 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
11:16:23 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
11:16:23 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
11:16:25 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:16:25 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:16:26 UTC success: [kubesphere-worker-1]
11:16:26 UTC success: [kubesphere-master-1]
11:16:26 UTC success: [kubesphere-master-2]
11:16:26 UTC [ETCDPreCheckModule] Get etcd status
11:16:26 UTC stdout: [kubesphere-master-1]
ETCD_NAME=etcd-kubesphere-master-1
11:16:26 UTC stdout: [kubesphere-master-2]
ETCD_NAME=etcd-kubesphere-master-2
11:16:26 UTC success: [kubesphere-master-1]
11:16:26 UTC success: [kubesphere-master-2]
11:16:26 UTC [CertsModule] Fetch etcd certs
11:16:26 UTC success: [kubesphere-master-1]
11:16:26 UTC skipped: [kubesphere-master-2]
11:16:26 UTC [CertsModule] Generate etcd Certs
11:16:26 UTC success: [LocalHost]
11:16:26 UTC [CertsModule] Synchronize certs file
11:16:27 UTC success: [kubesphere-master-1]
11:16:27 UTC success: [kubesphere-master-2]
11:16:27 UTC [CertsModule] Synchronize certs file to master
11:16:27 UTC skipped: [kubesphere-master-1]
11:16:27 UTC skipped: [kubesphere-master-2]
11:16:27 UTC [InstallETCDBinaryModule] Install etcd using binary
11:16:27 UTC success: [kubesphere-master-2]
11:16:27 UTC success: [kubesphere-master-1]
11:16:27 UTC [InstallETCDBinaryModule] Generate etcd service
11:16:27 UTC success: [kubesphere-master-2]
11:16:27 UTC success: [kubesphere-master-1]
11:16:27 UTC [InstallETCDBinaryModule] Generate access address
11:16:27 UTC skipped: [kubesphere-master-2]
11:16:27 UTC success: [kubesphere-master-1]
11:16:27 UTC [ETCDConfigureModule] Health check on exist etcd
11:16:31 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout: Process exited with status 1
11:16:31 UTC retry: [kubesphere-master-1]
11:16:31 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout: Process exited with status 1
11:16:31 UTC retry: [kubesphere-master-2]
11:16:40 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.152:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.151:2379 exceeded header timeout: Process exited with status 1
11:16:40 UTC retry: [kubesphere-master-1]
11:16:40 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
; error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout

error #0: client: endpoint https://192.168.100.151:2379 exceeded header timeout
error #1: client: endpoint https://192.168.100.152:2379 exceeded header timeout: Process exited with status 1
11:16:40 UTC retry: [kubesphere-master-2]
11:16:45 UTC message: [kubesphere-master-2]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-2-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: read tcp 192.168.100.152:47812->192.168.100.151:2379: read: connection reset by peer
; error #1: read tcp 192.168.100.152:52266->192.168.100.152:2379: read: connection reset by peer

error #0: read tcp 192.168.100.152:47812->192.168.100.151:2379: read: connection reset by peer
error #1: read tcp 192.168.100.152:52266->192.168.100.152:2379: read: connection reset by peer: Process exited with status 1
11:16:45 UTC retry: [kubesphere-master-2]
11:16:45 UTC message: [kubesphere-master-1]
etcd health check failed: Failed to exec command: sudo -E /bin/bash -c "export ETCDCTL_API=2;export ETCDCTL_CERT_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1.pem';export ETCDCTL_KEY_FILE='/etc/ssl/etcd/ssl/admin-kubesphere-master-1-key.pem';export ETCDCTL_CA_FILE='/etc/ssl/etcd/ssl/ca.pem';/usr/local/bin/etcdctl --endpoints=https://192.168.100.151:2379,https://192.168.100.152:2379 cluster-health | grep -q 'cluster is healthy'" 
Error:  client: etcd cluster is unavailable or misconfigured; error #0: read tcp 192.168.100.151:54926->192.168.100.152:2379: read: connection reset by peer
; error #1: dial tcp 192.168.100.151:2379: connect: connection refused

error #0: read tcp 192.168.100.151:54926->192.168.100.152:2379: read: connection reset by peer
error #1: dial tcp 192.168.100.151:2379: connect: connection refused: Process exited with status 1
11:16:45 UTC retry: [kubesphere-master-1]
11:35:04 UTC [GreetingsModule] Greetings
11:35:05 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:35:05 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:35:05 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:35:05 UTC success: [kubesphere-worker-1]
11:35:05 UTC success: [kubesphere-master-1]
11:35:05 UTC success: [kubesphere-master-2]
11:35:05 UTC [NodePreCheckModule] A pre-check on nodes
11:35:05 UTC success: [kubesphere-master-2]
11:35:05 UTC success: [kubesphere-worker-1]
11:35:05 UTC success: [kubesphere-master-1]
11:35:05 UTC [ConfirmModule] Display confirmation form
11:35:05 UTC [ERRO] kubesphere-master-1: conntrack is required.
11:35:05 UTC [ERRO] kubesphere-master-1: socat is required.
11:35:05 UTC [ERRO] kubesphere-master-2: conntrack is required.
11:35:05 UTC [ERRO] kubesphere-master-2: socat is required.
11:35:05 UTC [ERRO] kubesphere-worker-1: conntrack is required.
11:35:05 UTC [ERRO] kubesphere-worker-1: socat is required.
11:38:21 UTC [GreetingsModule] Greetings
11:38:21 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:38:22 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:38:22 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:38:22 UTC success: [kubesphere-worker-1]
11:38:22 UTC success: [kubesphere-master-1]
11:38:22 UTC success: [kubesphere-master-2]
11:38:22 UTC [NodePreCheckModule] A pre-check on nodes
11:38:22 UTC success: [kubesphere-worker-1]
11:38:22 UTC success: [kubesphere-master-2]
11:38:22 UTC success: [kubesphere-master-1]
11:38:22 UTC [ConfirmModule] Display confirmation form
11:38:22 UTC [ERRO] kubesphere-master-1: conntrack is required.
11:38:22 UTC [ERRO] kubesphere-master-1: socat is required.
11:38:22 UTC [ERRO] kubesphere-master-2: conntrack is required.
11:38:22 UTC [ERRO] kubesphere-master-2: socat is required.
11:38:22 UTC [ERRO] kubesphere-worker-1: conntrack is required.
11:38:22 UTC [ERRO] kubesphere-worker-1: socat is required.
11:40:03 UTC [GreetingsModule] Greetings
11:40:04 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:40:04 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:40:04 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:40:04 UTC success: [kubesphere-master-1]
11:40:04 UTC success: [kubesphere-worker-1]
11:40:04 UTC success: [kubesphere-master-2]
11:40:04 UTC [NodePreCheckModule] A pre-check on nodes
11:40:04 UTC success: [kubesphere-master-1]
11:40:04 UTC success: [kubesphere-worker-1]
11:40:04 UTC success: [kubesphere-master-2]
11:40:04 UTC [ConfirmModule] Display confirmation form
11:40:04 UTC success: [LocalHost]
11:40:04 UTC [NodeBinariesModule] Download installation binaries
11:40:04 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
11:40:05 UTC message: [localhost]
kubeadm is existed
11:40:05 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
11:40:05 UTC message: [localhost]
kubelet is existed
11:40:05 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
11:40:05 UTC message: [localhost]
kubectl is existed
11:40:05 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
11:40:05 UTC message: [localhost]
helm is existed
11:40:05 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
11:40:05 UTC message: [localhost]
kubecni is existed
11:40:05 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
11:40:05 UTC message: [localhost]
crictl is existed
11:40:05 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
11:40:05 UTC message: [localhost]
etcd is existed
11:40:05 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
11:40:05 UTC message: [localhost]
containerd is existed
11:40:05 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
11:40:05 UTC message: [localhost]
runc is existed
11:40:05 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
11:40:05 UTC message: [localhost]
calicoctl is existed
11:40:05 UTC success: [LocalHost]
11:40:05 UTC [ConfigureOSModule] Get OS release
11:40:05 UTC success: [kubesphere-worker-1]
11:40:05 UTC success: [kubesphere-master-2]
11:40:05 UTC success: [kubesphere-master-1]
11:40:05 UTC [ConfigureOSModule] Prepare to init OS
11:40:06 UTC success: [kubesphere-worker-1]
11:40:06 UTC success: [kubesphere-master-1]
11:40:06 UTC success: [kubesphere-master-2]
11:40:06 UTC [ConfigureOSModule] Generate init os script
11:40:06 UTC success: [kubesphere-master-2]
11:40:06 UTC success: [kubesphere-worker-1]
11:40:06 UTC success: [kubesphere-master-1]
11:40:06 UTC [ConfigureOSModule] Exec init os script
11:40:07 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:40:07 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:40:07 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:40:07 UTC success: [kubesphere-master-2]
11:40:07 UTC success: [kubesphere-worker-1]
11:40:07 UTC success: [kubesphere-master-1]
11:40:07 UTC [ConfigureOSModule] configure the ntp server for each node
11:40:07 UTC skipped: [kubesphere-master-1]
11:40:07 UTC skipped: [kubesphere-worker-1]
11:40:07 UTC skipped: [kubesphere-master-2]
11:40:07 UTC [KubernetesStatusModule] Get kubernetes cluster status
11:40:07 UTC success: [kubesphere-master-2]
11:40:07 UTC success: [kubesphere-master-1]
11:40:07 UTC [InstallContainerModule] Sync containerd binaries
11:40:08 UTC success: [kubesphere-master-1]
11:40:08 UTC success: [kubesphere-master-2]
11:40:08 UTC success: [kubesphere-worker-1]
11:40:08 UTC [InstallContainerModule] Sync crictl binaries
11:40:08 UTC success: [kubesphere-master-1]
11:40:08 UTC success: [kubesphere-master-2]
11:40:08 UTC success: [kubesphere-worker-1]
11:40:08 UTC [InstallContainerModule] Generate containerd service
11:40:08 UTC success: [kubesphere-worker-1]
11:40:08 UTC success: [kubesphere-master-2]
11:40:08 UTC success: [kubesphere-master-1]
11:40:08 UTC [InstallContainerModule] Generate containerd config
11:40:08 UTC success: [kubesphere-master-1]
11:40:08 UTC success: [kubesphere-worker-1]
11:40:08 UTC success: [kubesphere-master-2]
11:40:08 UTC [InstallContainerModule] Generate crictl config
11:40:08 UTC success: [kubesphere-worker-1]
11:40:08 UTC success: [kubesphere-master-2]
11:40:08 UTC success: [kubesphere-master-1]
11:40:08 UTC [InstallContainerModule] Enable containerd
11:40:09 UTC success: [kubesphere-master-1]
11:40:09 UTC success: [kubesphere-master-2]
11:40:09 UTC success: [kubesphere-worker-1]
11:40:09 UTC [PullModule] Start to pull images on all nodes
11:40:09 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
11:40:09 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
11:40:09 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
11:40:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:40:13 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:40:14 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
11:40:21 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:40:21 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
11:40:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
11:40:27 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:40:28 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:40:28 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
11:40:33 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
11:40:35 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
11:40:35 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
11:40:43 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
11:40:43 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
11:40:44 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
11:40:48 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:40:50 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
11:40:53 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
11:40:55 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
11:40:58 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
11:41:01 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:41:04 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
11:41:07 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
11:41:12 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
11:41:17 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
11:41:20 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:41:25 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
11:41:31 UTC success: [kubesphere-worker-1]
11:41:31 UTC success: [kubesphere-master-1]
11:41:31 UTC success: [kubesphere-master-2]
11:41:31 UTC [ETCDPreCheckModule] Get etcd status
11:41:31 UTC success: [kubesphere-master-2]
11:41:31 UTC success: [kubesphere-master-1]
11:41:31 UTC [CertsModule] Fetch etcd certs
11:41:31 UTC success: [kubesphere-master-2]
11:41:31 UTC skipped: [kubesphere-master-1]
11:41:31 UTC [CertsModule] Generate etcd Certs
11:41:31 UTC success: [LocalHost]
11:41:31 UTC [CertsModule] Synchronize certs file
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC success: [kubesphere-master-1]
11:41:32 UTC [CertsModule] Synchronize certs file to master
11:41:32 UTC skipped: [kubesphere-master-1]
11:41:32 UTC skipped: [kubesphere-master-2]
11:41:32 UTC [InstallETCDBinaryModule] Install etcd using binary
11:41:32 UTC success: [kubesphere-master-1]
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC [InstallETCDBinaryModule] Generate etcd service
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC success: [kubesphere-master-1]
11:41:32 UTC [InstallETCDBinaryModule] Generate access address
11:41:32 UTC skipped: [kubesphere-master-1]
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC [ETCDConfigureModule] Health check on exist etcd
11:41:32 UTC skipped: [kubesphere-master-1]
11:41:32 UTC skipped: [kubesphere-master-2]
11:41:32 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC success: [kubesphere-master-1]
11:41:32 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
11:41:32 UTC success: [kubesphere-master-2]
11:41:32 UTC success: [kubesphere-master-1]
11:41:32 UTC [ETCDConfigureModule] Restart etcd
11:41:38 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
11:41:38 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC [ETCDConfigureModule] Health check on all etcd
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC [ETCDConfigureModule] Health check on all etcd
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC [ETCDBackupModule] Backup etcd data regularly
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC [ETCDBackupModule] Generate backup ETCD service
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC [ETCDBackupModule] Generate backup ETCD timer
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC [ETCDBackupModule] Enable backup etcd service
11:41:38 UTC success: [kubesphere-master-2]
11:41:38 UTC success: [kubesphere-master-1]
11:41:38 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
11:41:42 UTC success: [kubesphere-worker-1]
11:41:42 UTC success: [kubesphere-master-2]
11:41:42 UTC success: [kubesphere-master-1]
11:41:42 UTC [InstallKubeBinariesModule] Change kubelet mode
11:41:42 UTC success: [kubesphere-master-1]
11:41:42 UTC success: [kubesphere-master-2]
11:41:42 UTC success: [kubesphere-worker-1]
11:41:42 UTC [InstallKubeBinariesModule] Generate kubelet service
11:41:42 UTC success: [kubesphere-master-1]
11:41:42 UTC success: [kubesphere-worker-1]
11:41:42 UTC success: [kubesphere-master-2]
11:41:42 UTC [InstallKubeBinariesModule] Enable kubelet service
11:41:43 UTC success: [kubesphere-master-2]
11:41:43 UTC success: [kubesphere-master-1]
11:41:43 UTC success: [kubesphere-worker-1]
11:41:43 UTC [InstallKubeBinariesModule] Generate kubelet env
11:41:43 UTC success: [kubesphere-master-1]
11:41:43 UTC success: [kubesphere-worker-1]
11:41:43 UTC success: [kubesphere-master-2]
11:41:43 UTC [InitKubernetesModule] Generate kubeadm config
11:41:43 UTC skipped: [kubesphere-master-1]
11:41:43 UTC success: [kubesphere-master-2]
11:41:43 UTC [InitKubernetesModule] Generate audit policy
11:41:43 UTC skipped: [kubesphere-master-1]
11:41:43 UTC skipped: [kubesphere-master-2]
11:41:43 UTC [InitKubernetesModule] Generate audit webhook
11:41:43 UTC skipped: [kubesphere-master-1]
11:41:43 UTC skipped: [kubesphere-master-2]
11:41:43 UTC [InitKubernetesModule] Init cluster using kubeadm
11:45:45 UTC stdout: [kubesphere-master-2]
W0917 11:41:43.337736    3864 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

	Unfortunately, an error has occurred:
		timed out waiting for the condition

	This error is likely caused by:
		- The kubelet is not running
		- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
		- 'systemctl status kubelet'
		- 'journalctl -xeu kubelet'

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl:
		- 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
		Once you have found the failing container, you can inspect its logs with:
		- 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'

error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher
11:45:46 UTC stdout: [kubesphere-master-2]
[reset] Reading configuration from the cluster...
[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0917 11:45:46.019600    4392 reset.go:101] [reset] Unable to fetch the kubeadm-config ConfigMap from cluster: failed to get config map: Get "https://lb.kubesphere.local:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 192.168.122.151:6443: connect: connection refused
[preflight] Running pre-flight checks
W0917 11:45:46.019664    4392 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
11:45:46 UTC message: [kubesphere-master-2]
init kubernetes cluster failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubeadm init --config=/etc/kubernetes/kubeadm-config.yaml --ignore-preflight-errors=FileExisting-crictl,ImagePull" 
W0917 11:41:43.337736    3864 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

	Unfortunately, an error has occurred:
		timed out waiting for the condition

	This error is likely caused by:
		- The kubelet is not running
		- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

	If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
		- 'systemctl status kubelet'
		- 'journalctl -xeu kubelet'

	Additionally, a control plane component may have crashed or exited when started by the container runtime.
	To troubleshoot, list all containers using your preferred container runtimes CLI.

	Here is one example how you may list all Kubernetes containers running in cri-o/containerd using crictl:
		- 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
		Once you have found the failing container, you can inspect its logs with:
		- 'crictl --runtime-endpoint unix:///run/containerd/containerd.sock logs CONTAINERID'

error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster
To see the stack trace of this error execute with --v=5 or higher: Process exited with status 1
11:45:46 UTC retry: [kubesphere-master-2]
11:46:43 UTC [GreetingsModule] Greetings
11:46:43 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:46:43 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:46:44 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:46:44 UTC success: [kubesphere-worker-1]
11:46:44 UTC success: [kubesphere-master-1]
11:46:44 UTC success: [kubesphere-master-2]
11:46:44 UTC [NodePreCheckModule] A pre-check on nodes
11:46:44 UTC success: [kubesphere-master-2]
11:46:44 UTC success: [kubesphere-worker-1]
11:46:44 UTC success: [kubesphere-master-1]
11:46:44 UTC [ConfirmModule] Display confirmation form
11:46:44 UTC success: [LocalHost]
11:46:44 UTC [NodeBinariesModule] Download installation binaries
11:46:44 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
11:46:44 UTC message: [localhost]
kubeadm is existed
11:46:44 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
11:46:44 UTC message: [localhost]
kubelet is existed
11:46:44 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
11:46:44 UTC message: [localhost]
kubectl is existed
11:46:44 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
11:46:45 UTC message: [localhost]
helm is existed
11:46:45 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
11:46:45 UTC message: [localhost]
kubecni is existed
11:46:45 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
11:46:45 UTC message: [localhost]
crictl is existed
11:46:45 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
11:46:45 UTC message: [localhost]
etcd is existed
11:46:45 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
11:46:45 UTC message: [localhost]
containerd is existed
11:46:45 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
11:46:45 UTC message: [localhost]
runc is existed
11:46:45 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
11:46:45 UTC message: [localhost]
calicoctl is existed
11:46:45 UTC success: [LocalHost]
11:46:45 UTC [ConfigureOSModule] Get OS release
11:46:45 UTC success: [kubesphere-worker-1]
11:46:45 UTC success: [kubesphere-master-1]
11:46:45 UTC success: [kubesphere-master-2]
11:46:45 UTC [ConfigureOSModule] Prepare to init OS
11:46:45 UTC success: [kubesphere-worker-1]
11:46:45 UTC success: [kubesphere-master-1]
11:46:45 UTC success: [kubesphere-master-2]
11:46:45 UTC [ConfigureOSModule] Generate init os script
11:46:45 UTC success: [kubesphere-master-1]
11:46:45 UTC success: [kubesphere-master-2]
11:46:45 UTC success: [kubesphere-worker-1]
11:46:45 UTC [ConfigureOSModule] Exec init os script
11:46:46 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:46:46 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:46:46 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:46:46 UTC success: [kubesphere-master-1]
11:46:46 UTC success: [kubesphere-worker-1]
11:46:46 UTC success: [kubesphere-master-2]
11:46:46 UTC [ConfigureOSModule] configure the ntp server for each node
11:46:46 UTC skipped: [kubesphere-worker-1]
11:46:46 UTC skipped: [kubesphere-master-1]
11:46:46 UTC skipped: [kubesphere-master-2]
11:46:46 UTC [KubernetesStatusModule] Get kubernetes cluster status
11:46:46 UTC stdout: [kubesphere-master-2]
v1.23.10
11:46:46 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:46:46 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:46:46 UTC retry: [kubesphere-master-2]
11:46:51 UTC stdout: [kubesphere-master-2]
v1.23.10
11:46:51 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:46:51 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:46:51 UTC retry: [kubesphere-master-2]
11:46:56 UTC stdout: [kubesphere-master-2]
v1.23.10
11:46:56 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:46:56 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:46:56 UTC success: [kubesphere-master-1]
11:46:56 UTC failed: [kubesphere-master-2]
11:47:11 UTC [GreetingsModule] Greetings
11:47:11 UTC message: [kw01-base]
Greetings, KubeKey!
11:47:11 UTC success: [kw01-base]
11:47:11 UTC [DeleteClusterConfirmModule] Display confirmation form
11:47:13 UTC success: [LocalHost]
11:47:13 UTC [ResetClusterModule] Reset the cluster using kubeadm
11:47:13 UTC stdout: [kw01-base]
/bin/bash: line 1: /usr/local/bin/kubeadm: No such file or directory
11:47:13 UTC success: [kw01-base]
11:47:13 UTC [ClearOSModule] Reset os network config
11:47:13 UTC stdout: [kw01-base]
/bin/bash: line 1: ipvsadm: command not found
11:47:13 UTC stdout: [kw01-base]
Cannot find device "kube-ipvs0"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "nodelocaldns"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "cni0"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "flannel.1"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "flannel-v6.1"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "flannel-wg"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "flannel-wg-v6"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "cilium_host"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "cilium_vxlan"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "vxlan.calico"
11:47:13 UTC stdout: [kw01-base]
Cannot find device "vxlan-v6.calico"
11:47:13 UTC success: [kw01-base]
11:47:13 UTC [ClearOSModule] Uninstall etcd
11:47:13 UTC success: [kw01-base]
11:47:13 UTC [ClearOSModule] Remove cluster files
11:47:13 UTC success: [kw01-base]
11:47:13 UTC [ClearOSModule] Systemd daemon reload
11:47:13 UTC success: [kw01-base]
11:47:13 UTC [UninstallAutoRenewCertsModule] UnInstall auto renew control-plane certs
11:47:13 UTC skipped: [kw01-base]
11:47:13 UTC Pipeline[DeleteClusterPipeline] execute successfully
11:47:16 UTC [GreetingsModule] Greetings
11:47:17 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
11:47:17 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
11:47:18 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
11:47:18 UTC success: [kubesphere-master-1]
11:47:18 UTC success: [kubesphere-worker-1]
11:47:18 UTC success: [kubesphere-master-2]
11:47:18 UTC [NodePreCheckModule] A pre-check on nodes
11:47:18 UTC success: [kubesphere-master-2]
11:47:18 UTC success: [kubesphere-master-1]
11:47:18 UTC success: [kubesphere-worker-1]
11:47:18 UTC [ConfirmModule] Display confirmation form
11:47:18 UTC success: [LocalHost]
11:47:18 UTC [NodeBinariesModule] Download installation binaries
11:47:18 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
11:47:18 UTC message: [localhost]
kubeadm is existed
11:47:18 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
11:47:18 UTC message: [localhost]
kubelet is existed
11:47:18 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
11:47:18 UTC message: [localhost]
kubectl is existed
11:47:18 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
11:47:18 UTC message: [localhost]
helm is existed
11:47:18 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
11:47:19 UTC message: [localhost]
kubecni is existed
11:47:19 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
11:47:19 UTC message: [localhost]
crictl is existed
11:47:19 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
11:47:19 UTC message: [localhost]
etcd is existed
11:47:19 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
11:47:19 UTC message: [localhost]
containerd is existed
11:47:19 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
11:47:19 UTC message: [localhost]
runc is existed
11:47:19 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
11:47:19 UTC message: [localhost]
calicoctl is existed
11:47:19 UTC success: [LocalHost]
11:47:19 UTC [ConfigureOSModule] Get OS release
11:47:19 UTC success: [kubesphere-master-1]
11:47:19 UTC success: [kubesphere-worker-1]
11:47:19 UTC success: [kubesphere-master-2]
11:47:19 UTC [ConfigureOSModule] Prepare to init OS
11:47:19 UTC success: [kubesphere-worker-1]
11:47:19 UTC success: [kubesphere-master-1]
11:47:19 UTC success: [kubesphere-master-2]
11:47:19 UTC [ConfigureOSModule] Generate init os script
11:47:19 UTC success: [kubesphere-master-1]
11:47:19 UTC success: [kubesphere-master-2]
11:47:19 UTC success: [kubesphere-worker-1]
11:47:19 UTC [ConfigureOSModule] Exec init os script
11:47:20 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:47:20 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:47:20 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
11:47:20 UTC success: [kubesphere-master-1]
11:47:20 UTC success: [kubesphere-worker-1]
11:47:20 UTC success: [kubesphere-master-2]
11:47:20 UTC [ConfigureOSModule] configure the ntp server for each node
11:47:20 UTC skipped: [kubesphere-worker-1]
11:47:20 UTC skipped: [kubesphere-master-1]
11:47:20 UTC skipped: [kubesphere-master-2]
11:47:20 UTC [KubernetesStatusModule] Get kubernetes cluster status
11:47:20 UTC stdout: [kubesphere-master-2]
v1.23.10
11:47:20 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:47:20 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:47:20 UTC retry: [kubesphere-master-2]
11:47:25 UTC stdout: [kubesphere-master-2]
v1.23.10
11:47:25 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:47:25 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:47:25 UTC retry: [kubesphere-master-2]
11:47:30 UTC stdout: [kubesphere-master-2]
v1.23.10
11:47:30 UTC stdout: [kubesphere-master-2]
The connection to the server localhost:8080 was refused - did you specify the right host or port?
11:47:30 UTC message: [kubesphere-master-2]
get kubernetes cluster info failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl --no-headers=true get nodes -o custom-columns=:metadata.name,:status.nodeInfo.kubeletVersion,:status.addresses" 
The connection to the server localhost:8080 was refused - did you specify the right host or port?: Process exited with status 1
11:47:30 UTC success: [kubesphere-master-1]
11:47:30 UTC failed: [kubesphere-master-2]
12:19:14 UTC [GreetingsModule] Greetings
12:19:15 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:19:15 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:19:15 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:19:15 UTC success: [kubesphere-worker-1]
12:19:15 UTC success: [kubesphere-master-1]
12:19:15 UTC success: [kubesphere-master-2]
12:19:15 UTC [NodePreCheckModule] A pre-check on nodes
12:19:16 UTC success: [kubesphere-master-1]
12:19:16 UTC success: [kubesphere-worker-1]
12:19:16 UTC success: [kubesphere-master-2]
12:19:16 UTC [ConfirmModule] Display confirmation form
12:19:16 UTC [ERRO] kubesphere-master-1: conntrack is required.
12:19:16 UTC [ERRO] kubesphere-master-1: socat is required.
12:19:16 UTC [ERRO] kubesphere-master-2: conntrack is required.
12:19:16 UTC [ERRO] kubesphere-master-2: socat is required.
12:19:16 UTC [ERRO] kubesphere-worker-1: conntrack is required.
12:19:16 UTC [ERRO] kubesphere-worker-1: socat is required.
12:21:05 UTC [GreetingsModule] Greetings
12:21:05 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:21:06 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:21:06 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:21:06 UTC success: [kubesphere-worker-1]
12:21:06 UTC success: [kubesphere-master-1]
12:21:06 UTC success: [kubesphere-master-2]
12:21:06 UTC [NodePreCheckModule] A pre-check on nodes
12:21:06 UTC success: [kubesphere-master-1]
12:21:06 UTC success: [kubesphere-master-2]
12:21:06 UTC success: [kubesphere-worker-1]
12:21:06 UTC [ConfirmModule] Display confirmation form
12:21:06 UTC success: [LocalHost]
12:21:06 UTC [NodeBinariesModule] Download installation binaries
12:21:06 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
12:21:06 UTC message: [localhost]
kubeadm is existed
12:21:06 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
12:21:07 UTC message: [localhost]
kubelet is existed
12:21:07 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
12:21:07 UTC message: [localhost]
kubectl is existed
12:21:07 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
12:21:07 UTC message: [localhost]
helm is existed
12:21:07 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
12:21:07 UTC message: [localhost]
kubecni is existed
12:21:07 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
12:21:07 UTC message: [localhost]
crictl is existed
12:21:07 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
12:21:07 UTC message: [localhost]
etcd is existed
12:21:07 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
12:21:07 UTC message: [localhost]
containerd is existed
12:21:07 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
12:21:07 UTC message: [localhost]
runc is existed
12:21:07 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
12:21:07 UTC message: [localhost]
calicoctl is existed
12:21:07 UTC success: [LocalHost]
12:21:07 UTC [ConfigureOSModule] Get OS release
12:21:07 UTC success: [kubesphere-worker-1]
12:21:07 UTC success: [kubesphere-master-2]
12:21:07 UTC success: [kubesphere-master-1]
12:21:07 UTC [ConfigureOSModule] Prepare to init OS
12:21:08 UTC success: [kubesphere-worker-1]
12:21:08 UTC success: [kubesphere-master-2]
12:21:08 UTC success: [kubesphere-master-1]
12:21:08 UTC [ConfigureOSModule] Generate init os script
12:21:08 UTC success: [kubesphere-worker-1]
12:21:08 UTC success: [kubesphere-master-1]
12:21:08 UTC success: [kubesphere-master-2]
12:21:08 UTC [ConfigureOSModule] Exec init os script
12:21:08 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:21:08 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:21:08 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:21:08 UTC success: [kubesphere-master-2]
12:21:08 UTC success: [kubesphere-worker-1]
12:21:08 UTC success: [kubesphere-master-1]
12:21:08 UTC [ConfigureOSModule] configure the ntp server for each node
12:21:08 UTC skipped: [kubesphere-worker-1]
12:21:08 UTC skipped: [kubesphere-master-1]
12:21:08 UTC skipped: [kubesphere-master-2]
12:21:08 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:21:09 UTC success: [kubesphere-master-1]
12:21:09 UTC success: [kubesphere-master-2]
12:21:09 UTC [InstallContainerModule] Sync containerd binaries
12:21:10 UTC success: [kubesphere-master-2]
12:21:10 UTC success: [kubesphere-worker-1]
12:21:10 UTC success: [kubesphere-master-1]
12:21:10 UTC [InstallContainerModule] Sync crictl binaries
12:21:10 UTC success: [kubesphere-worker-1]
12:21:10 UTC success: [kubesphere-master-1]
12:21:10 UTC success: [kubesphere-master-2]
12:21:10 UTC [InstallContainerModule] Generate containerd service
12:21:10 UTC success: [kubesphere-master-1]
12:21:10 UTC success: [kubesphere-master-2]
12:21:10 UTC success: [kubesphere-worker-1]
12:21:10 UTC [InstallContainerModule] Generate containerd config
12:21:10 UTC success: [kubesphere-worker-1]
12:21:10 UTC success: [kubesphere-master-2]
12:21:10 UTC success: [kubesphere-master-1]
12:21:10 UTC [InstallContainerModule] Generate crictl config
12:21:10 UTC success: [kubesphere-master-1]
12:21:10 UTC success: [kubesphere-master-2]
12:21:10 UTC success: [kubesphere-worker-1]
12:21:10 UTC [InstallContainerModule] Enable containerd
12:21:11 UTC success: [kubesphere-master-1]
12:21:11 UTC success: [kubesphere-worker-1]
12:21:11 UTC success: [kubesphere-master-2]
12:21:11 UTC [PullModule] Start to pull images on all nodes
12:21:11 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
12:21:11 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
12:21:11 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
12:21:15 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:21:15 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:21:15 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:21:22 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
12:21:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:21:23 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:21:27 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:21:27 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:21:29 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:21:34 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
12:21:34 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:21:34 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
12:21:40 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
12:21:41 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
12:21:42 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
12:21:45 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:21:46 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:21:50 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
12:21:52 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
12:21:52 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
12:21:57 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:22:00 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
12:22:01 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
12:22:09 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
12:22:10 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
12:22:17 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:22:18 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:22:24 UTC success: [kubesphere-worker-1]
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC [ETCDPreCheckModule] Get etcd status
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC [CertsModule] Fetch etcd certs
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC skipped: [kubesphere-master-2]
12:22:24 UTC [CertsModule] Generate etcd Certs
12:22:24 UTC success: [LocalHost]
12:22:24 UTC [CertsModule] Synchronize certs file
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC [CertsModule] Synchronize certs file to master
12:22:24 UTC skipped: [kubesphere-master-1]
12:22:24 UTC skipped: [kubesphere-master-2]
12:22:24 UTC [InstallETCDBinaryModule] Install etcd using binary
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC [InstallETCDBinaryModule] Generate etcd service
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC [InstallETCDBinaryModule] Generate access address
12:22:24 UTC skipped: [kubesphere-master-2]
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC [ETCDConfigureModule] Health check on exist etcd
12:22:24 UTC skipped: [kubesphere-master-2]
12:22:24 UTC skipped: [kubesphere-master-1]
12:22:24 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
12:22:24 UTC success: [kubesphere-master-1]
12:22:24 UTC success: [kubesphere-master-2]
12:22:24 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
12:22:25 UTC success: [kubesphere-master-1]
12:22:25 UTC success: [kubesphere-master-2]
12:22:25 UTC [ETCDConfigureModule] Restart etcd
12:22:31 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
12:22:31 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC [ETCDConfigureModule] Health check on all etcd
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC [ETCDConfigureModule] Health check on all etcd
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC [ETCDBackupModule] Backup etcd data regularly
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC [ETCDBackupModule] Generate backup ETCD service
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC [ETCDBackupModule] Generate backup ETCD timer
12:22:31 UTC success: [kubesphere-master-2]
12:22:31 UTC success: [kubesphere-master-1]
12:22:31 UTC [ETCDBackupModule] Enable backup etcd service
12:22:32 UTC success: [kubesphere-master-2]
12:22:32 UTC success: [kubesphere-master-1]
12:22:32 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC success: [kubesphere-worker-1]
12:22:36 UTC success: [kubesphere-master-2]
12:22:36 UTC [InstallKubeBinariesModule] Change kubelet mode
12:22:36 UTC success: [kubesphere-master-2]
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC success: [kubesphere-worker-1]
12:22:36 UTC [InstallKubeBinariesModule] Generate kubelet service
12:22:36 UTC success: [kubesphere-master-2]
12:22:36 UTC success: [kubesphere-worker-1]
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC [InstallKubeBinariesModule] Enable kubelet service
12:22:36 UTC success: [kubesphere-master-2]
12:22:36 UTC success: [kubesphere-worker-1]
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC [InstallKubeBinariesModule] Generate kubelet env
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC success: [kubesphere-master-2]
12:22:36 UTC success: [kubesphere-worker-1]
12:22:36 UTC [InitKubernetesModule] Generate kubeadm config
12:22:36 UTC skipped: [kubesphere-master-2]
12:22:36 UTC success: [kubesphere-master-1]
12:22:36 UTC [InitKubernetesModule] Generate audit policy
12:22:36 UTC skipped: [kubesphere-master-2]
12:22:36 UTC skipped: [kubesphere-master-1]
12:22:36 UTC [InitKubernetesModule] Generate audit webhook
12:22:36 UTC skipped: [kubesphere-master-2]
12:22:36 UTC skipped: [kubesphere-master-1]
12:22:36 UTC [InitKubernetesModule] Init cluster using kubeadm
12:22:50 UTC stdout: [kubesphere-master-1]
W0917 12:22:36.674900    3838 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 10.502683 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: wmubbw.jqzw8mpx4u3st3ou
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token wmubbw.jqzw8mpx4u3st3ou \
	--discovery-token-ca-cert-hash sha256:0201b318bd8a9d4007a381985170038e47a5b4f4194fafca1e00ad94039ff8ee \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token wmubbw.jqzw8mpx4u3st3ou \
	--discovery-token-ca-cert-hash sha256:0201b318bd8a9d4007a381985170038e47a5b4f4194fafca1e00ad94039ff8ee
12:22:50 UTC skipped: [kubesphere-master-2]
12:22:50 UTC success: [kubesphere-master-1]
12:22:50 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
12:22:50 UTC skipped: [kubesphere-master-2]
12:22:50 UTC success: [kubesphere-master-1]
12:22:50 UTC [InitKubernetesModule] Remove master taint
12:22:50 UTC skipped: [kubesphere-master-2]
12:22:50 UTC skipped: [kubesphere-master-1]
12:22:50 UTC [ClusterDNSModule] Generate coredns service
12:22:52 UTC skipped: [kubesphere-master-2]
12:22:52 UTC success: [kubesphere-master-1]
12:22:52 UTC [ClusterDNSModule] Override coredns service
12:22:52 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
12:22:52 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
12:22:52 UTC skipped: [kubesphere-master-2]
12:22:52 UTC success: [kubesphere-master-1]
12:22:52 UTC [ClusterDNSModule] Generate nodelocaldns
12:22:52 UTC skipped: [kubesphere-master-2]
12:22:52 UTC success: [kubesphere-master-1]
12:22:52 UTC [ClusterDNSModule] Deploy nodelocaldns
12:22:53 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
12:22:53 UTC skipped: [kubesphere-master-2]
12:22:53 UTC success: [kubesphere-master-1]
12:22:53 UTC [ClusterDNSModule] Generate nodelocaldns configmap
12:22:53 UTC skipped: [kubesphere-master-2]
12:22:53 UTC success: [kubesphere-master-1]
12:22:53 UTC [ClusterDNSModule] Apply nodelocaldns configmap
12:22:53 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
12:22:53 UTC skipped: [kubesphere-master-2]
12:22:53 UTC success: [kubesphere-master-1]
12:22:53 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:22:53 UTC stdout: [kubesphere-master-1]
v1.23.10
12:22:53 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
12:22:53 UTC stdout: [kubesphere-master-1]
W0917 12:22:53.387167    4501 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
58616d8b3c50eeb7f25620e93240a826407cccef0f65de09cd118f80c70bb769
12:22:53 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
12:22:53 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
12:22:53 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
12:22:53 UTC stdout: [kubesphere-master-1]
l9jzxu.dmdhji4p260x5wwc
12:22:53 UTC success: [kubesphere-master-1]
12:22:53 UTC success: [kubesphere-master-2]
12:22:53 UTC [JoinNodesModule] Generate kubeadm config
12:22:53 UTC skipped: [kubesphere-master-1]
12:22:53 UTC success: [kubesphere-worker-1]
12:22:53 UTC success: [kubesphere-master-2]
12:22:53 UTC [JoinNodesModule] Generate audit policy
12:22:53 UTC skipped: [kubesphere-master-2]
12:22:53 UTC skipped: [kubesphere-master-1]
12:22:53 UTC [JoinNodesModule] Generate audit webhook
12:22:53 UTC skipped: [kubesphere-master-2]
12:22:53 UTC skipped: [kubesphere-master-1]
12:22:53 UTC [JoinNodesModule] Join control-plane node
12:23:13 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0917 12:23:06.610334    3766 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0917 12:23:06.612662    3766 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
12:23:13 UTC skipped: [kubesphere-master-1]
12:23:13 UTC success: [kubesphere-master-2]
12:23:13 UTC [JoinNodesModule] Join worker node
12:23:20 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0917 12:23:14.362988    3259 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0917 12:23:14.366084    3259 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
12:23:20 UTC skipped: [kubesphere-master-2]
12:23:20 UTC success: [kubesphere-worker-1]
12:23:20 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
12:23:20 UTC skipped: [kubesphere-master-1]
12:23:20 UTC success: [kubesphere-master-2]
12:23:20 UTC [JoinNodesModule] Remove master taint
12:23:21 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
12:23:21 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
12:23:21 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
12:23:21 UTC skipped: [kubesphere-master-1]
12:23:21 UTC success: [kubesphere-master-2]
12:23:21 UTC [JoinNodesModule] Add worker label to all nodes
12:23:21 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
12:23:21 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
12:23:21 UTC success: [kubesphere-master-1]
12:23:21 UTC skipped: [kubesphere-master-2]
12:23:21 UTC [DeployNetworkPluginModule] Generate calico
12:23:21 UTC skipped: [kubesphere-master-2]
12:23:21 UTC success: [kubesphere-master-1]
12:23:21 UTC [DeployNetworkPluginModule] Deploy calico
12:23:23 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
12:23:23 UTC skipped: [kubesphere-master-2]
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC [ConfigureKubernetesModule] Configure kubernetes
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC skipped: [kubesphere-master-2]
12:23:23 UTC [ChownModule] Chown user $HOME/.kube dir
12:23:23 UTC success: [kubesphere-worker-1]
12:23:23 UTC success: [kubesphere-master-2]
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC [AutoRenewCertsModule] Generate k8s certs renew script
12:23:23 UTC success: [kubesphere-master-2]
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC [AutoRenewCertsModule] Generate k8s certs renew service
12:23:23 UTC success: [kubesphere-master-2]
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC success: [kubesphere-master-2]
12:23:23 UTC [AutoRenewCertsModule] Enable k8s certs renew service
12:23:23 UTC success: [kubesphere-master-2]
12:23:23 UTC success: [kubesphere-master-1]
12:23:23 UTC [SaveKubeConfigModule] Save kube config as a configmap
12:23:23 UTC success: [LocalHost]
12:23:23 UTC [AddonsModule] Install addons
12:23:23 UTC success: [LocalHost]
12:23:23 UTC Pipeline[CreateClusterPipeline] execute successfully
12:26:04 UTC [GreetingsModule] Greetings
12:26:04 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:26:05 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:26:05 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:26:05 UTC success: [kubesphere-worker-1]
12:26:05 UTC success: [kubesphere-master-1]
12:26:05 UTC success: [kubesphere-master-2]
12:26:05 UTC [NodePreCheckModule] A pre-check on nodes
12:26:05 UTC success: [kubesphere-worker-1]
12:26:05 UTC success: [kubesphere-master-1]
12:26:05 UTC success: [kubesphere-master-2]
12:26:05 UTC [ConfirmModule] Display confirmation form
12:26:05 UTC [ERRO] kubesphere-master-1: conntrack is required.
12:26:05 UTC [ERRO] kubesphere-master-1: socat is required.
12:26:05 UTC [ERRO] kubesphere-master-2: conntrack is required.
12:26:05 UTC [ERRO] kubesphere-master-2: socat is required.
12:26:05 UTC [ERRO] kubesphere-worker-1: conntrack is required.
12:26:05 UTC [ERRO] kubesphere-worker-1: socat is required.
12:27:11 UTC [GreetingsModule] Greetings
12:27:11 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:27:12 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:27:12 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:27:12 UTC success: [kubesphere-master-1]
12:27:12 UTC success: [kubesphere-worker-1]
12:27:12 UTC success: [kubesphere-master-2]
12:27:12 UTC [NodePreCheckModule] A pre-check on nodes
12:27:12 UTC success: [kubesphere-master-1]
12:27:12 UTC success: [kubesphere-worker-1]
12:27:12 UTC success: [kubesphere-master-2]
12:27:12 UTC [ConfirmModule] Display confirmation form
12:27:12 UTC success: [LocalHost]
12:27:12 UTC [NodeBinariesModule] Download installation binaries
12:27:12 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
12:27:12 UTC message: [localhost]
kubeadm is existed
12:27:12 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
12:27:13 UTC message: [localhost]
kubelet is existed
12:27:13 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
12:27:13 UTC message: [localhost]
kubectl is existed
12:27:13 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
12:27:13 UTC message: [localhost]
helm is existed
12:27:13 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
12:27:13 UTC message: [localhost]
kubecni is existed
12:27:13 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
12:27:13 UTC message: [localhost]
crictl is existed
12:27:13 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
12:27:13 UTC message: [localhost]
etcd is existed
12:27:13 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
12:27:13 UTC message: [localhost]
containerd is existed
12:27:13 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
12:27:13 UTC message: [localhost]
runc is existed
12:27:13 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
12:27:13 UTC message: [localhost]
calicoctl is existed
12:27:13 UTC success: [LocalHost]
12:27:13 UTC [ConfigureOSModule] Get OS release
12:27:13 UTC success: [kubesphere-master-1]
12:27:13 UTC success: [kubesphere-master-2]
12:27:13 UTC success: [kubesphere-worker-1]
12:27:13 UTC [ConfigureOSModule] Prepare to init OS
12:27:14 UTC success: [kubesphere-worker-1]
12:27:14 UTC success: [kubesphere-master-1]
12:27:14 UTC success: [kubesphere-master-2]
12:27:14 UTC [ConfigureOSModule] Generate init os script
12:27:14 UTC success: [kubesphere-master-2]
12:27:14 UTC success: [kubesphere-master-1]
12:27:14 UTC success: [kubesphere-worker-1]
12:27:14 UTC [ConfigureOSModule] Exec init os script
12:27:15 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:27:15 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:27:15 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:27:15 UTC success: [kubesphere-master-2]
12:27:15 UTC success: [kubesphere-master-1]
12:27:15 UTC success: [kubesphere-worker-1]
12:27:15 UTC [ConfigureOSModule] configure the ntp server for each node
12:27:15 UTC skipped: [kubesphere-master-1]
12:27:15 UTC skipped: [kubesphere-master-2]
12:27:15 UTC skipped: [kubesphere-worker-1]
12:27:15 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:27:15 UTC success: [kubesphere-master-2]
12:27:15 UTC success: [kubesphere-master-1]
12:27:15 UTC [InstallContainerModule] Sync containerd binaries
12:27:16 UTC success: [kubesphere-master-1]
12:27:16 UTC success: [kubesphere-worker-1]
12:27:16 UTC success: [kubesphere-master-2]
12:27:16 UTC [InstallContainerModule] Sync crictl binaries
12:27:16 UTC success: [kubesphere-master-2]
12:27:16 UTC success: [kubesphere-worker-1]
12:27:16 UTC success: [kubesphere-master-1]
12:27:16 UTC [InstallContainerModule] Generate containerd service
12:27:16 UTC success: [kubesphere-master-2]
12:27:16 UTC success: [kubesphere-master-1]
12:27:16 UTC success: [kubesphere-worker-1]
12:27:16 UTC [InstallContainerModule] Generate containerd config
12:27:16 UTC success: [kubesphere-master-2]
12:27:16 UTC success: [kubesphere-master-1]
12:27:16 UTC success: [kubesphere-worker-1]
12:27:16 UTC [InstallContainerModule] Generate crictl config
12:27:16 UTC success: [kubesphere-master-2]
12:27:16 UTC success: [kubesphere-worker-1]
12:27:16 UTC success: [kubesphere-master-1]
12:27:16 UTC [InstallContainerModule] Enable containerd
12:27:17 UTC success: [kubesphere-worker-1]
12:27:17 UTC success: [kubesphere-master-2]
12:27:17 UTC success: [kubesphere-master-1]
12:27:17 UTC [PullModule] Start to pull images on all nodes
12:27:17 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
12:27:17 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
12:27:17 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
12:27:22 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:27:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:27:23 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:27:29 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:27:29 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
12:27:29 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:27:34 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:27:35 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:27:36 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:27:40 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
12:27:41 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
12:27:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:27:47 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
12:27:48 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
12:27:49 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
12:27:53 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:27:54 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:27:57 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
12:27:59 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
12:28:00 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
12:28:05 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:28:06 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
12:28:08 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
12:28:17 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
12:28:18 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
12:28:25 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:28:26 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
12:28:31 UTC success: [kubesphere-worker-1]
12:28:31 UTC success: [kubesphere-master-1]
12:28:31 UTC success: [kubesphere-master-2]
12:28:31 UTC [ETCDPreCheckModule] Get etcd status
12:28:31 UTC success: [kubesphere-master-2]
12:28:31 UTC success: [kubesphere-master-1]
12:28:31 UTC [CertsModule] Fetch etcd certs
12:28:31 UTC success: [kubesphere-master-2]
12:28:31 UTC skipped: [kubesphere-master-1]
12:28:31 UTC [CertsModule] Generate etcd Certs
12:28:31 UTC success: [LocalHost]
12:28:31 UTC [CertsModule] Synchronize certs file
12:28:31 UTC success: [kubesphere-master-1]
12:28:31 UTC success: [kubesphere-master-2]
12:28:31 UTC [CertsModule] Synchronize certs file to master
12:28:31 UTC skipped: [kubesphere-master-1]
12:28:31 UTC skipped: [kubesphere-master-2]
12:28:31 UTC [InstallETCDBinaryModule] Install etcd using binary
12:28:32 UTC success: [kubesphere-master-1]
12:28:32 UTC success: [kubesphere-master-2]
12:28:32 UTC [InstallETCDBinaryModule] Generate etcd service
12:28:32 UTC success: [kubesphere-master-2]
12:28:32 UTC success: [kubesphere-master-1]
12:28:32 UTC [InstallETCDBinaryModule] Generate access address
12:28:32 UTC skipped: [kubesphere-master-1]
12:28:32 UTC success: [kubesphere-master-2]
12:28:32 UTC [ETCDConfigureModule] Health check on exist etcd
12:28:32 UTC skipped: [kubesphere-master-1]
12:28:32 UTC skipped: [kubesphere-master-2]
12:28:32 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
12:28:32 UTC success: [kubesphere-master-2]
12:28:32 UTC success: [kubesphere-master-1]
12:28:32 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
12:28:32 UTC success: [kubesphere-master-2]
12:28:32 UTC success: [kubesphere-master-1]
12:28:32 UTC [ETCDConfigureModule] Restart etcd
12:28:49 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
12:28:49 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC [ETCDConfigureModule] Health check on all etcd
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC [ETCDConfigureModule] Health check on all etcd
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC [ETCDBackupModule] Backup etcd data regularly
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC [ETCDBackupModule] Generate backup ETCD service
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC [ETCDBackupModule] Generate backup ETCD timer
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC [ETCDBackupModule] Enable backup etcd service
12:28:49 UTC success: [kubesphere-master-2]
12:28:49 UTC success: [kubesphere-master-1]
12:28:49 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
12:30:33 UTC [GreetingsModule] Greetings
12:30:34 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:30:34 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:30:34 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:30:34 UTC success: [kubesphere-worker-1]
12:30:34 UTC success: [kubesphere-master-2]
12:30:34 UTC success: [kubesphere-master-1]
12:30:34 UTC [NodePreCheckModule] A pre-check on nodes
12:30:34 UTC success: [kubesphere-master-1]
12:30:34 UTC success: [kubesphere-worker-1]
12:30:34 UTC success: [kubesphere-master-2]
12:30:34 UTC [ConfirmModule] Display confirmation form
12:30:34 UTC [ERRO] kubesphere-master-1: conntrack is required.
12:30:34 UTC [ERRO] kubesphere-master-1: socat is required.
12:30:34 UTC [ERRO] kubesphere-master-2: conntrack is required.
12:30:34 UTC [ERRO] kubesphere-master-2: socat is required.
12:30:34 UTC [ERRO] kubesphere-worker-1: conntrack is required.
12:30:34 UTC [ERRO] kubesphere-worker-1: socat is required.
12:32:12 UTC [GreetingsModule] Greetings
12:32:12 UTC message: [kw01-base]
Greetings, KubeKey!
12:32:12 UTC success: [kw01-base]
12:32:12 UTC [NodePreCheckModule] A pre-check on nodes
12:32:12 UTC success: [kw01-base]
12:32:12 UTC [ConfirmModule] Display confirmation form
12:33:53 UTC [GreetingsModule] Greetings
12:33:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:33:54 UTC stdout: [kubesphere-master-1]
Greetings, KubeKey!
12:33:54 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:33:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:33:54 UTC stdout: [kubesphere-worker-1]
Greetings, KubeKey!
12:33:54 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:33:55 UTC stdout: [kubesphere-master-2]
Greetings, KubeKey!
12:33:55 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:33:55 UTC success: [kubesphere-master-1]
12:33:55 UTC success: [kubesphere-worker-1]
12:33:55 UTC success: [kubesphere-master-2]
12:33:55 UTC [NodePreCheckModule] A pre-check on nodes
12:33:55 UTC command: [kubesphere-master-2]
which sudo
12:33:55 UTC stdout: [kubesphere-master-2]
/usr/bin/sudo
12:33:55 UTC command: [kubesphere-master-1]
which sudo
12:33:55 UTC stdout: [kubesphere-master-1]
/usr/bin/sudo
12:33:55 UTC command: [kubesphere-worker-1]
which sudo
12:33:55 UTC stdout: [kubesphere-worker-1]
/usr/bin/sudo
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which curl"
12:33:55 UTC stdout: [kubesphere-master-2]
/usr/bin/curl
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which curl"
12:33:55 UTC stdout: [kubesphere-master-1]
/usr/bin/curl
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which openssl"
12:33:55 UTC stdout: [kubesphere-master-2]
/usr/bin/openssl
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which curl"
12:33:55 UTC stdout: [kubesphere-worker-1]
/usr/bin/curl
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ebtables"
12:33:55 UTC stdout: [kubesphere-master-2]
/usr/sbin/ebtables
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which openssl"
12:33:55 UTC stdout: [kubesphere-worker-1]
/usr/bin/openssl
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which openssl"
12:33:55 UTC stdout: [kubesphere-master-1]
/usr/bin/openssl
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which socat"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ebtables"
12:33:55 UTC stdout: [kubesphere-worker-1]
/usr/sbin/ebtables
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipset"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ebtables"
12:33:55 UTC stdout: [kubesphere-master-1]
/usr/sbin/ebtables
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which socat"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipvsadm"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipset"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which socat"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which conntrack"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipvsadm"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which chronyd"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which conntrack"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipset"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:33:55 UTC stdout: [kubesphere-master-2]
/bin/bash: docker: command not found
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which chronyd"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipvsadm"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:33:55 UTC stdout: [kubesphere-worker-1]
/bin/bash: docker: command not found
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:33:55 UTC stdout: [kubesphere-master-2]
/bin/bash: containerd: command not found
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which conntrack"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which showmount"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:33:55 UTC stdout: [kubesphere-worker-1]
/bin/bash: containerd: command not found
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which chronyd"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which showmount"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which rbd"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:33:55 UTC stdout: [kubesphere-master-1]
/bin/bash: docker: command not found
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which rbd"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which glusterfs"
12:33:55 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-2]
date +"%Z %H:%M:%S"
12:33:55 UTC stdout: [kubesphere-master-2]
UTC 12:33:54
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:33:55 UTC stdout: [kubesphere-master-1]
/bin/bash: containerd: command not found
12:33:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which glusterfs"
12:33:55 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-worker-1]
date +"%Z %H:%M:%S"
12:33:55 UTC stdout: [kubesphere-worker-1]
UTC 12:33:54
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which showmount"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which rbd"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which glusterfs"
12:33:55 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:33:55 UTC command: [kubesphere-master-1]
date +"%Z %H:%M:%S"
12:33:55 UTC stdout: [kubesphere-master-1]
UTC 12:33:54
12:33:55 UTC success: [kubesphere-master-2]
12:33:55 UTC success: [kubesphere-worker-1]
12:33:55 UTC success: [kubesphere-master-1]
12:33:55 UTC [ConfirmModule] Display confirmation form
12:33:55 UTC [ERRO] kubesphere-worker-1: conntrack is required.
12:33:55 UTC [ERRO] kubesphere-worker-1: socat is required.
12:33:55 UTC [ERRO] kubesphere-master-2: conntrack is required.
12:33:55 UTC [ERRO] kubesphere-master-2: socat is required.
12:33:55 UTC [ERRO] kubesphere-master-1: conntrack is required.
12:33:55 UTC [ERRO] kubesphere-master-1: socat is required.
12:37:53 UTC [GreetingsModule] Greetings
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:37:54 UTC stdout: [kubesphere-worker-1]
Greetings, KubeKey!
12:37:54 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:37:54 UTC stdout: [kubesphere-master-1]
Greetings, KubeKey!
12:37:54 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:37:54 UTC stdout: [kubesphere-master-2]
Greetings, KubeKey!
12:37:54 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:37:54 UTC success: [kubesphere-worker-1]
12:37:54 UTC success: [kubesphere-master-1]
12:37:54 UTC success: [kubesphere-master-2]
12:37:54 UTC [NodePreCheckModule] A pre-check on nodes
12:37:54 UTC command: [kubesphere-master-2]
which sudo
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/bin/sudo
12:37:54 UTC command: [kubesphere-worker-1]
which sudo
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/bin/sudo
12:37:54 UTC command: [kubesphere-master-1]
which sudo
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/bin/sudo
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which curl"
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/bin/curl
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which curl"
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/bin/curl
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which curl"
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/bin/curl
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which openssl"
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/bin/openssl
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which openssl"
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/bin/openssl
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which openssl"
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/bin/openssl
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ebtables"
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/sbin/ebtables
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ebtables"
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/sbin/ebtables
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ebtables"
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/sbin/ebtables
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which socat"
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/bin/socat
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which socat"
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/bin/socat
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which socat"
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/bin/socat
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipset"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipset"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipset"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipvsadm"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipvsadm"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipvsadm"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which conntrack"
12:37:54 UTC stdout: [kubesphere-master-2]
/usr/sbin/conntrack
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which conntrack"
12:37:54 UTC stdout: [kubesphere-worker-1]
/usr/sbin/conntrack
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which conntrack"
12:37:54 UTC stdout: [kubesphere-master-1]
/usr/sbin/conntrack
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which chronyd"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which chronyd"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which chronyd"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:37:54 UTC stdout: [kubesphere-master-2]
/bin/bash: docker: command not found
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:37:54 UTC stdout: [kubesphere-worker-1]
/bin/bash: docker: command not found
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:37:54 UTC stdout: [kubesphere-master-1]
/bin/bash: docker: command not found
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:37:54 UTC stdout: [kubesphere-master-2]
/bin/bash: containerd: command not found
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:37:54 UTC stdout: [kubesphere-worker-1]
/bin/bash: containerd: command not found
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:37:54 UTC stdout: [kubesphere-master-1]
/bin/bash: containerd: command not found
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which showmount"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which showmount"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which showmount"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which rbd"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which rbd"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which rbd"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which glusterfs"
12:37:54 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-2]
date +"%Z %H:%M:%S"
12:37:54 UTC stdout: [kubesphere-master-2]
UTC 12:37:54
12:37:54 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which glusterfs"
12:37:54 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-worker-1]
date +"%Z %H:%M:%S"
12:37:54 UTC stdout: [kubesphere-worker-1]
UTC 12:37:54
12:37:54 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which glusterfs"
12:37:54 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:37:54 UTC command: [kubesphere-master-1]
date +"%Z %H:%M:%S"
12:37:54 UTC stdout: [kubesphere-master-1]
UTC 12:37:54
12:37:54 UTC success: [kubesphere-master-2]
12:37:54 UTC success: [kubesphere-worker-1]
12:37:54 UTC success: [kubesphere-master-1]
12:37:54 UTC [ConfirmModule] Display confirmation form
12:37:54 UTC success: [LocalHost]
12:37:54 UTC [NodeBinariesModule] Download installation binaries
12:37:54 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
12:37:54 UTC message: [localhost]
kubeadm is existed
12:37:54 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
12:37:55 UTC message: [localhost]
kubelet is existed
12:37:55 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
12:37:55 UTC message: [localhost]
kubectl is existed
12:37:55 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
12:37:55 UTC message: [localhost]
helm is existed
12:37:55 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
12:37:55 UTC message: [localhost]
kubecni is existed
12:37:55 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
12:37:55 UTC message: [localhost]
crictl is existed
12:37:55 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
12:37:55 UTC message: [localhost]
etcd is existed
12:37:55 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
12:37:55 UTC message: [localhost]
containerd is existed
12:37:55 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
12:37:55 UTC message: [localhost]
runc is existed
12:37:55 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
12:37:55 UTC message: [localhost]
calicoctl is existed
12:37:55 UTC success: [LocalHost]
12:37:55 UTC [ConfigureOSModule] Get OS release
12:37:55 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "cat /etc/os-release"
12:37:55 UTC stdout: [kubesphere-master-2]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:37:55 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "cat /etc/os-release"
12:37:55 UTC stdout: [kubesphere-worker-1]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:37:55 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "cat /etc/os-release"
12:37:55 UTC stdout: [kubesphere-master-1]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:37:55 UTC success: [kubesphere-master-2]
12:37:55 UTC success: [kubesphere-worker-1]
12:37:55 UTC success: [kubesphere-master-1]
12:37:55 UTC [ConfigureOSModule] Prepare to init OS
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-worker-1 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-worker-1/g' /etc/hosts"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "useradd -M -c 'Etcd user' -s /sbin/nologin -r etcd || :"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "useradd -M -c 'Etcd user' -s /sbin/nologin -r etcd || :"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /var/lib/etcd && chown etcd -R /var/lib/etcd"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/etcd && chown etcd -R /var/lib/etcd"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-master-2 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-master-2/g' /etc/hosts"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-master-1 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-master-1/g' /etc/hosts"
12:37:56 UTC success: [kubesphere-worker-1]
12:37:56 UTC success: [kubesphere-master-2]
12:37:56 UTC success: [kubesphere-master-1]
12:37:56 UTC [ConfigureOSModule] Generate init os script
12:37:56 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-2/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:37:56 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-worker-1/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:37:56 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-1/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:56 UTC success: [kubesphere-master-2]
12:37:56 UTC success: [kubesphere-worker-1]
12:37:56 UTC success: [kubesphere-master-1]
12:37:56 UTC [ConfigureOSModule] Exec init os script
12:37:56 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:37:56 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:37:56 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:37:57 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:37:57 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:37:57 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:37:57 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:37:57 UTC success: [kubesphere-worker-1]
12:37:57 UTC success: [kubesphere-master-1]
12:37:57 UTC success: [kubesphere-master-2]
12:37:57 UTC [ConfigureOSModule] configure the ntp server for each node
12:37:57 UTC skipped: [kubesphere-worker-1]
12:37:57 UTC skipped: [kubesphere-master-2]
12:37:57 UTC skipped: [kubesphere-master-1]
12:37:57 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:37:57 UTC check remote file exist: false
12:37:57 UTC check remote file exist: false
12:37:57 UTC success: [kubesphere-master-1]
12:37:57 UTC success: [kubesphere-master-2]
12:37:57 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:57 UTC [InstallContainerModule] Sync containerd binaries
12:37:57 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:57 UTC stdout: [kubesphere-master-2]
not exist
12:37:57 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:57 UTC stdout: [kubesphere-master-1]
not exist
12:37:57 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:57 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:57 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:57 UTC stdout: [kubesphere-worker-1]
not exist
12:37:57 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:57 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/containerd/1.6.4/amd64/containerd-1.6.4-linux-amd64.tar.gz to remote /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz success
12:37:57 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/containerd/1.6.4/amd64/containerd-1.6.4-linux-amd64.tar.gz to remote /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz success
12:37:57 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/containerd/1.6.4/amd64/containerd-1.6.4-linux-amd64.tar.gz to remote /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz success
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz && mv bin/* /usr/bin && rm -rf bin"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz && mv bin/* /usr/bin && rm -rf bin"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/containerd-1.6.4-linux-amd64.tar.gz && mv bin/* /usr/bin && rm -rf bin"
12:37:58 UTC success: [kubesphere-master-2]
12:37:58 UTC success: [kubesphere-worker-1]
12:37:58 UTC success: [kubesphere-master-1]
12:37:58 UTC [InstallContainerModule] Sync crictl binaries
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-1]
not exist
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-worker-1]
not exist
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-2]
not exist
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/crictl/v1.24.0/amd64/crictl-v1.24.0-linux-amd64.tar.gz to remote /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/crictl/v1.24.0/amd64/crictl-v1.24.0-linux-amd64.tar.gz to remote /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/crictl/v1.24.0/amd64/crictl-v1.24.0-linux-amd64.tar.gz to remote /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz success
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz -C /usr/bin "
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz -C /usr/bin "
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/bin && tar -zxf /tmp/kubekey/crictl-v1.24.0-linux-amd64.tar.gz -C /usr/bin "
12:37:58 UTC success: [kubesphere-master-2]
12:37:58 UTC success: [kubesphere-master-1]
12:37:58 UTC success: [kubesphere-worker-1]
12:37:58 UTC [InstallContainerModule] Generate containerd service
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-worker-1]
not exist
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-1]
not exist
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-2]
not exist
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-worker-1/containerd.service to remote /tmp/kubekey/etc/systemd/system/containerd.service success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-1/containerd.service to remote /tmp/kubekey/etc/systemd/system/containerd.service success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-2/containerd.service to remote /tmp/kubekey/etc/systemd/system/containerd.service success
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/systemd/system/containerd.service /etc/systemd/system/containerd.service"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/systemd/system/containerd.service /etc/systemd/system/containerd.service"
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/systemd/system/containerd.service /etc/systemd/system/containerd.service"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC success: [kubesphere-worker-1]
12:37:58 UTC success: [kubesphere-master-1]
12:37:58 UTC success: [kubesphere-master-2]
12:37:58 UTC [InstallContainerModule] Generate containerd config
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-2]
not exist
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-1]
not exist
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-worker-1]
not exist
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-2/config.toml to remote /tmp/kubekey/etc/containerd/config.toml success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-1/config.toml to remote /tmp/kubekey/etc/containerd/config.toml success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-worker-1/config.toml to remote /tmp/kubekey/etc/containerd/config.toml success
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/containerd/config.toml /etc/containerd/config.toml"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/containerd/config.toml /etc/containerd/config.toml"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/containerd/config.toml /etc/containerd/config.toml"
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC success: [kubesphere-master-2]
12:37:58 UTC success: [kubesphere-master-1]
12:37:58 UTC success: [kubesphere-worker-1]
12:37:58 UTC [InstallContainerModule] Generate crictl config
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-worker-1]
not exist
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-1]
not exist
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-2]
not exist
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-2/crictl.yaml to remote /tmp/kubekey/etc/crictl.yaml success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-1/crictl.yaml to remote /tmp/kubekey/etc/crictl.yaml success
12:37:58 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-worker-1/crictl.yaml to remote /tmp/kubekey/etc/crictl.yaml success
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/crictl.yaml /etc/crictl.yaml"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/crictl.yaml /etc/crictl.yaml"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/etc/crictl.yaml /etc/crictl.yaml"
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:37:58 UTC success: [kubesphere-master-2]
12:37:58 UTC success: [kubesphere-master-1]
12:37:58 UTC success: [kubesphere-worker-1]
12:37:58 UTC [InstallContainerModule] Enable containerd
12:37:58 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-worker-1]
not exist
12:37:58 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-1]
not exist
12:37:58 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:37:58 UTC stdout: [kubesphere-master-2]
not exist
12:37:59 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "systemctl daemon-reload && systemctl enable containerd && systemctl start containerd"
12:37:59 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /etc/systemd/system/containerd.service.
12:37:59 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:59 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "systemctl daemon-reload && systemctl enable containerd && systemctl start containerd"
12:37:59 UTC stdout: [kubesphere-worker-1]
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /etc/systemd/system/containerd.service.
12:37:59 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "systemctl daemon-reload && systemctl enable containerd && systemctl start containerd"
12:37:59 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/containerd.service  /etc/systemd/system/containerd.service.
12:37:59 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:59 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:37:59 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/runc/v1.1.1/amd64/runc.amd64 to remote /tmp/kubekey/runc.amd64 success
12:37:59 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "install -m 755 /tmp/kubekey/runc.amd64 /usr/local/sbin/runc"
12:37:59 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/runc/v1.1.1/amd64/runc.amd64 to remote /tmp/kubekey/runc.amd64 success
12:37:59 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/runc/v1.1.1/amd64/runc.amd64 to remote /tmp/kubekey/runc.amd64 success
12:37:59 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "install -m 755 /tmp/kubekey/runc.amd64 /usr/local/sbin/runc"
12:37:59 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "install -m 755 /tmp/kubekey/runc.amd64 /usr/local/sbin/runc"
12:37:59 UTC success: [kubesphere-master-2]
12:37:59 UTC success: [kubesphere-master-1]
12:37:59 UTC success: [kubesphere-worker-1]
12:37:59 UTC [PullModule] Start to pull images on all nodes
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
12:37:59 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:37:59 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
12:38:06 UTC [GreetingsModule] Greetings
12:38:06 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:38:06 UTC stdout: [kubesphere-worker-1]
Greetings, KubeKey!
12:38:06 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:38:06 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:38:06 UTC stdout: [kubesphere-master-2]
Greetings, KubeKey!
12:38:06 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:38:07 UTC stdout: [kubesphere-master-1]
Greetings, KubeKey!
12:38:07 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:38:07 UTC success: [kubesphere-worker-1]
12:38:07 UTC success: [kubesphere-master-2]
12:38:07 UTC success: [kubesphere-master-1]
12:38:07 UTC [NodePreCheckModule] A pre-check on nodes
12:38:07 UTC command: [kubesphere-master-1]
which sudo
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/bin/sudo
12:38:07 UTC command: [kubesphere-worker-1]
which sudo
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/bin/sudo
12:38:07 UTC command: [kubesphere-master-2]
which sudo
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/bin/sudo
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which curl"
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/bin/curl
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which curl"
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/bin/curl
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which curl"
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/bin/curl
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which openssl"
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/bin/openssl
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which openssl"
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/bin/openssl
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which openssl"
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/bin/openssl
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ebtables"
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/sbin/ebtables
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ebtables"
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/sbin/ebtables
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ebtables"
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/sbin/ebtables
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which socat"
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/bin/socat
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which socat"
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/bin/socat
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which socat"
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/bin/socat
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipset"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipset"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipset"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipvsadm"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipvsadm"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipvsadm"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which conntrack"
12:38:07 UTC stdout: [kubesphere-master-1]
/usr/sbin/conntrack
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which conntrack"
12:38:07 UTC stdout: [kubesphere-worker-1]
/usr/sbin/conntrack
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which conntrack"
12:38:07 UTC stdout: [kubesphere-master-2]
/usr/sbin/conntrack
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which chronyd"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which chronyd"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which chronyd"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:38:07 UTC stdout: [kubesphere-master-2]
/bin/bash: docker: command not found
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:38:07 UTC stdout: [kubesphere-master-1]
/bin/bash: docker: command not found
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:38:07 UTC stdout: [kubesphere-worker-1]
/bin/bash: docker: command not found
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:38:07 UTC stdout: [kubesphere-master-2]
v1.6.4
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:38:07 UTC stdout: [kubesphere-master-1]
v1.6.4
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:38:07 UTC stdout: [kubesphere-worker-1]
v1.6.4
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which showmount"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which showmount"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which showmount"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which rbd"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which rbd"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which rbd"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which glusterfs"
12:38:07 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-2]
date +"%Z %H:%M:%S"
12:38:07 UTC stdout: [kubesphere-master-2]
UTC 12:38:07
12:38:07 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which glusterfs"
12:38:07 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which glusterfs"
12:38:07 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:38:07 UTC command: [kubesphere-worker-1]
date +"%Z %H:%M:%S"
12:38:07 UTC stdout: [kubesphere-worker-1]
UTC 12:38:07
12:38:07 UTC command: [kubesphere-master-1]
date +"%Z %H:%M:%S"
12:38:07 UTC stdout: [kubesphere-master-1]
UTC 12:38:07
12:38:07 UTC success: [kubesphere-master-2]
12:38:07 UTC success: [kubesphere-worker-1]
12:38:07 UTC success: [kubesphere-master-1]
12:38:07 UTC [ConfirmModule] Display confirmation form
12:38:07 UTC success: [LocalHost]
12:38:07 UTC [NodeBinariesModule] Download installation binaries
12:38:07 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
12:38:07 UTC message: [localhost]
kubeadm is existed
12:38:07 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
12:38:07 UTC message: [localhost]
kubelet is existed
12:38:07 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
12:38:07 UTC message: [localhost]
kubectl is existed
12:38:07 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
12:38:08 UTC message: [localhost]
helm is existed
12:38:08 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
12:38:08 UTC message: [localhost]
kubecni is existed
12:38:08 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
12:38:08 UTC message: [localhost]
crictl is existed
12:38:08 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
12:38:08 UTC message: [localhost]
etcd is existed
12:38:08 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
12:38:08 UTC message: [localhost]
containerd is existed
12:38:08 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
12:38:08 UTC message: [localhost]
runc is existed
12:38:08 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
12:38:08 UTC message: [localhost]
calicoctl is existed
12:38:08 UTC success: [LocalHost]
12:38:08 UTC [ConfigureOSModule] Get OS release
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "cat /etc/os-release"
12:38:08 UTC stdout: [kubesphere-worker-1]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "cat /etc/os-release"
12:38:08 UTC stdout: [kubesphere-master-2]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "cat /etc/os-release"
12:38:08 UTC stdout: [kubesphere-master-1]
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
12:38:08 UTC success: [kubesphere-worker-1]
12:38:08 UTC success: [kubesphere-master-2]
12:38:08 UTC success: [kubesphere-master-1]
12:38:08 UTC [ConfigureOSModule] Prepare to init OS
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:38:08 UTC stdout: [kubesphere-master-2]
useradd: user 'kube' already exists
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:38:08 UTC stdout: [kubesphere-worker-1]
useradd: user 'kube' already exists
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "useradd -M -c 'Kubernetes user' -s /sbin/nologin -r kube || :"
12:38:08 UTC stdout: [kubesphere-master-1]
useradd: user 'kube' already exists
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "useradd -M -c 'Etcd user' -s /sbin/nologin -r etcd || :"
12:38:08 UTC stdout: [kubesphere-master-2]
useradd: user 'etcd' already exists
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "useradd -M -c 'Etcd user' -s /sbin/nologin -r etcd || :"
12:38:08 UTC stdout: [kubesphere-master-1]
useradd: user 'etcd' already exists
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/pki"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /etc/kubernetes/manifests"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/local/bin/kube-scripts"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /usr/libexec/kubernetes/kubelet-plugins/volume/exec"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chown kube -R /usr/libexec/kubernetes"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /etc/cni/net.d && chown kube -R /etc/cni"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /opt/cni/bin && chown kube -R /opt/cni"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mkdir -p /var/lib/etcd && chown etcd -R /var/lib/etcd"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/calico && chown kube -R /var/lib/calico"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mkdir -p /var/lib/etcd && chown etcd -R /var/lib/etcd"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-worker-1 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-worker-1/g' /etc/hosts"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-master-2 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-master-2/g' /etc/hosts"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -d /tmp/kubekey ]; then rm -rf /tmp/kubekey ;fi && mkdir -m 777 -p /tmp/kubekey"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "hostnamectl set-hostname kubesphere-master-1 && sed -i '/^127.0.1.1/s/.*/127.0.1.1      kubesphere-master-1/g' /etc/hosts"
12:38:08 UTC success: [kubesphere-worker-1]
12:38:08 UTC success: [kubesphere-master-2]
12:38:08 UTC success: [kubesphere-master-1]
12:38:08 UTC [ConfigureOSModule] Generate init os script
12:38:08 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-1/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:38:08 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-master-2/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:38:08 UTC scp local file /root/works/terraform-kb/kubesphere-libvirt/deploy/artifacts/kubesphere/kubekey/kubesphere-worker-1/initOS.sh to remote /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh success
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "mv -f /tmp/kubekey/usr/local/bin/kube-scripts/initOS.sh /usr/local/bin/kube-scripts/initOS.sh"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "rm -rf /tmp/kubekey/*"
12:38:08 UTC success: [kubesphere-master-2]
12:38:08 UTC success: [kubesphere-master-1]
12:38:08 UTC success: [kubesphere-worker-1]
12:38:08 UTC [ConfigureOSModule] Exec init os script
12:38:08 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:38:08 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:38:08 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "chmod +x /usr/local/bin/kube-scripts/initOS.sh"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:38:09 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:38:09 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "/usr/local/bin/kube-scripts/initOS.sh"
12:38:09 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:38:09 UTC success: [kubesphere-worker-1]
12:38:09 UTC success: [kubesphere-master-1]
12:38:09 UTC success: [kubesphere-master-2]
12:38:09 UTC [ConfigureOSModule] configure the ntp server for each node
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:38:09 UTC check remote file exist: false
12:38:09 UTC check remote file exist: false
12:38:09 UTC success: [kubesphere-master-1]
12:38:09 UTC success: [kubesphere-master-2]
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC [InstallContainerModule] Sync containerd binaries
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC [InstallContainerModule] Sync crictl binaries
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which crictl) ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC [InstallContainerModule] Generate containerd service
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC [InstallContainerModule] Generate containerd config
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC [InstallContainerModule] Generate crictl config
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC [InstallContainerModule] Enable containerd
12:38:09 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "if [ -z $(which containerd) ] || [ ! -e /run/containerd/containerd.sock ]; then echo 'not exist'; fi"
12:38:09 UTC skipped: [kubesphere-master-2]
12:38:09 UTC skipped: [kubesphere-master-1]
12:38:09 UTC skipped: [kubesphere-worker-1]
12:38:09 UTC [PullModule] Start to pull images on all nodes
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC pauseTag: 3.6, corednsTag: 1.8.6
12:38:09 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
12:38:09 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
12:38:13 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64"
12:38:13 UTC stdout: [kubesphere-master-1]
Image is up to date for sha256:6270bb605e12e581514ada5fd5b3216f727db55dc87d5889c790e4c760683fee
12:38:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:38:13 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64"
12:38:13 UTC stdout: [kubesphere-master-2]
Image is up to date for sha256:6270bb605e12e581514ada5fd5b3216f727db55dc87d5889c790e4c760683fee
12:38:13 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:38:13 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64"
12:38:13 UTC stdout: [kubesphere-worker-1]
Image is up to date for sha256:6270bb605e12e581514ada5fd5b3216f727db55dc87d5889c790e4c760683fee
12:38:13 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:41:08 UTC [GreetingsModule] Greetings
12:41:08 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:41:08 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:41:09 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:41:09 UTC success: [kubesphere-worker-1]
12:41:09 UTC success: [kubesphere-master-2]
12:41:09 UTC success: [kubesphere-master-1]
12:41:09 UTC [NodePreCheckModule] A pre-check on nodes
12:41:09 UTC success: [kubesphere-master-2]
12:41:09 UTC success: [kubesphere-worker-1]
12:41:09 UTC success: [kubesphere-master-1]
12:41:09 UTC [ConfirmModule] Display confirmation form
12:41:09 UTC success: [LocalHost]
12:41:09 UTC [NodeBinariesModule] Download installation binaries
12:41:09 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
12:41:09 UTC message: [localhost]
kubeadm is existed
12:41:09 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
12:41:09 UTC message: [localhost]
kubelet is existed
12:41:09 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
12:41:09 UTC message: [localhost]
kubectl is existed
12:41:09 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
12:41:10 UTC message: [localhost]
helm is existed
12:41:10 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
12:41:10 UTC message: [localhost]
kubecni is existed
12:41:10 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
12:41:10 UTC message: [localhost]
crictl is existed
12:41:10 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
12:41:10 UTC message: [localhost]
etcd is existed
12:41:10 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
12:41:10 UTC message: [localhost]
containerd is existed
12:41:10 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
12:41:10 UTC message: [localhost]
runc is existed
12:41:10 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
12:41:10 UTC message: [localhost]
calicoctl is existed
12:41:10 UTC success: [LocalHost]
12:41:10 UTC [ConfigureOSModule] Get OS release
12:41:10 UTC success: [kubesphere-master-1]
12:41:10 UTC success: [kubesphere-master-2]
12:41:10 UTC success: [kubesphere-worker-1]
12:41:10 UTC [ConfigureOSModule] Prepare to init OS
12:41:10 UTC success: [kubesphere-worker-1]
12:41:10 UTC success: [kubesphere-master-2]
12:41:10 UTC success: [kubesphere-master-1]
12:41:10 UTC [ConfigureOSModule] Generate init os script
12:41:10 UTC success: [kubesphere-master-1]
12:41:10 UTC success: [kubesphere-master-2]
12:41:10 UTC success: [kubesphere-worker-1]
12:41:10 UTC [ConfigureOSModule] Exec init os script
12:41:11 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:41:11 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:41:11 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
12:41:11 UTC success: [kubesphere-master-1]
12:41:11 UTC success: [kubesphere-master-2]
12:41:11 UTC success: [kubesphere-worker-1]
12:41:11 UTC [ConfigureOSModule] configure the ntp server for each node
12:41:11 UTC skipped: [kubesphere-master-1]
12:41:11 UTC skipped: [kubesphere-worker-1]
12:41:11 UTC skipped: [kubesphere-master-2]
12:41:11 UTC [KubernetesStatusModule] Get kubernetes cluster status
12:41:11 UTC success: [kubesphere-master-1]
12:41:11 UTC success: [kubesphere-master-2]
12:41:11 UTC [InstallContainerModule] Sync containerd binaries
12:41:12 UTC success: [kubesphere-master-2]
12:41:12 UTC success: [kubesphere-worker-1]
12:41:12 UTC success: [kubesphere-master-1]
12:41:12 UTC [InstallContainerModule] Sync crictl binaries
12:41:13 UTC success: [kubesphere-master-1]
12:41:13 UTC success: [kubesphere-master-2]
12:41:13 UTC success: [kubesphere-worker-1]
12:41:13 UTC [InstallContainerModule] Generate containerd service
12:41:13 UTC success: [kubesphere-master-2]
12:41:13 UTC success: [kubesphere-master-1]
12:41:13 UTC success: [kubesphere-worker-1]
12:41:13 UTC [InstallContainerModule] Generate containerd config
12:41:13 UTC success: [kubesphere-master-1]
12:41:13 UTC success: [kubesphere-worker-1]
12:41:13 UTC success: [kubesphere-master-2]
12:41:13 UTC [InstallContainerModule] Generate crictl config
12:41:13 UTC success: [kubesphere-worker-1]
12:41:13 UTC success: [kubesphere-master-1]
12:41:13 UTC success: [kubesphere-master-2]
12:41:13 UTC [InstallContainerModule] Enable containerd
12:41:13 UTC success: [kubesphere-master-2]
12:41:13 UTC success: [kubesphere-master-1]
12:41:13 UTC success: [kubesphere-worker-1]
12:41:13 UTC [PullModule] Start to pull images on all nodes
12:41:13 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
12:41:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
12:41:13 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
12:41:18 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:41:18 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:41:18 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
12:41:24 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:41:25 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
12:41:25 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
12:41:31 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
12:41:31 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:41:31 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
12:41:36 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
12:41:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
12:41:37 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
12:43:10 UTC [GreetingsModule] Greetings
12:43:10 UTC failed: [kubesphere-master-1]
12:43:10 UTC failed: [kubesphere-worker-1]
12:43:10 UTC failed: [kubesphere-master-2]
12:46:04 UTC [GreetingsModule] Greetings
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:46:05 UTC stdout: [kubesphere-master-1]
Greetings, KubeKey!
12:46:05 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:46:05 UTC stdout: [kubesphere-worker-1]
Greetings, KubeKey!
12:46:05 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "echo 'Greetings, KubeKey!'"
12:46:05 UTC stdout: [kubesphere-master-2]
Greetings, KubeKey!
12:46:05 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
12:46:05 UTC success: [kubesphere-master-1]
12:46:05 UTC success: [kubesphere-worker-1]
12:46:05 UTC success: [kubesphere-master-2]
12:46:05 UTC [NodePreCheckModule] A pre-check on nodes
12:46:05 UTC command: [kubesphere-master-2]
which sudo
12:46:05 UTC stdout: [kubesphere-master-2]
/usr/bin/sudo
12:46:05 UTC command: [kubesphere-master-1]
which sudo
12:46:05 UTC stdout: [kubesphere-master-1]
/usr/bin/sudo
12:46:05 UTC command: [kubesphere-worker-1]
which sudo
12:46:05 UTC stdout: [kubesphere-worker-1]
/usr/bin/sudo
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which curl"
12:46:05 UTC stdout: [kubesphere-master-2]
/usr/bin/curl
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which curl"
12:46:05 UTC stdout: [kubesphere-master-1]
/usr/bin/curl
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which curl"
12:46:05 UTC stdout: [kubesphere-worker-1]
/usr/bin/curl
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which openssl"
12:46:05 UTC stdout: [kubesphere-master-1]
/usr/bin/openssl
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which openssl"
12:46:05 UTC stdout: [kubesphere-master-2]
/usr/bin/openssl
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ebtables"
12:46:05 UTC stdout: [kubesphere-master-1]
/usr/sbin/ebtables
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which openssl"
12:46:05 UTC stdout: [kubesphere-worker-1]
/usr/bin/openssl
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ebtables"
12:46:05 UTC stdout: [kubesphere-master-2]
/usr/sbin/ebtables
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which socat"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which socat"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ebtables"
12:46:05 UTC stdout: [kubesphere-worker-1]
/usr/sbin/ebtables
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipset"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipset"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which ipvsadm"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which socat"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which socat" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which ipvsadm"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which conntrack"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipset"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipset" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which conntrack"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which chronyd"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which ipvsadm"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which ipvsadm" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:46:05 UTC stdout: [kubesphere-master-2]
/bin/bash: docker: command not found
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which chronyd"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which conntrack"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which conntrack" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:46:05 UTC stdout: [kubesphere-master-1]
/bin/bash: docker: command not found
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:46:05 UTC stdout: [kubesphere-master-2]
/bin/bash: containerd: command not found
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which chronyd"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which chronyd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which showmount"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:46:05 UTC stdout: [kubesphere-master-1]
/bin/bash: containerd: command not found
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which showmount"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'"
12:46:05 UTC stdout: [kubesphere-worker-1]
/bin/bash: docker: command not found
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "docker version --format '{{.Server.Version}}'" 
/bin/bash: docker: command not found: Process exited with status 127
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which rbd"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which rbd"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
sudo -E /bin/bash -c "which glusterfs"
12:46:05 UTC stderr: [kubesphere-master-2]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-2]
date +"%Z %H:%M:%S"
12:46:05 UTC stdout: [kubesphere-master-2]
UTC 12:46:05
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "containerd --version | cut -d ' ' -f 3"
12:46:05 UTC stdout: [kubesphere-worker-1]
/bin/bash: containerd: command not found
12:46:05 UTC command: [kubesphere-master-1]
sudo -E /bin/bash -c "which glusterfs"
12:46:05 UTC stderr: [kubesphere-master-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-master-1]
date +"%Z %H:%M:%S"
12:46:05 UTC stdout: [kubesphere-master-1]
UTC 12:46:05
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which showmount"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which showmount" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which rbd"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which rbd" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
sudo -E /bin/bash -c "which glusterfs"
12:46:05 UTC stderr: [kubesphere-worker-1]
Failed to exec command: sudo -E /bin/bash -c "which glusterfs" 
: Process exited with status 1
12:46:05 UTC command: [kubesphere-worker-1]
date +"%Z %H:%M:%S"
12:46:05 UTC stdout: [kubesphere-worker-1]
UTC 12:46:05
12:46:05 UTC success: [kubesphere-master-2]
12:46:05 UTC success: [kubesphere-master-1]
12:46:05 UTC success: [kubesphere-worker-1]
12:46:05 UTC [ConfirmModule] Display confirmation form
12:46:05 UTC [ERRO] kubesphere-worker-1: conntrack is required.
12:46:05 UTC [ERRO] kubesphere-worker-1: socat is required.
12:46:05 UTC [ERRO] kubesphere-master-2: conntrack is required.
12:46:05 UTC [ERRO] kubesphere-master-2: socat is required.
12:46:05 UTC [ERRO] kubesphere-master-1: conntrack is required.
12:46:05 UTC [ERRO] kubesphere-master-1: socat is required.
13:10:25 UTC [GreetingsModule] Greetings
13:10:25 UTC failed: [kubesphere-worker-1]
13:10:25 UTC failed: [kubesphere-master-2]
13:10:25 UTC failed: [kubesphere-master-1]
13:13:46 UTC [GreetingsModule] Greetings
13:13:46 UTC failed: [kubesphere-worker-1]
13:13:46 UTC failed: [kubesphere-master-1]
13:13:46 UTC failed: [kubesphere-master-2]
13:14:29 UTC [GreetingsModule] Greetings
13:14:30 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
13:14:30 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
13:14:30 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
13:14:30 UTC success: [kubesphere-worker-1]
13:14:30 UTC success: [kubesphere-master-2]
13:14:30 UTC success: [kubesphere-master-1]
13:14:30 UTC [NodePreCheckModule] A pre-check on nodes
13:14:30 UTC success: [kubesphere-master-1]
13:14:30 UTC success: [kubesphere-master-2]
13:14:30 UTC success: [kubesphere-worker-1]
13:14:30 UTC [ConfirmModule] Display confirmation form
13:14:30 UTC success: [LocalHost]
13:14:30 UTC [NodeBinariesModule] Download installation binaries
13:14:30 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
13:14:31 UTC message: [localhost]
kubeadm is existed
13:14:31 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
13:14:31 UTC message: [localhost]
kubelet is existed
13:14:31 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
13:14:31 UTC message: [localhost]
kubectl is existed
13:14:31 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
13:14:31 UTC message: [localhost]
helm is existed
13:14:31 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
13:14:31 UTC message: [localhost]
kubecni is existed
13:14:31 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
13:14:31 UTC message: [localhost]
crictl is existed
13:14:31 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
13:14:31 UTC message: [localhost]
etcd is existed
13:14:31 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
13:14:31 UTC message: [localhost]
containerd is existed
13:14:31 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
13:14:31 UTC message: [localhost]
runc is existed
13:14:31 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
13:14:31 UTC message: [localhost]
calicoctl is existed
13:14:31 UTC success: [LocalHost]
13:14:31 UTC [ConfigureOSModule] Get OS release
13:14:31 UTC success: [kubesphere-master-1]
13:14:31 UTC success: [kubesphere-master-2]
13:14:31 UTC success: [kubesphere-worker-1]
13:14:31 UTC [ConfigureOSModule] Prepare to init OS
13:14:32 UTC success: [kubesphere-worker-1]
13:14:32 UTC success: [kubesphere-master-2]
13:14:32 UTC success: [kubesphere-master-1]
13:14:32 UTC [ConfigureOSModule] Generate init os script
13:14:32 UTC success: [kubesphere-master-1]
13:14:32 UTC success: [kubesphere-worker-1]
13:14:32 UTC success: [kubesphere-master-2]
13:14:32 UTC [ConfigureOSModule] Exec init os script
13:14:34 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:14:34 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:14:34 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:14:34 UTC success: [kubesphere-master-1]
13:14:34 UTC success: [kubesphere-master-2]
13:14:34 UTC success: [kubesphere-worker-1]
13:14:34 UTC [ConfigureOSModule] configure the ntp server for each node
13:14:34 UTC skipped: [kubesphere-master-1]
13:14:34 UTC skipped: [kubesphere-master-2]
13:14:34 UTC skipped: [kubesphere-worker-1]
13:14:34 UTC [KubernetesStatusModule] Get kubernetes cluster status
13:14:34 UTC success: [kubesphere-master-2]
13:14:34 UTC success: [kubesphere-master-1]
13:14:34 UTC [InstallContainerModule] Sync containerd binaries
13:14:35 UTC success: [kubesphere-master-2]
13:14:35 UTC success: [kubesphere-master-1]
13:14:35 UTC success: [kubesphere-worker-1]
13:14:35 UTC [InstallContainerModule] Sync crictl binaries
13:14:36 UTC success: [kubesphere-master-2]
13:14:36 UTC success: [kubesphere-master-1]
13:14:36 UTC success: [kubesphere-worker-1]
13:14:36 UTC [InstallContainerModule] Generate containerd service
13:14:36 UTC success: [kubesphere-master-1]
13:14:36 UTC success: [kubesphere-worker-1]
13:14:36 UTC success: [kubesphere-master-2]
13:14:36 UTC [InstallContainerModule] Generate containerd config
13:14:36 UTC success: [kubesphere-master-1]
13:14:36 UTC success: [kubesphere-worker-1]
13:14:36 UTC success: [kubesphere-master-2]
13:14:36 UTC [InstallContainerModule] Generate crictl config
13:14:36 UTC success: [kubesphere-master-2]
13:14:36 UTC success: [kubesphere-master-1]
13:14:36 UTC success: [kubesphere-worker-1]
13:14:36 UTC [InstallContainerModule] Enable containerd
13:14:36 UTC success: [kubesphere-master-2]
13:14:36 UTC success: [kubesphere-worker-1]
13:14:36 UTC success: [kubesphere-master-1]
13:14:36 UTC [PullModule] Start to pull images on all nodes
13:14:36 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:14:36 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:14:36 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:14:41 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
13:14:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:14:41 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:14:47 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
13:14:51 UTC message: [kubesphere-master-2]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:14:51.946791    2913 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:5bc227b27b219690df5f6f465354d62836e5cdf81998f801b1c952baf2333dca: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0010] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:5bc227b27b219690df5f6f465354d62836e5cdf81998f801b1c952baf2333dca: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:14:51 UTC retry: [kubesphere-master-2]
13:14:52 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:14:51.896053    2973 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:5bc227b27b219690df5f6f465354d62836e5cdf81998f801b1c952baf2333dca: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0010] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:5bc227b27b219690df5f6f465354d62836e5cdf81998f801b1c952baf2333dca: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:14:52 UTC retry: [kubesphere-master-1]
13:14:52 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:14:56 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:14:57 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:14:58 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:14:58 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:15:02 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/k8s-dns-node-cache:1.15.12 --platform amd64" 
E0917 13:15:02.511787    2932 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/k8s-dns-node-cache:1.15.12\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/k8s-dns-node-cache:1.15.12"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/k8s-dns-node-cache:1.15.12": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:02 UTC retry: [kubesphere-worker-1]
13:15:07 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:15:08 UTC message: [kubesphere-master-2]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:15:08.292274    2945 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:08 UTC retry: [kubesphere-master-2]
13:15:08 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:15:08.277234    3016 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:08 UTC retry: [kubesphere-master-1]
13:15:09 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
13:15:10 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
13:15:12 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:15:13 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:15:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:15:14 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:15:14 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:15:21 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/k8s-dns-node-cache:1.15.12 --platform amd64" 
E0917 13:15:21.735453    2990 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/k8s-dns-node-cache:1.15.12\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/k8s-dns-node-cache:1.15.12"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/k8s-dns-node-cache:1.15.12": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:21 UTC retry: [kubesphere-worker-1]
13:15:24 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:15:24.479466    3047 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:24 UTC message: [kubesphere-master-2]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/kube-apiserver:v1.23.10 --platform amd64" 
E0917 13:15:24.570414    2976 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/kube-apiserver:v1.23.10\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/kube-apiserver:v1.23.10"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/kube-apiserver:v1.23.10": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/kube-apiserver/manifests/sha256:a3b6ba0b713cfba71e161e84cef0b2766b99c0afb0d96cd4f1e0f7d6ae0b0467: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:26 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:15:28 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
13:15:29 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
13:15:31 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:15:41 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/k8s-dns-node-cache:1.15.12 --platform amd64" 
E0917 13:15:41.220394    3010 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/k8s-dns-node-cache:1.15.12\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/k8s-dns-node-cache:1.15.12"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/k8s-dns-node-cache:1.15.12": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/k8s-dns-node-cache/manifests/sha256:8e765f63b3a5b4832c484b4397f4932bd607713ec2bb3e639118bc164ab4a958: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:15:41 UTC failed: [kubesphere-master-1]
13:15:41 UTC failed: [kubesphere-master-2]
13:15:41 UTC failed: [kubesphere-worker-1]
13:30:00 UTC [GreetingsModule] Greetings
13:30:01 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
13:30:01 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
13:30:01 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
13:30:01 UTC success: [kubesphere-worker-1]
13:30:01 UTC success: [kubesphere-master-2]
13:30:01 UTC success: [kubesphere-master-1]
13:30:01 UTC [NodePreCheckModule] A pre-check on nodes
13:30:01 UTC success: [kubesphere-master-1]
13:30:01 UTC success: [kubesphere-worker-1]
13:30:01 UTC success: [kubesphere-master-2]
13:30:01 UTC [ConfirmModule] Display confirmation form
13:30:01 UTC success: [LocalHost]
13:30:01 UTC [NodeBinariesModule] Download installation binaries
13:30:01 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
13:30:02 UTC message: [localhost]
kubeadm is existed
13:30:02 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
13:30:02 UTC message: [localhost]
kubelet is existed
13:30:02 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
13:30:02 UTC message: [localhost]
kubectl is existed
13:30:02 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
13:30:02 UTC message: [localhost]
helm is existed
13:30:02 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
13:30:02 UTC message: [localhost]
kubecni is existed
13:30:02 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
13:30:02 UTC message: [localhost]
crictl is existed
13:30:02 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
13:30:02 UTC message: [localhost]
etcd is existed
13:30:02 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
13:30:02 UTC message: [localhost]
containerd is existed
13:30:02 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
13:30:02 UTC message: [localhost]
runc is existed
13:30:02 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
13:30:02 UTC message: [localhost]
calicoctl is existed
13:30:02 UTC success: [LocalHost]
13:30:02 UTC [ConfigureOSModule] Get OS release
13:30:02 UTC success: [kubesphere-master-2]
13:30:02 UTC success: [kubesphere-master-1]
13:30:02 UTC success: [kubesphere-worker-1]
13:30:02 UTC [ConfigureOSModule] Prepare to init OS
13:30:03 UTC success: [kubesphere-worker-1]
13:30:03 UTC success: [kubesphere-master-2]
13:30:03 UTC success: [kubesphere-master-1]
13:30:03 UTC [ConfigureOSModule] Generate init os script
13:30:03 UTC success: [kubesphere-master-1]
13:30:03 UTC success: [kubesphere-master-2]
13:30:03 UTC success: [kubesphere-worker-1]
13:30:03 UTC [ConfigureOSModule] Exec init os script
13:30:05 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:30:05 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:30:05 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:30:05 UTC success: [kubesphere-worker-1]
13:30:05 UTC success: [kubesphere-master-2]
13:30:05 UTC success: [kubesphere-master-1]
13:30:05 UTC [ConfigureOSModule] configure the ntp server for each node
13:30:05 UTC skipped: [kubesphere-worker-1]
13:30:05 UTC skipped: [kubesphere-master-2]
13:30:05 UTC skipped: [kubesphere-master-1]
13:30:05 UTC [KubernetesStatusModule] Get kubernetes cluster status
13:30:05 UTC success: [kubesphere-master-1]
13:30:05 UTC success: [kubesphere-master-2]
13:30:05 UTC [InstallContainerModule] Sync containerd binaries
13:30:06 UTC success: [kubesphere-master-1]
13:30:06 UTC success: [kubesphere-master-2]
13:30:06 UTC success: [kubesphere-worker-1]
13:30:06 UTC [InstallContainerModule] Sync crictl binaries
13:30:07 UTC success: [kubesphere-master-2]
13:30:07 UTC success: [kubesphere-master-1]
13:30:07 UTC success: [kubesphere-worker-1]
13:30:07 UTC [InstallContainerModule] Generate containerd service
13:30:07 UTC success: [kubesphere-worker-1]
13:30:07 UTC success: [kubesphere-master-2]
13:30:07 UTC success: [kubesphere-master-1]
13:30:07 UTC [InstallContainerModule] Generate containerd config
13:30:07 UTC success: [kubesphere-master-1]
13:30:07 UTC success: [kubesphere-worker-1]
13:30:07 UTC success: [kubesphere-master-2]
13:30:07 UTC [InstallContainerModule] Generate crictl config
13:30:07 UTC success: [kubesphere-master-1]
13:30:07 UTC success: [kubesphere-worker-1]
13:30:07 UTC success: [kubesphere-master-2]
13:30:07 UTC [InstallContainerModule] Enable containerd
13:30:07 UTC success: [kubesphere-master-2]
13:30:07 UTC success: [kubesphere-worker-1]
13:30:07 UTC success: [kubesphere-master-1]
13:30:07 UTC [PullModule] Start to pull images on all nodes
13:30:07 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:30:07 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:30:07 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:30:17 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:17.408172    2925 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:17 UTC retry: [kubesphere-worker-1]
13:30:17 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:17.575938    2949 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:17 UTC retry: [kubesphere-master-1]
13:30:17 UTC message: [kubesphere-master-2]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:17.621265    2934 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:17 UTC retry: [kubesphere-master-2]
13:30:22 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:30:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:30:22 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:30:32 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:32.022105    2951 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:32 UTC retry: [kubesphere-worker-1]
13:30:32 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:32.309653    2974 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:32 UTC retry: [kubesphere-master-1]
13:30:32 UTC message: [kubesphere-master-2]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0917 13:30:32.539740    2940 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit" image="kubesphere/pause:3.6"
[31mFATA[0m[0009] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to copy: httpReadSeeker: failed open: unexpected status code https://registry-1.docker.io/v2/kubesphere/pause/manifests/sha256:3d380ca8864549e74af4b29c10f9cb0956236dfb01c40ca076fb6c37253234db: 429 Too Many Requests - Server message: toomanyrequests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: https://www.docker.com/increase-rate-limit: Process exited with status 1
13:30:32 UTC retry: [kubesphere-master-2]
13:30:37 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:30:37 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:30:37 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:33:40 UTC [GreetingsModule] Greetings
13:33:40 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
13:33:41 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
13:33:41 UTC message: [kubesphere-master-2]
Greetings, KubeKey!
13:33:41 UTC success: [kubesphere-worker-1]
13:33:41 UTC success: [kubesphere-master-1]
13:33:41 UTC success: [kubesphere-master-2]
13:33:41 UTC [NodePreCheckModule] A pre-check on nodes
13:33:41 UTC success: [kubesphere-master-2]
13:33:41 UTC success: [kubesphere-worker-1]
13:33:41 UTC success: [kubesphere-master-1]
13:33:41 UTC [ConfirmModule] Display confirmation form
13:33:41 UTC success: [LocalHost]
13:33:41 UTC [NodeBinariesModule] Download installation binaries
13:33:41 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
13:33:41 UTC message: [localhost]
kubeadm is existed
13:33:41 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
13:33:42 UTC message: [localhost]
kubelet is existed
13:33:42 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
13:33:42 UTC message: [localhost]
kubectl is existed
13:33:42 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
13:33:42 UTC message: [localhost]
helm is existed
13:33:42 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
13:33:42 UTC message: [localhost]
kubecni is existed
13:33:42 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
13:33:42 UTC message: [localhost]
crictl is existed
13:33:42 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
13:33:42 UTC message: [localhost]
etcd is existed
13:33:42 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
13:33:42 UTC message: [localhost]
containerd is existed
13:33:42 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
13:33:42 UTC message: [localhost]
runc is existed
13:33:42 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
13:33:42 UTC message: [localhost]
calicoctl is existed
13:33:42 UTC success: [LocalHost]
13:33:42 UTC [ConfigureOSModule] Get OS release
13:33:42 UTC success: [kubesphere-master-1]
13:33:42 UTC success: [kubesphere-worker-1]
13:33:42 UTC success: [kubesphere-master-2]
13:33:42 UTC [ConfigureOSModule] Prepare to init OS
13:33:43 UTC success: [kubesphere-worker-1]
13:33:43 UTC success: [kubesphere-master-2]
13:33:43 UTC success: [kubesphere-master-1]
13:33:43 UTC [ConfigureOSModule] Generate init os script
13:33:43 UTC success: [kubesphere-master-2]
13:33:43 UTC success: [kubesphere-master-1]
13:33:43 UTC success: [kubesphere-worker-1]
13:33:43 UTC [ConfigureOSModule] Exec init os script
13:33:44 UTC stdout: [kubesphere-master-2]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:33:44 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:33:44 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
13:33:44 UTC success: [kubesphere-master-2]
13:33:44 UTC success: [kubesphere-worker-1]
13:33:44 UTC success: [kubesphere-master-1]
13:33:44 UTC [ConfigureOSModule] configure the ntp server for each node
13:33:44 UTC skipped: [kubesphere-worker-1]
13:33:44 UTC skipped: [kubesphere-master-1]
13:33:44 UTC skipped: [kubesphere-master-2]
13:33:44 UTC [KubernetesStatusModule] Get kubernetes cluster status
13:33:44 UTC success: [kubesphere-master-1]
13:33:44 UTC success: [kubesphere-master-2]
13:33:44 UTC [InstallContainerModule] Sync containerd binaries
13:33:44 UTC skipped: [kubesphere-master-1]
13:33:44 UTC skipped: [kubesphere-master-2]
13:33:44 UTC skipped: [kubesphere-worker-1]
13:33:44 UTC [InstallContainerModule] Sync crictl binaries
13:33:45 UTC success: [kubesphere-master-2]
13:33:45 UTC success: [kubesphere-master-1]
13:33:45 UTC success: [kubesphere-worker-1]
13:33:45 UTC [InstallContainerModule] Generate containerd service
13:33:45 UTC skipped: [kubesphere-master-2]
13:33:45 UTC skipped: [kubesphere-worker-1]
13:33:45 UTC skipped: [kubesphere-master-1]
13:33:45 UTC [InstallContainerModule] Generate containerd config
13:33:45 UTC skipped: [kubesphere-master-1]
13:33:45 UTC skipped: [kubesphere-master-2]
13:33:45 UTC skipped: [kubesphere-worker-1]
13:33:45 UTC [InstallContainerModule] Generate crictl config
13:33:45 UTC skipped: [kubesphere-worker-1]
13:33:45 UTC skipped: [kubesphere-master-2]
13:33:45 UTC skipped: [kubesphere-master-1]
13:33:45 UTC [InstallContainerModule] Enable containerd
13:33:45 UTC skipped: [kubesphere-master-1]
13:33:45 UTC skipped: [kubesphere-worker-1]
13:33:45 UTC skipped: [kubesphere-master-2]
13:33:45 UTC [PullModule] Start to pull images on all nodes
13:33:45 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
13:33:45 UTC message: [kubesphere-master-2]
downloading image: kubesphere/pause:3.6
13:33:45 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
13:33:46 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:33:46 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-apiserver:v1.23.10
13:33:46 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
13:33:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
13:33:49 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
13:33:49 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-controller-manager:v1.23.10
13:33:51 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:33:52 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
13:33:52 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-scheduler:v1.23.10
13:33:54 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
13:33:54 UTC message: [kubesphere-master-2]
downloading image: kubesphere/kube-proxy:v1.23.10
13:33:54 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
13:33:57 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
13:33:57 UTC message: [kubesphere-master-2]
downloading image: coredns/coredns:1.8.6
13:33:57 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
13:33:59 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:33:59 UTC message: [kubesphere-master-2]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
13:34:01 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
13:34:01 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
13:34:02 UTC message: [kubesphere-master-2]
downloading image: calico/kube-controllers:v3.23.2
13:34:05 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
13:34:05 UTC message: [kubesphere-master-2]
downloading image: calico/cni:v3.23.2
13:34:05 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
13:34:09 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
13:34:10 UTC message: [kubesphere-master-2]
downloading image: calico/node:v3.23.2
13:34:13 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
13:34:13 UTC message: [kubesphere-master-2]
downloading image: calico/pod2daemon-flexvol:v3.23.2
13:34:15 UTC success: [kubesphere-worker-1]
13:34:15 UTC success: [kubesphere-master-1]
13:34:15 UTC success: [kubesphere-master-2]
13:34:15 UTC [ETCDPreCheckModule] Get etcd status
13:34:15 UTC success: [kubesphere-master-1]
13:34:15 UTC success: [kubesphere-master-2]
13:34:15 UTC [CertsModule] Fetch etcd certs
13:34:15 UTC success: [kubesphere-master-1]
13:34:15 UTC skipped: [kubesphere-master-2]
13:34:15 UTC [CertsModule] Generate etcd Certs
13:34:15 UTC success: [LocalHost]
13:34:15 UTC [CertsModule] Synchronize certs file
13:34:16 UTC success: [kubesphere-master-2]
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC [CertsModule] Synchronize certs file to master
13:34:16 UTC skipped: [kubesphere-master-1]
13:34:16 UTC skipped: [kubesphere-master-2]
13:34:16 UTC [InstallETCDBinaryModule] Install etcd using binary
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC success: [kubesphere-master-2]
13:34:16 UTC [InstallETCDBinaryModule] Generate etcd service
13:34:16 UTC success: [kubesphere-master-2]
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC [InstallETCDBinaryModule] Generate access address
13:34:16 UTC skipped: [kubesphere-master-2]
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC [ETCDConfigureModule] Health check on exist etcd
13:34:16 UTC skipped: [kubesphere-master-2]
13:34:16 UTC skipped: [kubesphere-master-1]
13:34:16 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC success: [kubesphere-master-2]
13:34:16 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
13:34:16 UTC success: [kubesphere-master-1]
13:34:16 UTC success: [kubesphere-master-2]
13:34:16 UTC [ETCDConfigureModule] Restart etcd
13:34:23 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
13:34:23 UTC stdout: [kubesphere-master-2]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service  /etc/systemd/system/etcd.service.
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [ETCDConfigureModule] Health check on all etcd
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [ETCDConfigureModule] Health check on all etcd
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [ETCDBackupModule] Backup etcd data regularly
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC [ETCDBackupModule] Generate backup ETCD service
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC [ETCDBackupModule] Generate backup ETCD timer
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [ETCDBackupModule] Enable backup etcd service
13:34:23 UTC success: [kubesphere-master-1]
13:34:23 UTC success: [kubesphere-master-2]
13:34:23 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
13:34:27 UTC success: [kubesphere-worker-1]
13:34:27 UTC success: [kubesphere-master-2]
13:34:27 UTC success: [kubesphere-master-1]
13:34:27 UTC [InstallKubeBinariesModule] Change kubelet mode
13:34:27 UTC success: [kubesphere-master-1]
13:34:27 UTC success: [kubesphere-worker-1]
13:34:27 UTC success: [kubesphere-master-2]
13:34:27 UTC [InstallKubeBinariesModule] Generate kubelet service
13:34:27 UTC success: [kubesphere-master-1]
13:34:27 UTC success: [kubesphere-master-2]
13:34:27 UTC success: [kubesphere-worker-1]
13:34:27 UTC [InstallKubeBinariesModule] Enable kubelet service
13:34:28 UTC success: [kubesphere-master-1]
13:34:28 UTC success: [kubesphere-master-2]
13:34:28 UTC success: [kubesphere-worker-1]
13:34:28 UTC [InstallKubeBinariesModule] Generate kubelet env
13:34:28 UTC success: [kubesphere-master-2]
13:34:28 UTC success: [kubesphere-master-1]
13:34:28 UTC success: [kubesphere-worker-1]
13:34:28 UTC [InitKubernetesModule] Generate kubeadm config
13:34:28 UTC skipped: [kubesphere-master-2]
13:34:28 UTC success: [kubesphere-master-1]
13:34:28 UTC [InitKubernetesModule] Generate audit policy
13:34:28 UTC skipped: [kubesphere-master-2]
13:34:28 UTC skipped: [kubesphere-master-1]
13:34:28 UTC [InitKubernetesModule] Generate audit webhook
13:34:28 UTC skipped: [kubesphere-master-2]
13:34:28 UTC skipped: [kubesphere-master-1]
13:34:28 UTC [InitKubernetesModule] Init cluster using kubeadm
13:34:44 UTC stdout: [kubesphere-master-1]
W0917 13:34:28.379249    3882 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.152 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 13.003016 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 3yvyp5.qhfpromdp78qajwh
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 3yvyp5.qhfpromdp78qajwh \
	--discovery-token-ca-cert-hash sha256:4e5279f0eab8a7d0169730772fd26e98122f620f0303bc030ce352ca6f9fd9ac \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 3yvyp5.qhfpromdp78qajwh \
	--discovery-token-ca-cert-hash sha256:4e5279f0eab8a7d0169730772fd26e98122f620f0303bc030ce352ca6f9fd9ac
13:34:44 UTC skipped: [kubesphere-master-2]
13:34:44 UTC success: [kubesphere-master-1]
13:34:44 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
13:34:44 UTC skipped: [kubesphere-master-2]
13:34:44 UTC success: [kubesphere-master-1]
13:34:44 UTC [InitKubernetesModule] Remove master taint
13:34:44 UTC skipped: [kubesphere-master-2]
13:34:44 UTC skipped: [kubesphere-master-1]
13:34:44 UTC [ClusterDNSModule] Generate coredns service
13:34:45 UTC skipped: [kubesphere-master-2]
13:34:45 UTC success: [kubesphere-master-1]
13:34:45 UTC [ClusterDNSModule] Override coredns service
13:34:45 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
13:34:45 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
13:34:45 UTC skipped: [kubesphere-master-2]
13:34:45 UTC success: [kubesphere-master-1]
13:34:45 UTC [ClusterDNSModule] Generate nodelocaldns
13:34:45 UTC skipped: [kubesphere-master-2]
13:34:45 UTC success: [kubesphere-master-1]
13:34:45 UTC [ClusterDNSModule] Deploy nodelocaldns
13:34:45 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
13:34:45 UTC skipped: [kubesphere-master-2]
13:34:45 UTC success: [kubesphere-master-1]
13:34:45 UTC [ClusterDNSModule] Generate nodelocaldns configmap
13:34:45 UTC skipped: [kubesphere-master-2]
13:34:45 UTC success: [kubesphere-master-1]
13:34:45 UTC [ClusterDNSModule] Apply nodelocaldns configmap
13:34:46 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
13:34:46 UTC skipped: [kubesphere-master-2]
13:34:46 UTC success: [kubesphere-master-1]
13:34:46 UTC [KubernetesStatusModule] Get kubernetes cluster status
13:34:46 UTC stdout: [kubesphere-master-1]
v1.23.10
13:34:46 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
13:34:46 UTC stdout: [kubesphere-master-1]
W0917 13:34:46.092496    4558 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
55bbebd765d042b21f6fac8612f68899bdcd91ca8307195b105a970d3a45cea7
13:34:46 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
13:34:46 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
13:34:46 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
13:34:46 UTC stdout: [kubesphere-master-1]
ixh6rh.1wbzg91ut24nnhvb
13:34:46 UTC success: [kubesphere-master-1]
13:34:46 UTC success: [kubesphere-master-2]
13:34:46 UTC [JoinNodesModule] Generate kubeadm config
13:34:46 UTC skipped: [kubesphere-master-1]
13:34:46 UTC success: [kubesphere-master-2]
13:34:46 UTC success: [kubesphere-worker-1]
13:34:46 UTC [JoinNodesModule] Generate audit policy
13:34:46 UTC skipped: [kubesphere-master-2]
13:34:46 UTC skipped: [kubesphere-master-1]
13:34:46 UTC [JoinNodesModule] Generate audit webhook
13:34:46 UTC skipped: [kubesphere-master-2]
13:34:46 UTC skipped: [kubesphere-master-1]
13:34:46 UTC [JoinNodesModule] Join control-plane node
13:35:07 UTC stdout: [kubesphere-master-2]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0917 13:34:59.226564    3906 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0917 13:34:59.229488    3906 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[preflight] Running pre-flight checks before initializing the new control plane instance
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[download-certs] Downloading the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-master-2 kubesphere-master-2.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.152 127.0.0.1 192.168.122.151 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[certs] Using the existing "sa" key
[kubeconfig] Generating kubeconfig files
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[check-etcd] Skipping etcd check in external mode
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...
[control-plane-join] using external etcd - no local stacked instance added
The 'update-status' phase is deprecated and will be removed in a future release. Currently it performs no operation
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-2 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]

This node has joined the cluster and a new control plane instance was created:

* Certificate signing request was sent to apiserver and approval was received.
* The Kubelet was informed of the new secure connection details.
* Control plane (master) label and taint were applied to the new node.
* The Kubernetes control plane instances scaled up.


To start administering your cluster from this node, you need to run the following as a regular user:

	mkdir -p $HOME/.kube
	sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	sudo chown $(id -u):$(id -g) $HOME/.kube/config

Run 'kubectl get nodes' to see this node join the cluster.
13:35:07 UTC skipped: [kubesphere-master-1]
13:35:07 UTC success: [kubesphere-master-2]
13:35:07 UTC [JoinNodesModule] Join worker node
13:35:13 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0917 13:35:07.565580    3322 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0917 13:35:07.568547    3322 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
13:35:13 UTC skipped: [kubesphere-master-2]
13:35:13 UTC success: [kubesphere-worker-1]
13:35:13 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
13:35:13 UTC skipped: [kubesphere-master-1]
13:35:13 UTC success: [kubesphere-master-2]
13:35:13 UTC [JoinNodesModule] Remove master taint
13:35:13 UTC stdout: [kubesphere-master-2]
node/kubesphere-master-2 untainted
13:35:13 UTC stdout: [kubesphere-master-2]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
13:35:13 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-2 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
13:35:13 UTC skipped: [kubesphere-master-1]
13:35:13 UTC success: [kubesphere-master-2]
13:35:13 UTC [JoinNodesModule] Add worker label to all nodes
13:35:13 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-2 labeled
13:35:13 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
13:35:13 UTC success: [kubesphere-master-1]
13:35:13 UTC skipped: [kubesphere-master-2]
13:35:13 UTC [DeployNetworkPluginModule] Generate calico
13:35:13 UTC skipped: [kubesphere-master-2]
13:35:13 UTC success: [kubesphere-master-1]
13:35:13 UTC [DeployNetworkPluginModule] Deploy calico
13:35:15 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
13:35:15 UTC skipped: [kubesphere-master-2]
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC [ConfigureKubernetesModule] Configure kubernetes
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC skipped: [kubesphere-master-2]
13:35:15 UTC [ChownModule] Chown user $HOME/.kube dir
13:35:15 UTC success: [kubesphere-worker-1]
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC success: [kubesphere-master-2]
13:35:15 UTC [AutoRenewCertsModule] Generate k8s certs renew script
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC success: [kubesphere-master-2]
13:35:15 UTC [AutoRenewCertsModule] Generate k8s certs renew service
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC success: [kubesphere-master-2]
13:35:15 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
13:35:15 UTC success: [kubesphere-master-1]
13:35:15 UTC success: [kubesphere-master-2]
13:35:15 UTC [AutoRenewCertsModule] Enable k8s certs renew service
13:35:16 UTC success: [kubesphere-master-2]
13:35:16 UTC success: [kubesphere-master-1]
13:35:16 UTC [SaveKubeConfigModule] Save kube config as a configmap
13:35:16 UTC success: [LocalHost]
13:35:16 UTC [AddonsModule] Install addons
13:35:16 UTC success: [LocalHost]
13:35:16 UTC Pipeline[CreateClusterPipeline] execute successfully
