04:17:29 UTC [GreetingsModule] Greetings
04:17:29 UTC failed: [kubesphere-master-2]
04:17:29 UTC failed: [kubesphere-master-1]
04:19:21 UTC [GreetingsModule] Greetings
04:19:21 UTC failed: [kubesphere-master-1]
04:19:21 UTC failed: [kubesphere-master-2]
04:19:56 UTC [GreetingsModule] Greetings
04:20:00 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:20:00 UTC failed: [kubesphere-master-2]
04:20:00 UTC success: [kubesphere-master-1]
04:21:02 UTC [GreetingsModule] Greetings
04:21:02 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:21:03 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:21:03 UTC success: [kubesphere-worker-1]
04:21:03 UTC success: [kubesphere-master-1]
04:21:03 UTC [NodePreCheckModule] A pre-check on nodes
04:21:03 UTC success: [kubesphere-worker-1]
04:21:03 UTC success: [kubesphere-master-1]
04:21:03 UTC [ConfirmModule] Display confirmation form
04:21:03 UTC [ERRO] kubesphere-worker-1: conntrack is required.
04:21:03 UTC [ERRO] kubesphere-worker-1: socat is required.
04:22:43 UTC [GreetingsModule] Greetings
04:22:43 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:22:44 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:22:44 UTC success: [kubesphere-worker-1]
04:22:44 UTC success: [kubesphere-master-1]
04:22:44 UTC [NodePreCheckModule] A pre-check on nodes
04:22:44 UTC success: [kubesphere-worker-1]
04:22:44 UTC success: [kubesphere-master-1]
04:22:44 UTC [ConfirmModule] Display confirmation form
04:22:50 UTC success: [LocalHost]
04:22:50 UTC [NodeBinariesModule] Download installation binaries
04:22:50 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
04:22:52 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
04:22:56 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
04:22:59 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
04:23:01 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
04:23:03 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
04:23:05 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
04:23:07 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
04:23:09 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
04:23:11 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
04:23:14 UTC success: [LocalHost]
04:23:14 UTC [ConfigureOSModule] Get OS release
04:23:14 UTC success: [kubesphere-master-1]
04:23:14 UTC success: [kubesphere-worker-1]
04:23:14 UTC [ConfigureOSModule] Prepare to init OS
04:23:14 UTC success: [kubesphere-master-1]
04:23:14 UTC success: [kubesphere-worker-1]
04:23:14 UTC [ConfigureOSModule] Generate init os script
04:23:14 UTC success: [kubesphere-worker-1]
04:23:14 UTC success: [kubesphere-master-1]
04:23:14 UTC [ConfigureOSModule] Exec init os script
04:23:15 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:23:15 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 0
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.overcommit_memory = 0
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:23:15 UTC success: [kubesphere-worker-1]
04:23:15 UTC success: [kubesphere-master-1]
04:23:15 UTC [ConfigureOSModule] configure the ntp server for each node
04:23:15 UTC skipped: [kubesphere-worker-1]
04:23:15 UTC skipped: [kubesphere-master-1]
04:23:15 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:23:15 UTC stdout: [kubesphere-master-1]
v1.23.10
04:23:15 UTC stdout: [kubesphere-master-1]
node1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:node1 type:Hostname]]
04:23:15 UTC stdout: [kubesphere-master-1]
W0913 04:23:15.581039   63631 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
3d62a17267921bdbc24fb236c287c470636fce4fd3ea2f7b757d285bda9a2900
04:23:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:23:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:23:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:23:15 UTC stdout: [kubesphere-master-1]
gkkz0d.8gq0lniaqyle6zy7
04:23:15 UTC success: [kubesphere-master-1]
04:23:15 UTC [InstallContainerModule] Sync containerd binaries
04:23:16 UTC skipped: [kubesphere-master-1]
04:23:16 UTC success: [kubesphere-worker-1]
04:23:16 UTC [InstallContainerModule] Sync crictl binaries
04:23:17 UTC skipped: [kubesphere-master-1]
04:23:17 UTC success: [kubesphere-worker-1]
04:23:17 UTC [InstallContainerModule] Generate containerd service
04:23:17 UTC skipped: [kubesphere-master-1]
04:23:17 UTC success: [kubesphere-worker-1]
04:23:17 UTC [InstallContainerModule] Generate containerd config
04:23:17 UTC skipped: [kubesphere-master-1]
04:23:17 UTC success: [kubesphere-worker-1]
04:23:17 UTC [InstallContainerModule] Generate crictl config
04:23:17 UTC skipped: [kubesphere-master-1]
04:23:17 UTC success: [kubesphere-worker-1]
04:23:17 UTC [InstallContainerModule] Enable containerd
04:23:17 UTC skipped: [kubesphere-master-1]
04:23:17 UTC success: [kubesphere-worker-1]
04:23:17 UTC [PullModule] Start to pull images on all nodes
04:23:17 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
04:23:17 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
04:23:19 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:23:20 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:23:22 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:23:22 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:23:23 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:23:25 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
04:23:27 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:23:28 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
04:23:28 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
04:23:29 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
04:23:31 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
04:23:33 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:23:33 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:23:39 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
04:23:46 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
04:23:54 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
04:24:02 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:24:07 UTC success: [kubesphere-master-1]
04:24:07 UTC success: [kubesphere-worker-1]
04:24:07 UTC [ETCDPreCheckModule] Get etcd status
04:24:07 UTC stdout: [kubesphere-master-1]
ETCD_NAME=etcd-node1
04:24:07 UTC success: [kubesphere-master-1]
04:24:07 UTC [CertsModule] Fetch etcd certs
04:24:07 UTC success: [kubesphere-master-1]
04:24:07 UTC [CertsModule] Generate etcd Certs
04:24:08 UTC success: [LocalHost]
04:24:08 UTC [CertsModule] Synchronize certs file
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [CertsModule] Synchronize certs file to master
04:24:08 UTC skipped: [kubesphere-master-1]
04:24:08 UTC [InstallETCDBinaryModule] Install etcd using binary
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [InstallETCDBinaryModule] Generate etcd service
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [InstallETCDBinaryModule] Generate access address
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Health check on exist etcd
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
04:24:08 UTC skipped: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Join etcd member
04:24:08 UTC skipped: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Health check on new etcd
04:24:08 UTC skipped: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Check etcd member
04:24:08 UTC skipped: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDConfigureModule] Health check on all etcd
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDBackupModule] Backup etcd data regularly
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDBackupModule] Generate backup ETCD service
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDBackupModule] Generate backup ETCD timer
04:24:08 UTC success: [kubesphere-master-1]
04:24:08 UTC [ETCDBackupModule] Enable backup etcd service
04:24:09 UTC success: [kubesphere-master-1]
04:24:09 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC success: [kubesphere-worker-1]
04:24:13 UTC [InstallKubeBinariesModule] Change kubelet mode
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC success: [kubesphere-worker-1]
04:24:13 UTC [InstallKubeBinariesModule] Generate kubelet service
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC success: [kubesphere-worker-1]
04:24:13 UTC [InstallKubeBinariesModule] Enable kubelet service
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC success: [kubesphere-worker-1]
04:24:13 UTC [InstallKubeBinariesModule] Generate kubelet env
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC success: [kubesphere-worker-1]
04:24:13 UTC [InitKubernetesModule] Generate kubeadm config
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [InitKubernetesModule] Generate audit policy
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [InitKubernetesModule] Generate audit webhook
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [InitKubernetesModule] Init cluster using kubeadm
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [InitKubernetesModule] Remove master taint
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [ClusterDNSModule] Generate coredns service
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [ClusterDNSModule] Override coredns service
04:24:13 UTC skipped: [kubesphere-master-1]
04:24:13 UTC [ClusterDNSModule] Generate nodelocaldns
04:24:14 UTC success: [kubesphere-master-1]
04:24:14 UTC [ClusterDNSModule] Deploy nodelocaldns
04:24:14 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns unchanged
daemonset.apps/nodelocaldns unchanged
04:24:14 UTC success: [kubesphere-master-1]
04:24:14 UTC [ClusterDNSModule] Generate nodelocaldns configmap
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC [ClusterDNSModule] Apply nodelocaldns configmap
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:24:14 UTC stdout: [kubesphere-master-1]
v1.23.10
04:24:14 UTC stdout: [kubesphere-master-1]
node1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:node1 type:Hostname]]
04:24:14 UTC stdout: [kubesphere-master-1]
W0913 04:24:14.678874   64953 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
8f51d37bd252165542df8ef43cd4c881bf58f7251bb086e4a32a89844c41a057
04:24:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:24:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:24:14 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:24:14 UTC stdout: [kubesphere-master-1]
w9dkkf.lw2p1qnhc2pox5wt
04:24:14 UTC success: [kubesphere-master-1]
04:24:14 UTC [JoinNodesModule] Generate kubeadm config
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC success: [kubesphere-worker-1]
04:24:14 UTC [JoinNodesModule] Generate audit policy
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC [JoinNodesModule] Generate audit webhook
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC [JoinNodesModule] Join control-plane node
04:24:14 UTC skipped: [kubesphere-master-1]
04:24:14 UTC [JoinNodesModule] Join worker node
04:24:21 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 04:24:15.362006    3750 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0913 04:24:15.366008    3750 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:24:21 UTC skipped: [kubesphere-master-1]
04:24:21 UTC success: [kubesphere-worker-1]
04:24:21 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
04:24:21 UTC skipped: [kubesphere-master-1]
04:24:21 UTC [JoinNodesModule] Remove master taint
04:24:21 UTC skipped: [kubesphere-master-1]
04:24:21 UTC [JoinNodesModule] Add worker label to all nodes
04:24:21 UTC stdout: [kubesphere-master-1]
Error from server (NotFound): nodes "kubesphere-master-1" not found
04:24:21 UTC message: [kubesphere-master-1]
add worker label failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl label --overwrite node kubesphere-master-1 node-role.kubernetes.io/worker=" 
Error from server (NotFound): nodes "kubesphere-master-1" not found: Process exited with status 1
04:24:21 UTC retry: [kubesphere-master-1]
04:24:26 UTC stdout: [kubesphere-master-1]
Error from server (NotFound): nodes "kubesphere-master-1" not found
04:24:26 UTC message: [kubesphere-master-1]
add worker label failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl label --overwrite node kubesphere-master-1 node-role.kubernetes.io/worker=" 
Error from server (NotFound): nodes "kubesphere-master-1" not found: Process exited with status 1
04:24:26 UTC retry: [kubesphere-master-1]
04:24:31 UTC stdout: [kubesphere-master-1]
Error from server (NotFound): nodes "kubesphere-master-1" not found
04:24:31 UTC message: [kubesphere-master-1]
add worker label failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl label --overwrite node kubesphere-master-1 node-role.kubernetes.io/worker=" 
Error from server (NotFound): nodes "kubesphere-master-1" not found: Process exited with status 1
04:24:31 UTC failed: [kubesphere-master-1]
04:26:51 UTC [GreetingsModule] Greetings
04:26:52 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
04:26:52 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
04:26:52 UTC success: [kubesphere-worker-1]
04:26:52 UTC success: [kubesphere-master-1]
04:26:52 UTC [NodePreCheckModule] A pre-check on nodes
04:26:52 UTC success: [kubesphere-worker-1]
04:26:52 UTC success: [kubesphere-master-1]
04:26:52 UTC [ConfirmModule] Display confirmation form
04:26:56 UTC success: [LocalHost]
04:26:56 UTC [NodeBinariesModule] Download installation binaries
04:26:56 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
04:26:56 UTC message: [localhost]
kubeadm is existed
04:26:56 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
04:26:57 UTC message: [localhost]
kubelet is existed
04:26:57 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
04:26:57 UTC message: [localhost]
kubectl is existed
04:26:57 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
04:26:57 UTC message: [localhost]
helm is existed
04:26:57 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
04:26:57 UTC message: [localhost]
kubecni is existed
04:26:57 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
04:26:57 UTC message: [localhost]
crictl is existed
04:26:57 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
04:26:57 UTC message: [localhost]
etcd is existed
04:26:57 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
04:26:57 UTC message: [localhost]
containerd is existed
04:26:57 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
04:26:57 UTC message: [localhost]
runc is existed
04:26:57 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
04:26:57 UTC message: [localhost]
calicoctl is existed
04:26:57 UTC success: [LocalHost]
04:26:57 UTC [ConfigureOSModule] Get OS release
04:26:57 UTC success: [kubesphere-worker-1]
04:26:57 UTC success: [kubesphere-master-1]
04:26:57 UTC [ConfigureOSModule] Prepare to init OS
04:26:57 UTC success: [kubesphere-worker-1]
04:26:57 UTC success: [kubesphere-master-1]
04:26:57 UTC [ConfigureOSModule] Generate init os script
04:26:57 UTC success: [kubesphere-master-1]
04:26:57 UTC success: [kubesphere-worker-1]
04:26:57 UTC [ConfigureOSModule] Exec init os script
04:26:58 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 0
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.overcommit_memory = 0
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:26:58 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
04:26:58 UTC success: [kubesphere-master-1]
04:26:58 UTC success: [kubesphere-worker-1]
04:26:58 UTC [ConfigureOSModule] configure the ntp server for each node
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:26:58 UTC success: [kubesphere-master-1]
04:26:58 UTC [InstallContainerModule] Sync containerd binaries
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC [InstallContainerModule] Sync crictl binaries
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC [InstallContainerModule] Generate containerd service
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC [InstallContainerModule] Generate containerd config
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC [InstallContainerModule] Generate crictl config
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC [InstallContainerModule] Enable containerd
04:26:58 UTC skipped: [kubesphere-master-1]
04:26:58 UTC skipped: [kubesphere-worker-1]
04:26:58 UTC [PullModule] Start to pull images on all nodes
04:26:58 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
04:26:58 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
04:27:00 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:27:00 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
04:27:01 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
04:27:01 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
04:27:03 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:27:03 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
04:27:04 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
04:27:05 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
04:27:06 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
04:27:06 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
04:27:07 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
04:27:08 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
04:27:09 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:27:09 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
04:27:11 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
04:27:12 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
04:27:14 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
04:27:15 UTC success: [kubesphere-worker-1]
04:27:15 UTC success: [kubesphere-master-1]
04:27:15 UTC [ETCDPreCheckModule] Get etcd status
04:27:15 UTC success: [kubesphere-master-1]
04:27:15 UTC [CertsModule] Fetch etcd certs
04:27:15 UTC success: [kubesphere-master-1]
04:27:15 UTC [CertsModule] Generate etcd Certs
04:27:15 UTC success: [LocalHost]
04:27:15 UTC [CertsModule] Synchronize certs file
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [CertsModule] Synchronize certs file to master
04:27:16 UTC skipped: [kubesphere-master-1]
04:27:16 UTC [InstallETCDBinaryModule] Install etcd using binary
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [InstallETCDBinaryModule] Generate etcd service
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [InstallETCDBinaryModule] Generate access address
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [ETCDConfigureModule] Health check on exist etcd
04:27:16 UTC skipped: [kubesphere-master-1]
04:27:16 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
04:27:16 UTC success: [kubesphere-master-1]
04:27:16 UTC [ETCDConfigureModule] Restart etcd
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDConfigureModule] Health check on all etcd
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDConfigureModule] Health check on all etcd
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDBackupModule] Backup etcd data regularly
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDBackupModule] Generate backup ETCD service
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDBackupModule] Generate backup ETCD timer
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [ETCDBackupModule] Enable backup etcd service
04:27:21 UTC success: [kubesphere-master-1]
04:27:21 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
04:27:25 UTC success: [kubesphere-master-1]
04:27:25 UTC success: [kubesphere-worker-1]
04:27:25 UTC [InstallKubeBinariesModule] Change kubelet mode
04:27:25 UTC success: [kubesphere-worker-1]
04:27:25 UTC success: [kubesphere-master-1]
04:27:25 UTC [InstallKubeBinariesModule] Generate kubelet service
04:27:26 UTC success: [kubesphere-worker-1]
04:27:26 UTC success: [kubesphere-master-1]
04:27:26 UTC [InstallKubeBinariesModule] Enable kubelet service
04:27:26 UTC success: [kubesphere-master-1]
04:27:26 UTC success: [kubesphere-worker-1]
04:27:26 UTC [InstallKubeBinariesModule] Generate kubelet env
04:27:26 UTC success: [kubesphere-worker-1]
04:27:26 UTC success: [kubesphere-master-1]
04:27:26 UTC [InitKubernetesModule] Generate kubeadm config
04:27:26 UTC success: [kubesphere-master-1]
04:27:26 UTC [InitKubernetesModule] Generate audit policy
04:27:26 UTC skipped: [kubesphere-master-1]
04:27:26 UTC [InitKubernetesModule] Generate audit webhook
04:27:26 UTC skipped: [kubesphere-master-1]
04:27:26 UTC [InitKubernetesModule] Init cluster using kubeadm
04:27:40 UTC stdout: [kubesphere-master-1]
W0913 04:27:26.540008    4245 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 10.004369 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: oymptr.umww0ccyt4qpyewb
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token oymptr.umww0ccyt4qpyewb \
	--discovery-token-ca-cert-hash sha256:de3394f244ed3861ed5837b6334a47dea1fbcec2d00791a634ce90d4b2b41750 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token oymptr.umww0ccyt4qpyewb \
	--discovery-token-ca-cert-hash sha256:de3394f244ed3861ed5837b6334a47dea1fbcec2d00791a634ce90d4b2b41750
04:27:40 UTC success: [kubesphere-master-1]
04:27:40 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
04:27:40 UTC success: [kubesphere-master-1]
04:27:40 UTC [InitKubernetesModule] Remove master taint
04:27:40 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
04:27:40 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
04:27:40 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
04:27:40 UTC success: [kubesphere-master-1]
04:27:40 UTC [ClusterDNSModule] Generate coredns service
04:27:40 UTC success: [kubesphere-master-1]
04:27:40 UTC [ClusterDNSModule] Override coredns service
04:27:40 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
04:27:41 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [ClusterDNSModule] Generate nodelocaldns
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [ClusterDNSModule] Deploy nodelocaldns
04:27:41 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [ClusterDNSModule] Generate nodelocaldns configmap
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [ClusterDNSModule] Apply nodelocaldns configmap
04:27:41 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [KubernetesStatusModule] Get kubernetes cluster status
04:27:41 UTC stdout: [kubesphere-master-1]
v1.23.10
04:27:41 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
04:27:41 UTC stdout: [kubesphere-master-1]
W0913 04:27:41.460775    4985 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
1976fdf103daa30ed84340048dcada9873c5040ee270d32c99419734fbf34d25
04:27:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:27:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:27:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
04:27:41 UTC stdout: [kubesphere-master-1]
hj51hb.9tz79wl17bzpzzo6
04:27:41 UTC success: [kubesphere-master-1]
04:27:41 UTC [JoinNodesModule] Generate kubeadm config
04:27:41 UTC skipped: [kubesphere-master-1]
04:27:41 UTC success: [kubesphere-worker-1]
04:27:41 UTC [JoinNodesModule] Generate audit policy
04:27:41 UTC skipped: [kubesphere-master-1]
04:27:41 UTC [JoinNodesModule] Generate audit webhook
04:27:41 UTC skipped: [kubesphere-master-1]
04:27:41 UTC [JoinNodesModule] Join control-plane node
04:27:41 UTC skipped: [kubesphere-master-1]
04:27:41 UTC [JoinNodesModule] Join worker node
04:27:41 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
04:27:42 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
W0913 04:27:41.827898    7382 removeetcdmember.go:80] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.
04:27:42 UTC message: [kubesphere-worker-1]
join node failed: Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubeadm join --config=/etc/kubernetes/kubeadm-config.yaml --ignore-preflight-errors=FileExisting-crictl,ImagePull" 
[preflight] Running pre-flight checks
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR FileAvailable--etc-kubernetes-kubelet.conf]: /etc/kubernetes/kubelet.conf already exists
	[ERROR Port-10250]: Port 10250 is in use
	[ERROR FileAvailable--etc-kubernetes-pki-ca.crt]: /etc/kubernetes/pki/ca.crt already exists
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher: Process exited with status 1
04:27:42 UTC retry: [kubesphere-worker-1]
04:28:05 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 04:28:00.169256    7617 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0913 04:28:00.172898    7617 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
04:28:05 UTC skipped: [kubesphere-master-1]
04:28:05 UTC success: [kubesphere-worker-1]
04:28:05 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
04:28:05 UTC skipped: [kubesphere-master-1]
04:28:05 UTC [JoinNodesModule] Remove master taint
04:28:05 UTC skipped: [kubesphere-master-1]
04:28:05 UTC [JoinNodesModule] Add worker label to all nodes
04:28:05 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
04:28:06 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [DeployNetworkPluginModule] Generate calico
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [DeployNetworkPluginModule] Deploy calico
04:28:06 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [ConfigureKubernetesModule] Configure kubernetes
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [ChownModule] Chown user $HOME/.kube dir
04:28:06 UTC success: [kubesphere-worker-1]
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [AutoRenewCertsModule] Generate k8s certs renew script
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [AutoRenewCertsModule] Generate k8s certs renew service
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
04:28:06 UTC success: [kubesphere-master-1]
04:28:06 UTC [AutoRenewCertsModule] Enable k8s certs renew service
04:28:07 UTC success: [kubesphere-master-1]
04:28:07 UTC [SaveKubeConfigModule] Save kube config as a configmap
04:28:07 UTC success: [LocalHost]
04:28:07 UTC [AddonsModule] Install addons
04:28:07 UTC success: [LocalHost]
04:28:07 UTC Pipeline[CreateClusterPipeline] execute successfully
05:43:32 UTC [GreetingsModule] Greetings
05:43:32 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
05:43:33 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
05:43:33 UTC success: [kubesphere-worker-1]
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [NodePreCheckModule] A pre-check on nodes
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC success: [kubesphere-worker-1]
05:43:33 UTC [ClusterPreCheckModule] Get KubeConfig file
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Get all nodes Kubernetes version
05:43:33 UTC success: [kubesphere-worker-1]
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Calculate min Kubernetes version
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Check desired Kubernetes version
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Check KubeSphere version
05:43:33 UTC success: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Check dependency matrix for KubeSphere and Kubernetes
05:43:33 UTC skipped: [kubesphere-master-1]
05:43:33 UTC [ClusterPreCheckModule] Get kubernetes nodes status
05:43:34 UTC success: [kubesphere-master-1]
05:43:34 UTC [UpgradeConfirmModule] Display confirmation form
05:43:37 UTC success: [LocalHost]
05:43:37 UTC [SetUpgradePlanModule 1/2] Set upgrade plan
05:43:37 UTC success: [LocalHost]
05:43:37 UTC [SetUpgradePlanModule 1/2] Generate kubeadm config
05:43:37 UTC success: [kubesphere-master-1]
05:43:37 UTC [ProgressiveUpgradeModule 1/2] Calculate next upgrade version
05:43:37 UTC success: [LocalHost]
05:43:37 UTC [ProgressiveUpgradeModule 1/2] Download installation binaries
05:43:37 UTC message: [localhost]
downloading amd64 kubeadm v1.24.1 ...
05:43:39 UTC message: [localhost]
downloading amd64 kubelet v1.24.1 ...
05:43:45 UTC message: [localhost]
downloading amd64 kubectl v1.24.1 ...
05:43:47 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
05:43:47 UTC message: [localhost]
helm is existed
05:43:47 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
05:43:47 UTC message: [localhost]
kubecni is existed
05:43:47 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
05:43:47 UTC message: [localhost]
crictl is existed
05:43:47 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
05:43:47 UTC message: [localhost]
etcd is existed
05:43:47 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
05:43:47 UTC message: [localhost]
containerd is existed
05:43:47 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
05:43:47 UTC message: [localhost]
runc is existed
05:43:47 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
05:43:47 UTC message: [localhost]
calicoctl is existed
05:43:47 UTC success: [LocalHost]
05:43:47 UTC [ProgressiveUpgradeModule 1/2] Start to pull images on all nodes
05:43:47 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.7
05:43:47 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.7
05:43:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.24.1
05:43:51 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.24.1
05:43:55 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.24.1
05:43:58 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
05:44:00 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
05:44:02 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
05:44:03 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.24.1
05:44:03 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
05:44:04 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
05:44:06 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
05:44:09 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.24.1
05:44:16 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
05:44:17 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
05:44:19 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
05:44:20 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
05:44:22 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
05:44:23 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
05:44:25 UTC success: [kubesphere-worker-1]
05:44:25 UTC success: [kubesphere-master-1]
05:44:25 UTC [ProgressiveUpgradeModule 1/2] Synchronize kubernetes binaries
05:44:29 UTC success: [kubesphere-master-1]
05:44:29 UTC success: [kubesphere-worker-1]
05:44:29 UTC [ProgressiveUpgradeModule 1/2] Upgrade cluster on master
05:46:02 UTC success: [kubesphere-master-1]
05:46:02 UTC [ProgressiveUpgradeModule 1/2] Get kubernetes cluster status
05:46:02 UTC stdout: [kubesphere-master-1]
v1.24.1
05:46:02 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1    [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.23.10   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
05:46:02 UTC stdout: [kubesphere-master-1]
W0913 05:46:02.869163   49560 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 05:46:02.869649   49560 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 05:46:02.870459   49560 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
ccdcdea25d9423f2a62c056c47108e1954352c3beb30c80aa89309987aeedd7f
05:46:02 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
05:46:02 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
05:46:03 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
05:46:03 UTC stdout: [kubesphere-master-1]
ng8bke.wxpvm99w9ywg3cgj
05:46:03 UTC success: [kubesphere-master-1]
05:46:03 UTC [ProgressiveUpgradeModule 1/2] Upgrade cluster on worker
05:46:03 UTC stdout: [kubesphere-worker-1]
[upgrade] Reading configuration from the cluster...
[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 05:46:03.125580    8032 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[preflight] Running pre-flight checks
[preflight] Skipping prepull. Not a control plane node.
[upgrade] Skipping phase. Not a control plane node.
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[upgrade] The configuration for this node was successfully updated!
[upgrade] Now you should go ahead and upgrade the kubelet package using your package manager.
05:46:14 UTC skipped: [kubesphere-master-1]
05:46:14 UTC success: [kubesphere-worker-1]
05:46:14 UTC [ProgressiveUpgradeModule 1/2] Reconfigure CoreDNS
05:46:14 UTC stdout: [kubesphere-master-1]
deployment.apps/coredns patched (no change)
05:46:14 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
05:46:14 UTC stdout: [kubesphere-master-1]
service/coredns unchanged
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
05:46:15 UTC stdout: [kubesphere-master-1]
Error from server (NotFound): services "kube-dns" not found
05:46:15 UTC stdout: [kubesphere-master-1]
service/coredns unchanged
clusterrole.rbac.authorization.k8s.io/system:coredns unchanged
05:46:15 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns unchanged
daemonset.apps/nodelocaldns unchanged
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 1/2] Set current k8s version
05:46:15 UTC success: [LocalHost]
05:46:15 UTC [SetUpgradePlanModule 2/2] Set upgrade plan
05:46:15 UTC success: [LocalHost]
05:46:15 UTC [SetUpgradePlanModule 2/2] Generate kubeadm config
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Calculate next upgrade version
05:46:15 UTC skipped: [LocalHost]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Download installation binaries
05:46:15 UTC skipped: [LocalHost]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Start to pull images on all nodes
05:46:15 UTC skipped: [kubesphere-worker-1]
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Synchronize kubernetes binaries
05:46:15 UTC skipped: [kubesphere-worker-1]
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Upgrade cluster on master
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Get kubernetes cluster status
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Upgrade cluster on worker
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC skipped: [kubesphere-worker-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Reconfigure CoreDNS
05:46:15 UTC skipped: [kubesphere-master-1]
05:46:15 UTC [ProgressiveUpgradeModule 2/2] Set current k8s version
05:46:15 UTC skipped: [LocalHost]
05:46:15 UTC [ChownModule] Chown user $HOME/.kube dir
05:46:15 UTC success: [kubesphere-worker-1]
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [AutoRenewCertsModule] Generate k8s certs renew script
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [AutoRenewCertsModule] Generate k8s certs renew service
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC [AutoRenewCertsModule] Enable k8s certs renew service
05:46:15 UTC success: [kubesphere-master-1]
05:46:15 UTC Pipeline[UpgradeClusterPipeline] execute successfully
06:01:25 UTC [GreetingsModule] Greetings
06:01:25 UTC message: [kw01-base]
Greetings, KubeKey!
06:01:25 UTC success: [kw01-base]
06:01:25 UTC [DeleteClusterConfirmModule] Display confirmation form
06:01:27 UTC success: [LocalHost]
06:01:27 UTC [ResetClusterModule] Reset the cluster using kubeadm
06:01:27 UTC stdout: [kw01-base]
/bin/bash: line 1: /usr/local/bin/kubeadm: No such file or directory
06:01:27 UTC success: [kw01-base]
06:01:27 UTC [ClearOSModule] Reset os network config
06:01:27 UTC stdout: [kw01-base]
/bin/bash: line 1: ipvsadm: command not found
06:01:27 UTC stdout: [kw01-base]
Cannot find device "kube-ipvs0"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "nodelocaldns"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "cni0"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "flannel.1"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "flannel-v6.1"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "flannel-wg"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "flannel-wg-v6"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "cilium_host"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "cilium_vxlan"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "vxlan.calico"
06:01:27 UTC stdout: [kw01-base]
Cannot find device "vxlan-v6.calico"
06:01:27 UTC success: [kw01-base]
06:01:27 UTC [ClearOSModule] Uninstall etcd
06:01:27 UTC success: [kw01-base]
06:01:27 UTC [ClearOSModule] Remove cluster files
06:01:27 UTC success: [kw01-base]
06:01:27 UTC [ClearOSModule] Systemd daemon reload
06:01:28 UTC success: [kw01-base]
06:01:28 UTC [UninstallAutoRenewCertsModule] UnInstall auto renew control-plane certs
06:01:28 UTC skipped: [kw01-base]
06:01:28 UTC Pipeline[DeleteClusterPipeline] execute successfully
06:01:36 UTC [GreetingsModule] Greetings
06:01:36 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
06:01:36 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
06:01:36 UTC success: [kubesphere-worker-1]
06:01:36 UTC success: [kubesphere-master-1]
06:01:36 UTC [NodePreCheckModule] A pre-check on nodes
06:01:37 UTC success: [kubesphere-worker-1]
06:01:37 UTC success: [kubesphere-master-1]
06:01:37 UTC [ConfirmModule] Display confirmation form
06:01:39 UTC success: [LocalHost]
06:01:39 UTC [NodeBinariesModule] Download installation binaries
06:01:39 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
06:01:39 UTC message: [localhost]
kubeadm is existed
06:01:39 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
06:01:39 UTC message: [localhost]
kubelet is existed
06:01:39 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
06:01:39 UTC message: [localhost]
kubectl is existed
06:01:39 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
06:01:39 UTC message: [localhost]
helm is existed
06:01:39 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
06:01:39 UTC message: [localhost]
kubecni is existed
06:01:39 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
06:01:39 UTC message: [localhost]
crictl is existed
06:01:39 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
06:01:39 UTC message: [localhost]
etcd is existed
06:01:39 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
06:01:40 UTC message: [localhost]
containerd is existed
06:01:40 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
06:01:40 UTC message: [localhost]
runc is existed
06:01:40 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
06:01:40 UTC message: [localhost]
calicoctl is existed
06:01:40 UTC success: [LocalHost]
06:01:40 UTC [ConfigureOSModule] Get OS release
06:01:40 UTC success: [kubesphere-master-1]
06:01:40 UTC success: [kubesphere-worker-1]
06:01:40 UTC [ConfigureOSModule] Prepare to init OS
06:01:40 UTC success: [kubesphere-worker-1]
06:01:40 UTC success: [kubesphere-master-1]
06:01:40 UTC [ConfigureOSModule] Generate init os script
06:01:40 UTC success: [kubesphere-worker-1]
06:01:40 UTC success: [kubesphere-master-1]
06:01:40 UTC [ConfigureOSModule] Exec init os script
06:01:41 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 0
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.overcommit_memory = 0
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:01:41 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:01:41 UTC success: [kubesphere-master-1]
06:01:41 UTC success: [kubesphere-worker-1]
06:01:41 UTC [ConfigureOSModule] configure the ntp server for each node
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [KubernetesStatusModule] Get kubernetes cluster status
06:01:41 UTC stdout: [kubesphere-master-1]
v1.24.1
06:01:41 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.24.1   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
06:01:41 UTC stdout: [kubesphere-master-1]
W0913 06:01:41.387237   58925 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:01:41.387787   58925 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:01:41.392374   58925 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
56205dd25cc7decb378bd58884a198f5611dec52edc32c6ecf7b820708dc37fe
06:01:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:01:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:01:41 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:01:41 UTC stdout: [kubesphere-master-1]
afhs6g.qlc5u5bdsvsn5qo6
06:01:41 UTC success: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Sync containerd binaries
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Sync crictl binaries
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Generate containerd service
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Generate containerd config
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Generate crictl config
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [InstallContainerModule] Enable containerd
06:01:41 UTC skipped: [kubesphere-worker-1]
06:01:41 UTC skipped: [kubesphere-master-1]
06:01:41 UTC [PullModule] Start to pull images on all nodes
06:01:41 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
06:01:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
06:02:11 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0913 06:02:11.637677   58954 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to resolve reference \"docker.io/kubesphere/pause:3.6\": failed to do request: Head \"https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6\": dial tcp 3.216.34.172:443: i/o timeout" image="kubesphere/pause:3.6"
[31mFATA[0m[0030] pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to resolve reference "docker.io/kubesphere/pause:3.6": failed to do request: Head "https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6": dial tcp 3.216.34.172:443: i/o timeout: Process exited with status 1
06:02:11 UTC retry: [kubesphere-master-1]
06:02:16 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
06:02:41 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0913 06:02:41.658932   21902 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to resolve reference \"docker.io/kubesphere/pause:3.6\": failed to do request: Head \"https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6\": dial tcp 34.205.13.154:443: i/o timeout" image="kubesphere/pause:3.6"
[31mFATA[0m[0060] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to resolve reference "docker.io/kubesphere/pause:3.6": failed to do request: Head "https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6": dial tcp 34.205.13.154:443: i/o timeout: Process exited with status 1
06:02:41 UTC retry: [kubesphere-worker-1]
06:02:46 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
E0913 06:02:46.665095   59277 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = DeadlineExceeded desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to resolve reference \"docker.io/kubesphere/pause:3.6\": failed to do request: Head \"https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6\": dial tcp 34.205.13.154:443: i/o timeout" image="kubesphere/pause:3.6"
[31mFATA[0m[0030] pulling image: rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to resolve reference "docker.io/kubesphere/pause:3.6": failed to do request: Head "https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6": dial tcp 34.205.13.154:443: i/o timeout: Process exited with status 1
06:02:46 UTC retry: [kubesphere-master-1]
06:02:46 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
06:02:51 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
06:04:46 UTC [GreetingsModule] Greetings
06:04:47 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
06:04:47 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
06:04:47 UTC success: [kubesphere-worker-1]
06:04:47 UTC success: [kubesphere-master-1]
06:04:47 UTC [NodePreCheckModule] A pre-check on nodes
06:04:47 UTC success: [kubesphere-master-1]
06:04:47 UTC success: [kubesphere-worker-1]
06:04:47 UTC [ConfirmModule] Display confirmation form
06:04:51 UTC success: [LocalHost]
06:04:51 UTC [NodeBinariesModule] Download installation binaries
06:04:51 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
06:04:51 UTC message: [localhost]
kubeadm is existed
06:04:51 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
06:04:52 UTC message: [localhost]
kubelet is existed
06:04:52 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
06:04:52 UTC message: [localhost]
kubectl is existed
06:04:52 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
06:04:52 UTC message: [localhost]
helm is existed
06:04:52 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
06:04:52 UTC message: [localhost]
kubecni is existed
06:04:52 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
06:04:52 UTC message: [localhost]
crictl is existed
06:04:52 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
06:04:52 UTC message: [localhost]
etcd is existed
06:04:52 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
06:04:52 UTC message: [localhost]
containerd is existed
06:04:52 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
06:04:52 UTC message: [localhost]
runc is existed
06:04:52 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
06:04:52 UTC message: [localhost]
calicoctl is existed
06:04:52 UTC success: [LocalHost]
06:04:52 UTC [ConfigureOSModule] Get OS release
06:04:52 UTC success: [kubesphere-master-1]
06:04:52 UTC success: [kubesphere-worker-1]
06:04:52 UTC [ConfigureOSModule] Prepare to init OS
06:04:52 UTC success: [kubesphere-worker-1]
06:04:52 UTC success: [kubesphere-master-1]
06:04:52 UTC [ConfigureOSModule] Generate init os script
06:04:52 UTC success: [kubesphere-master-1]
06:04:52 UTC success: [kubesphere-worker-1]
06:04:52 UTC [ConfigureOSModule] Exec init os script
06:04:53 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:04:53 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 0
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.overcommit_memory = 0
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:04:53 UTC success: [kubesphere-worker-1]
06:04:53 UTC success: [kubesphere-master-1]
06:04:53 UTC [ConfigureOSModule] configure the ntp server for each node
06:04:53 UTC skipped: [kubesphere-worker-1]
06:04:53 UTC skipped: [kubesphere-master-1]
06:04:53 UTC [KubernetesStatusModule] Get kubernetes cluster status
06:04:53 UTC stdout: [kubesphere-master-1]
v1.24.1
06:04:53 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.24.1   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
06:04:53 UTC stdout: [kubesphere-master-1]
W0913 06:04:53.859586   61259 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:04:53.860171   61259 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:04:53.863740   61259 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
0467a108b1e7c76788e10d6edf550369bd5b5d022f9b9a828bc61adb0a928fe6
06:04:53 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:04:54 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:04:54 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:04:54 UTC stdout: [kubesphere-master-1]
m4cgpt.m19jxqjkakkhq7tu
06:04:54 UTC success: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Sync containerd binaries
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Sync crictl binaries
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Generate containerd service
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Generate containerd config
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Generate crictl config
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [InstallContainerModule] Enable containerd
06:04:54 UTC skipped: [kubesphere-worker-1]
06:04:54 UTC skipped: [kubesphere-master-1]
06:04:54 UTC [PullModule] Start to pull images on all nodes
06:04:54 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
06:04:54 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
06:05:44 UTC [GreetingsModule] Greetings
06:05:45 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
06:05:45 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
06:05:45 UTC success: [kubesphere-worker-1]
06:05:45 UTC success: [kubesphere-master-1]
06:05:45 UTC [NodePreCheckModule] A pre-check on nodes
06:05:45 UTC success: [kubesphere-worker-1]
06:05:45 UTC success: [kubesphere-master-1]
06:05:45 UTC [ConfirmModule] Display confirmation form
06:05:47 UTC success: [LocalHost]
06:05:47 UTC [NodeBinariesModule] Download installation binaries
06:05:47 UTC message: [localhost]
downloading amd64 kubeadm v1.24.1 ...
06:05:47 UTC message: [localhost]
kubeadm is existed
06:05:47 UTC message: [localhost]
downloading amd64 kubelet v1.24.1 ...
06:05:47 UTC message: [localhost]
kubelet is existed
06:05:47 UTC message: [localhost]
downloading amd64 kubectl v1.24.1 ...
06:05:47 UTC message: [localhost]
kubectl is existed
06:05:47 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
06:05:47 UTC message: [localhost]
helm is existed
06:05:47 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
06:05:48 UTC message: [localhost]
kubecni is existed
06:05:48 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
06:05:48 UTC message: [localhost]
crictl is existed
06:05:48 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
06:05:48 UTC message: [localhost]
etcd is existed
06:05:48 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
06:05:48 UTC message: [localhost]
containerd is existed
06:05:48 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
06:05:48 UTC message: [localhost]
runc is existed
06:05:48 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
06:05:48 UTC message: [localhost]
calicoctl is existed
06:05:48 UTC success: [LocalHost]
06:05:48 UTC [ConfigureOSModule] Get OS release
06:05:48 UTC success: [kubesphere-master-1]
06:05:48 UTC success: [kubesphere-worker-1]
06:05:48 UTC [ConfigureOSModule] Prepare to init OS
06:05:48 UTC success: [kubesphere-master-1]
06:05:48 UTC success: [kubesphere-worker-1]
06:05:48 UTC [ConfigureOSModule] Generate init os script
06:05:48 UTC success: [kubesphere-worker-1]
06:05:48 UTC success: [kubesphere-master-1]
06:05:48 UTC [ConfigureOSModule] Exec init os script
06:05:49 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:05:49 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
vm.max_map_count = 262144
vm.swappiness = 0
fs.inotify.max_user_instances = 524288
kernel.pid_max = 65535
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.overcommit_memory = 0
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
06:05:49 UTC success: [kubesphere-worker-1]
06:05:49 UTC success: [kubesphere-master-1]
06:05:49 UTC [ConfigureOSModule] configure the ntp server for each node
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [KubernetesStatusModule] Get kubernetes cluster status
06:05:49 UTC stdout: [kubesphere-master-1]
v1.24.1
06:05:49 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.24.1   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
06:05:49 UTC stdout: [kubesphere-master-1]
W0913 06:05:49.464516   62236 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:05:49.465016   62236 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 06:05:49.465843   62236 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
213128272f8dd949cbfe6e8557a9693317010e0da372396d8dd94363e275307c
06:05:49 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:05:49 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:05:49 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
06:05:49 UTC stdout: [kubesphere-master-1]
ocbt64.p1a2j6bkfi0ot9ko
06:05:49 UTC success: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Sync containerd binaries
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Sync crictl binaries
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Generate containerd service
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Generate containerd config
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Generate crictl config
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [InstallContainerModule] Enable containerd
06:05:49 UTC skipped: [kubesphere-worker-1]
06:05:49 UTC skipped: [kubesphere-master-1]
06:05:49 UTC [PullModule] Start to pull images on all nodes
06:05:49 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.7
06:05:49 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.7
14:37:50 UTC [GreetingsModule] Greetings
14:37:50 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
14:37:51 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
14:37:51 UTC success: [kubesphere-worker-1]
14:37:51 UTC success: [kubesphere-master-1]
14:37:51 UTC [NodePreCheckModule] A pre-check on nodes
14:37:51 UTC success: [kubesphere-master-1]
14:37:51 UTC success: [kubesphere-worker-1]
14:37:51 UTC [ConfirmModule] Display confirmation form
14:37:53 UTC success: [LocalHost]
14:37:53 UTC [NodeBinariesModule] Download installation binaries
14:37:53 UTC message: [localhost]
downloading amd64 kubeadm v1.24.1 ...
14:37:53 UTC message: [localhost]
kubeadm is existed
14:37:53 UTC message: [localhost]
downloading amd64 kubelet v1.24.1 ...
14:37:53 UTC message: [localhost]
kubelet is existed
14:37:53 UTC message: [localhost]
downloading amd64 kubectl v1.24.1 ...
14:37:53 UTC message: [localhost]
kubectl is existed
14:37:53 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
14:37:54 UTC message: [localhost]
helm is existed
14:37:54 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
14:37:54 UTC message: [localhost]
kubecni is existed
14:37:54 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
14:37:54 UTC message: [localhost]
crictl is existed
14:37:54 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
14:37:54 UTC message: [localhost]
etcd is existed
14:37:54 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
14:37:54 UTC message: [localhost]
containerd is existed
14:37:54 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
14:37:54 UTC message: [localhost]
runc is existed
14:37:54 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
14:37:54 UTC message: [localhost]
calicoctl is existed
14:37:54 UTC success: [LocalHost]
14:37:54 UTC [ConfigureOSModule] Get OS release
14:37:54 UTC success: [kubesphere-worker-1]
14:37:54 UTC success: [kubesphere-master-1]
14:37:54 UTC [ConfigureOSModule] Prepare to init OS
14:37:54 UTC success: [kubesphere-worker-1]
14:37:54 UTC success: [kubesphere-master-1]
14:37:54 UTC [ConfigureOSModule] Generate init os script
14:37:54 UTC success: [kubesphere-master-1]
14:37:54 UTC success: [kubesphere-worker-1]
14:37:54 UTC [ConfigureOSModule] Exec init os script
14:37:55 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:37:55 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:37:55 UTC success: [kubesphere-worker-1]
14:37:55 UTC success: [kubesphere-master-1]
14:37:55 UTC [ConfigureOSModule] configure the ntp server for each node
14:37:55 UTC skipped: [kubesphere-worker-1]
14:37:55 UTC skipped: [kubesphere-master-1]
14:37:55 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:37:55 UTC success: [kubesphere-master-1]
14:37:55 UTC [InstallContainerModule] Sync containerd binaries
14:37:55 UTC skipped: [kubesphere-master-1]
14:37:55 UTC skipped: [kubesphere-worker-1]
14:37:55 UTC [InstallContainerModule] Sync crictl binaries
14:37:56 UTC success: [kubesphere-worker-1]
14:37:56 UTC success: [kubesphere-master-1]
14:37:56 UTC [InstallContainerModule] Generate containerd service
14:37:56 UTC skipped: [kubesphere-worker-1]
14:37:56 UTC skipped: [kubesphere-master-1]
14:37:56 UTC [InstallContainerModule] Generate containerd config
14:37:56 UTC skipped: [kubesphere-worker-1]
14:37:56 UTC skipped: [kubesphere-master-1]
14:37:56 UTC [InstallContainerModule] Generate crictl config
14:37:56 UTC skipped: [kubesphere-worker-1]
14:37:56 UTC skipped: [kubesphere-master-1]
14:37:56 UTC [InstallContainerModule] Enable containerd
14:37:56 UTC skipped: [kubesphere-worker-1]
14:37:56 UTC skipped: [kubesphere-master-1]
14:37:56 UTC [PullModule] Start to pull images on all nodes
14:37:56 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.7
14:37:56 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.7
14:38:00 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.24.1
14:38:00 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.24.1
14:38:06 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
14:38:07 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.24.1
14:38:08 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:38:09 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
14:38:11 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
14:38:12 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
14:38:13 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.24.1
14:38:14 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:38:18 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.24.1
14:38:24 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
14:38:26 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:38:27 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
14:38:29 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
14:38:30 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
14:38:32 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:38:33 UTC success: [kubesphere-worker-1]
14:38:33 UTC success: [kubesphere-master-1]
14:38:33 UTC [ETCDPreCheckModule] Get etcd status
14:38:33 UTC success: [kubesphere-master-1]
14:38:33 UTC [CertsModule] Fetch etcd certs
14:38:33 UTC success: [kubesphere-master-1]
14:38:33 UTC [CertsModule] Generate etcd Certs
14:38:33 UTC success: [LocalHost]
14:38:33 UTC [CertsModule] Synchronize certs file
14:38:33 UTC success: [kubesphere-master-1]
14:38:33 UTC [CertsModule] Synchronize certs file to master
14:38:33 UTC skipped: [kubesphere-master-1]
14:38:33 UTC [InstallETCDBinaryModule] Install etcd using binary
14:38:34 UTC success: [kubesphere-master-1]
14:38:34 UTC [InstallETCDBinaryModule] Generate etcd service
14:38:34 UTC success: [kubesphere-master-1]
14:38:34 UTC [InstallETCDBinaryModule] Generate access address
14:38:34 UTC success: [kubesphere-master-1]
14:38:34 UTC [ETCDConfigureModule] Health check on exist etcd
14:38:34 UTC skipped: [kubesphere-master-1]
14:38:34 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
14:38:34 UTC success: [kubesphere-master-1]
14:38:34 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
14:38:34 UTC success: [kubesphere-master-1]
14:38:34 UTC [ETCDConfigureModule] Restart etcd
14:38:39 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDConfigureModule] Health check on all etcd
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDConfigureModule] Health check on all etcd
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDBackupModule] Backup etcd data regularly
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDBackupModule] Generate backup ETCD service
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDBackupModule] Generate backup ETCD timer
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [ETCDBackupModule] Enable backup etcd service
14:38:39 UTC success: [kubesphere-master-1]
14:38:39 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC success: [kubesphere-worker-1]
14:38:43 UTC [InstallKubeBinariesModule] Change kubelet mode
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC success: [kubesphere-worker-1]
14:38:43 UTC [InstallKubeBinariesModule] Generate kubelet service
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC success: [kubesphere-worker-1]
14:38:43 UTC [InstallKubeBinariesModule] Enable kubelet service
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC success: [kubesphere-worker-1]
14:38:43 UTC [InstallKubeBinariesModule] Generate kubelet env
14:38:43 UTC success: [kubesphere-worker-1]
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC [InitKubernetesModule] Generate kubeadm config
14:38:43 UTC success: [kubesphere-master-1]
14:38:43 UTC [InitKubernetesModule] Generate audit policy
14:38:43 UTC skipped: [kubesphere-master-1]
14:38:43 UTC [InitKubernetesModule] Generate audit webhook
14:38:43 UTC skipped: [kubesphere-master-1]
14:38:43 UTC [InitKubernetesModule] Init cluster using kubeadm
14:38:53 UTC stdout: [kubesphere-master-1]
W0913 14:38:43.749182    3669 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:38:43.749775    3669 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:38:43.750444    3669 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.24.1
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 6.502187 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: fvfrio.zsfitisz0o78jp8w
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token fvfrio.zsfitisz0o78jp8w \
	--discovery-token-ca-cert-hash sha256:cd64179c72be1385454789b8df655762d6fc80f13b5ba6055281d4dce5a704e8 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token fvfrio.zsfitisz0o78jp8w \
	--discovery-token-ca-cert-hash sha256:cd64179c72be1385454789b8df655762d6fc80f13b5ba6055281d4dce5a704e8
14:38:53 UTC success: [kubesphere-master-1]
14:38:53 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
14:38:53 UTC success: [kubesphere-master-1]
14:38:53 UTC [InitKubernetesModule] Remove master taint
14:38:54 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
14:38:54 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
14:38:54 UTC success: [kubesphere-master-1]
14:38:54 UTC [ClusterDNSModule] Generate coredns service
14:38:54 UTC success: [kubesphere-master-1]
14:38:54 UTC [ClusterDNSModule] Override coredns service
14:38:54 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
14:38:55 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [ClusterDNSModule] Generate nodelocaldns
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [ClusterDNSModule] Deploy nodelocaldns
14:38:55 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [ClusterDNSModule] Generate nodelocaldns configmap
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [ClusterDNSModule] Apply nodelocaldns configmap
14:38:55 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:38:55 UTC stdout: [kubesphere-master-1]
v1.24.1
14:38:55 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
14:38:55 UTC stdout: [kubesphere-master-1]
W0913 14:38:55.583075    4359 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:38:55.583669    4359 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:38:55.584390    4359 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
e433f74ed366e46f840220e773899e0afa39894a926740e6ad9da8f56b7e7146
14:38:55 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:38:55 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:38:55 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:38:55 UTC stdout: [kubesphere-master-1]
g9z5zc.3slxfqsa8oxje4o0
14:38:55 UTC success: [kubesphere-master-1]
14:38:55 UTC [JoinNodesModule] Generate kubeadm config
14:38:55 UTC skipped: [kubesphere-master-1]
14:38:55 UTC success: [kubesphere-worker-1]
14:38:55 UTC [JoinNodesModule] Generate audit policy
14:38:55 UTC skipped: [kubesphere-master-1]
14:38:55 UTC [JoinNodesModule] Generate audit webhook
14:38:55 UTC skipped: [kubesphere-master-1]
14:38:55 UTC [JoinNodesModule] Join control-plane node
14:38:55 UTC skipped: [kubesphere-master-1]
14:38:55 UTC [JoinNodesModule] Join worker node
14:39:22 UTC stdout: [kubesphere-worker-1]
W0913 14:38:55.862093    3200 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 14:39:08.280262    3200 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
14:39:22 UTC skipped: [kubesphere-master-1]
14:39:22 UTC success: [kubesphere-worker-1]
14:39:22 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
14:39:22 UTC skipped: [kubesphere-master-1]
14:39:22 UTC [JoinNodesModule] Remove master taint
14:39:22 UTC skipped: [kubesphere-master-1]
14:39:22 UTC [JoinNodesModule] Add worker label to all nodes
14:39:22 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
14:39:22 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
14:39:22 UTC success: [kubesphere-master-1]
14:39:22 UTC [DeployNetworkPluginModule] Generate calico
14:39:22 UTC success: [kubesphere-master-1]
14:39:22 UTC [DeployNetworkPluginModule] Deploy calico
14:39:23 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [ConfigureKubernetesModule] Configure kubernetes
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [ChownModule] Chown user $HOME/.kube dir
14:39:23 UTC success: [kubesphere-worker-1]
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [AutoRenewCertsModule] Generate k8s certs renew script
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [AutoRenewCertsModule] Generate k8s certs renew service
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [AutoRenewCertsModule] Enable k8s certs renew service
14:39:23 UTC success: [kubesphere-master-1]
14:39:23 UTC [SaveKubeConfigModule] Save kube config as a configmap
14:39:23 UTC success: [LocalHost]
14:39:23 UTC [AddonsModule] Install addons
14:39:23 UTC success: [LocalHost]
14:39:23 UTC Pipeline[CreateClusterPipeline] execute successfully
14:39:32 UTC [GreetingsModule] Greetings
14:39:32 UTC message: [kw01-base]
Greetings, KubeKey!
14:39:32 UTC success: [kw01-base]
14:39:32 UTC [DeleteClusterConfirmModule] Display confirmation form
14:39:34 UTC success: [LocalHost]
14:39:34 UTC [ResetClusterModule] Reset the cluster using kubeadm
14:39:34 UTC stdout: [kw01-base]
/bin/bash: line 1: /usr/local/bin/kubeadm: No such file or directory
14:39:34 UTC success: [kw01-base]
14:39:34 UTC [ClearOSModule] Reset os network config
14:39:34 UTC stdout: [kw01-base]
/bin/bash: line 1: ipvsadm: command not found
14:39:34 UTC stdout: [kw01-base]
Cannot find device "kube-ipvs0"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "nodelocaldns"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "cni0"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "flannel.1"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "flannel-v6.1"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "flannel-wg"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "flannel-wg-v6"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "cilium_host"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "cilium_vxlan"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "vxlan.calico"
14:39:34 UTC stdout: [kw01-base]
Cannot find device "vxlan-v6.calico"
14:39:34 UTC success: [kw01-base]
14:39:34 UTC [ClearOSModule] Uninstall etcd
14:39:34 UTC success: [kw01-base]
14:39:34 UTC [ClearOSModule] Remove cluster files
14:39:34 UTC success: [kw01-base]
14:39:34 UTC [ClearOSModule] Systemd daemon reload
14:39:34 UTC success: [kw01-base]
14:39:34 UTC [UninstallAutoRenewCertsModule] UnInstall auto renew control-plane certs
14:39:34 UTC skipped: [kw01-base]
14:39:34 UTC Pipeline[DeleteClusterPipeline] execute successfully
14:39:56 UTC [GreetingsModule] Greetings
14:39:57 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
14:39:57 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
14:39:57 UTC success: [kubesphere-master-1]
14:39:57 UTC success: [kubesphere-worker-1]
14:39:57 UTC [NodePreCheckModule] A pre-check on nodes
14:39:57 UTC success: [kubesphere-worker-1]
14:39:57 UTC success: [kubesphere-master-1]
14:39:57 UTC [ConfirmModule] Display confirmation form
14:39:57 UTC success: [LocalHost]
14:39:57 UTC [NodeBinariesModule] Download installation binaries
14:39:57 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
14:39:57 UTC message: [localhost]
kubeadm is existed
14:39:57 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
14:39:58 UTC message: [localhost]
kubelet is existed
14:39:58 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
14:39:58 UTC message: [localhost]
kubectl is existed
14:39:58 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
14:39:58 UTC message: [localhost]
helm is existed
14:39:58 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
14:39:58 UTC message: [localhost]
kubecni is existed
14:39:58 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
14:39:58 UTC message: [localhost]
crictl is existed
14:39:58 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
14:39:58 UTC message: [localhost]
etcd is existed
14:39:58 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
14:39:58 UTC message: [localhost]
containerd is existed
14:39:58 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
14:39:58 UTC message: [localhost]
runc is existed
14:39:58 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
14:39:58 UTC message: [localhost]
calicoctl is existed
14:39:58 UTC success: [LocalHost]
14:39:58 UTC [ConfigureOSModule] Get OS release
14:39:58 UTC success: [kubesphere-master-1]
14:39:58 UTC success: [kubesphere-worker-1]
14:39:58 UTC [ConfigureOSModule] Prepare to init OS
14:39:59 UTC success: [kubesphere-worker-1]
14:39:59 UTC success: [kubesphere-master-1]
14:39:59 UTC [ConfigureOSModule] Generate init os script
14:39:59 UTC success: [kubesphere-worker-1]
14:39:59 UTC success: [kubesphere-master-1]
14:39:59 UTC [ConfigureOSModule] Exec init os script
14:39:59 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:40:00 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:40:00 UTC success: [kubesphere-worker-1]
14:40:00 UTC success: [kubesphere-master-1]
14:40:00 UTC [ConfigureOSModule] configure the ntp server for each node
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:40:00 UTC stdout: [kubesphere-master-1]
v1.24.1
14:40:00 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.24.1   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.24.1   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
14:40:00 UTC stdout: [kubesphere-master-1]
W0913 14:40:00.153014    6731 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:40:00.153492    6731 common.go:83] your configuration file uses a deprecated API spec: "kubeadm.k8s.io/v1beta2". Please use 'kubeadm config migrate --old-config old.yaml --new-config new.yaml', which will write the new, similar spec using a newer API version.
W0913 14:40:00.154842    6731 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
52d36d1354771bb48032809ee4540f1fffc8b13f98974d49b5bac9270872dc20
14:40:00 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:40:00 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:40:00 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:40:00 UTC stdout: [kubesphere-master-1]
kwek0m.i6moo0snml5dcpl7
14:40:00 UTC success: [kubesphere-master-1]
14:40:00 UTC [InstallContainerModule] Sync containerd binaries
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [InstallContainerModule] Sync crictl binaries
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [InstallContainerModule] Generate containerd service
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [InstallContainerModule] Generate containerd config
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC [InstallContainerModule] Generate crictl config
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [InstallContainerModule] Enable containerd
14:40:00 UTC skipped: [kubesphere-worker-1]
14:40:00 UTC skipped: [kubesphere-master-1]
14:40:00 UTC [PullModule] Start to pull images on all nodes
14:40:00 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
14:40:00 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
14:40:30 UTC message: [kubesphere-master-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
[33mWARN[0m[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
[31mERRO[0m[0000] unable to determine image API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory" 
E0913 14:40:30.413956    6759 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to resolve reference \"docker.io/kubesphere/pause:3.6\": failed to do request: Head \"https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6\": dial tcp 34.205.13.154:443: i/o timeout" image="kubesphere/pause:3.6"
[31mFATA[0m[0030] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to resolve reference "docker.io/kubesphere/pause:3.6": failed to do request: Head "https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6": dial tcp 34.205.13.154:443: i/o timeout: Process exited with status 1
14:40:30 UTC retry: [kubesphere-master-1]
14:40:30 UTC message: [kubesphere-worker-1]
pull image failed: Failed to exec command: sudo -E /bin/bash -c "env PATH=$PATH crictl pull kubesphere/pause:3.6 --platform amd64" 
[33mWARN[0m[0000] image connect using default endpoints: [unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
[31mERRO[0m[0000] unable to determine image API version: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing dial unix /var/run/dockershim.sock: connect: no such file or directory" 
E0913 14:40:30.428867    4728 remote_image.go:238] "PullImage from image service failed" err="rpc error: code = Unknown desc = failed to pull and unpack image \"docker.io/kubesphere/pause:3.6\": failed to resolve reference \"docker.io/kubesphere/pause:3.6\": failed to do request: Head \"https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6\": dial tcp 44.205.64.79:443: i/o timeout" image="kubesphere/pause:3.6"
[31mFATA[0m[0030] pulling image: rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/kubesphere/pause:3.6": failed to resolve reference "docker.io/kubesphere/pause:3.6": failed to do request: Head "https://registry-1.docker.io/v2/kubesphere/pause/manifests/3.6": dial tcp 44.205.64.79:443: i/o timeout: Process exited with status 1
14:40:30 UTC retry: [kubesphere-worker-1]
14:40:35 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
14:40:35 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
14:50:06 UTC [GreetingsModule] Greetings
14:50:06 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
14:50:07 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
14:50:07 UTC success: [kubesphere-worker-1]
14:50:07 UTC success: [kubesphere-master-1]
14:50:07 UTC [NodePreCheckModule] A pre-check on nodes
14:50:07 UTC success: [kubesphere-worker-1]
14:50:07 UTC success: [kubesphere-master-1]
14:50:07 UTC [ConfirmModule] Display confirmation form
14:50:07 UTC success: [LocalHost]
14:50:07 UTC [NodeBinariesModule] Download installation binaries
14:50:07 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
14:50:07 UTC message: [localhost]
kubeadm is existed
14:50:07 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
14:50:07 UTC message: [localhost]
kubelet is existed
14:50:07 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
14:50:07 UTC message: [localhost]
kubectl is existed
14:50:07 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
14:50:07 UTC message: [localhost]
helm is existed
14:50:07 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
14:50:07 UTC message: [localhost]
kubecni is existed
14:50:07 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
14:50:07 UTC message: [localhost]
crictl is existed
14:50:07 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
14:50:07 UTC message: [localhost]
etcd is existed
14:50:07 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
14:50:08 UTC message: [localhost]
containerd is existed
14:50:08 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
14:50:08 UTC message: [localhost]
runc is existed
14:50:08 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
14:50:08 UTC message: [localhost]
calicoctl is existed
14:50:08 UTC success: [LocalHost]
14:50:08 UTC [ConfigureOSModule] Get OS release
14:50:08 UTC success: [kubesphere-master-1]
14:50:08 UTC success: [kubesphere-worker-1]
14:50:08 UTC [ConfigureOSModule] Prepare to init OS
14:50:08 UTC success: [kubesphere-worker-1]
14:50:08 UTC success: [kubesphere-master-1]
14:50:08 UTC [ConfigureOSModule] Generate init os script
14:50:08 UTC success: [kubesphere-master-1]
14:50:08 UTC success: [kubesphere-worker-1]
14:50:08 UTC [ConfigureOSModule] Exec init os script
14:50:09 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:50:09 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:50:09 UTC success: [kubesphere-master-1]
14:50:09 UTC success: [kubesphere-worker-1]
14:50:09 UTC [ConfigureOSModule] configure the ntp server for each node
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:50:09 UTC success: [kubesphere-master-1]
14:50:09 UTC [InstallContainerModule] Sync containerd binaries
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC [InstallContainerModule] Sync crictl binaries
14:50:09 UTC success: [kubesphere-master-1]
14:50:09 UTC success: [kubesphere-worker-1]
14:50:09 UTC [InstallContainerModule] Generate containerd service
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC [InstallContainerModule] Generate containerd config
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC [InstallContainerModule] Generate crictl config
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC [InstallContainerModule] Enable containerd
14:50:09 UTC skipped: [kubesphere-master-1]
14:50:09 UTC skipped: [kubesphere-worker-1]
14:50:09 UTC [PullModule] Start to pull images on all nodes
14:50:09 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
14:50:09 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
14:50:11 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
14:50:11 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
14:50:12 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
14:50:12 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
14:50:14 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
14:50:14 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:50:15 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
14:50:16 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
14:50:17 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
14:50:17 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
14:50:19 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:50:19 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
14:50:20 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
14:50:20 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:50:21 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
14:50:23 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
14:50:24 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:50:26 UTC success: [kubesphere-worker-1]
14:50:26 UTC success: [kubesphere-master-1]
14:50:26 UTC [ETCDPreCheckModule] Get etcd status
14:50:26 UTC success: [kubesphere-master-1]
14:50:26 UTC [CertsModule] Fetch etcd certs
14:50:26 UTC success: [kubesphere-master-1]
14:50:26 UTC [CertsModule] Generate etcd Certs
14:50:26 UTC success: [LocalHost]
14:50:26 UTC [CertsModule] Synchronize certs file
14:50:26 UTC success: [kubesphere-master-1]
14:50:26 UTC [CertsModule] Synchronize certs file to master
14:50:26 UTC skipped: [kubesphere-master-1]
14:50:26 UTC [InstallETCDBinaryModule] Install etcd using binary
14:50:27 UTC success: [kubesphere-master-1]
14:50:27 UTC [InstallETCDBinaryModule] Generate etcd service
14:50:27 UTC success: [kubesphere-master-1]
14:50:27 UTC [InstallETCDBinaryModule] Generate access address
14:50:27 UTC success: [kubesphere-master-1]
14:50:27 UTC [ETCDConfigureModule] Health check on exist etcd
14:50:27 UTC skipped: [kubesphere-master-1]
14:50:27 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
14:50:27 UTC success: [kubesphere-master-1]
14:50:27 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
14:50:27 UTC success: [kubesphere-master-1]
14:50:27 UTC [ETCDConfigureModule] Restart etcd
14:50:29 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDConfigureModule] Health check on all etcd
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDConfigureModule] Health check on all etcd
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDBackupModule] Backup etcd data regularly
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDBackupModule] Generate backup ETCD service
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDBackupModule] Generate backup ETCD timer
14:50:29 UTC success: [kubesphere-master-1]
14:50:29 UTC [ETCDBackupModule] Enable backup etcd service
14:50:30 UTC success: [kubesphere-master-1]
14:50:30 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
14:50:33 UTC success: [kubesphere-master-1]
14:50:33 UTC success: [kubesphere-worker-1]
14:50:33 UTC [InstallKubeBinariesModule] Change kubelet mode
14:50:33 UTC success: [kubesphere-worker-1]
14:50:33 UTC success: [kubesphere-master-1]
14:50:33 UTC [InstallKubeBinariesModule] Generate kubelet service
14:50:33 UTC success: [kubesphere-master-1]
14:50:33 UTC success: [kubesphere-worker-1]
14:50:33 UTC [InstallKubeBinariesModule] Enable kubelet service
14:50:34 UTC success: [kubesphere-master-1]
14:50:34 UTC success: [kubesphere-worker-1]
14:50:34 UTC [InstallKubeBinariesModule] Generate kubelet env
14:50:34 UTC success: [kubesphere-master-1]
14:50:34 UTC success: [kubesphere-worker-1]
14:50:34 UTC [InitKubernetesModule] Generate kubeadm config
14:50:34 UTC success: [kubesphere-master-1]
14:50:34 UTC [InitKubernetesModule] Generate audit policy
14:50:34 UTC skipped: [kubesphere-master-1]
14:50:34 UTC [InitKubernetesModule] Generate audit webhook
14:50:34 UTC skipped: [kubesphere-master-1]
14:50:34 UTC [InitKubernetesModule] Init cluster using kubeadm
14:50:48 UTC stdout: [kubesphere-master-1]
W0913 14:50:34.402698    3368 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.502754 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 6cmu25.tfeb4fe7yezycdsn
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 6cmu25.tfeb4fe7yezycdsn \
	--discovery-token-ca-cert-hash sha256:4bd91a4c3f17b3a1a8c7b85ad4b2119312790922dd75385796a88800e141dc0e \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 6cmu25.tfeb4fe7yezycdsn \
	--discovery-token-ca-cert-hash sha256:4bd91a4c3f17b3a1a8c7b85ad4b2119312790922dd75385796a88800e141dc0e
14:50:48 UTC success: [kubesphere-master-1]
14:50:48 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
14:50:48 UTC success: [kubesphere-master-1]
14:50:48 UTC [InitKubernetesModule] Remove master taint
14:50:49 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
14:50:49 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
14:50:49 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
14:50:49 UTC success: [kubesphere-master-1]
14:50:49 UTC [ClusterDNSModule] Generate coredns service
14:50:49 UTC success: [kubesphere-master-1]
14:50:49 UTC [ClusterDNSModule] Override coredns service
14:50:49 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
14:50:50 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [ClusterDNSModule] Generate nodelocaldns
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [ClusterDNSModule] Deploy nodelocaldns
14:50:50 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [ClusterDNSModule] Generate nodelocaldns configmap
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [ClusterDNSModule] Apply nodelocaldns configmap
14:50:50 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:50:50 UTC stdout: [kubesphere-master-1]
v1.23.10
14:50:50 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
14:50:50 UTC stdout: [kubesphere-master-1]
W0913 14:50:50.695219    4031 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
59d1af1da7f0547500e16d05a8602c254ddf5112d460097c4e85ec40e4a20d74
14:50:50 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:50:50 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:50:50 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:50:50 UTC stdout: [kubesphere-master-1]
m1hw3l.sli55hle9z79e7i5
14:50:50 UTC success: [kubesphere-master-1]
14:50:50 UTC [JoinNodesModule] Generate kubeadm config
14:50:50 UTC skipped: [kubesphere-master-1]
14:50:50 UTC success: [kubesphere-worker-1]
14:50:50 UTC [JoinNodesModule] Generate audit policy
14:50:50 UTC skipped: [kubesphere-master-1]
14:50:50 UTC [JoinNodesModule] Generate audit webhook
14:50:50 UTC skipped: [kubesphere-master-1]
14:50:50 UTC [JoinNodesModule] Join control-plane node
14:50:50 UTC skipped: [kubesphere-master-1]
14:50:50 UTC [JoinNodesModule] Join worker node
14:51:09 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 14:51:03.753982    2792 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0913 14:51:03.757035    2792 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
14:51:09 UTC skipped: [kubesphere-master-1]
14:51:09 UTC success: [kubesphere-worker-1]
14:51:09 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
14:51:09 UTC skipped: [kubesphere-master-1]
14:51:09 UTC [JoinNodesModule] Remove master taint
14:51:09 UTC skipped: [kubesphere-master-1]
14:51:09 UTC [JoinNodesModule] Add worker label to all nodes
14:51:09 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
14:51:09 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
14:51:09 UTC success: [kubesphere-master-1]
14:51:09 UTC [DeployNetworkPluginModule] Generate calico
14:51:09 UTC success: [kubesphere-master-1]
14:51:09 UTC [DeployNetworkPluginModule] Deploy calico
14:51:10 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [ConfigureKubernetesModule] Configure kubernetes
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [ChownModule] Chown user $HOME/.kube dir
14:51:10 UTC success: [kubesphere-worker-1]
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [AutoRenewCertsModule] Generate k8s certs renew script
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [AutoRenewCertsModule] Generate k8s certs renew service
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [AutoRenewCertsModule] Enable k8s certs renew service
14:51:10 UTC success: [kubesphere-master-1]
14:51:10 UTC [SaveKubeConfigModule] Save kube config as a configmap
14:51:10 UTC success: [LocalHost]
14:51:10 UTC [AddonsModule] Install addons
14:51:10 UTC success: [LocalHost]
14:51:10 UTC Pipeline[CreateClusterPipeline] execute successfully
14:52:35 UTC [GreetingsModule] Greetings
14:52:35 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
14:52:36 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
14:52:36 UTC success: [kubesphere-worker-1]
14:52:36 UTC success: [kubesphere-master-1]
14:52:36 UTC [NodePreCheckModule] A pre-check on nodes
14:52:36 UTC success: [kubesphere-master-1]
14:52:36 UTC success: [kubesphere-worker-1]
14:52:36 UTC [ConfirmModule] Display confirmation form
14:52:36 UTC success: [LocalHost]
14:52:36 UTC [NodeBinariesModule] Download installation binaries
14:52:36 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
14:52:36 UTC message: [localhost]
kubeadm is existed
14:52:36 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
14:52:37 UTC message: [localhost]
kubelet is existed
14:52:37 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
14:52:37 UTC message: [localhost]
kubectl is existed
14:52:37 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
14:52:37 UTC message: [localhost]
helm is existed
14:52:37 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
14:52:37 UTC message: [localhost]
kubecni is existed
14:52:37 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
14:52:37 UTC message: [localhost]
crictl is existed
14:52:37 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
14:52:37 UTC message: [localhost]
etcd is existed
14:52:37 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
14:52:37 UTC message: [localhost]
containerd is existed
14:52:37 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
14:52:37 UTC message: [localhost]
runc is existed
14:52:37 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
14:52:37 UTC message: [localhost]
calicoctl is existed
14:52:37 UTC success: [LocalHost]
14:52:37 UTC [ConfigureOSModule] Get OS release
14:52:37 UTC success: [kubesphere-worker-1]
14:52:37 UTC success: [kubesphere-master-1]
14:52:37 UTC [ConfigureOSModule] Prepare to init OS
14:52:37 UTC success: [kubesphere-worker-1]
14:52:37 UTC success: [kubesphere-master-1]
14:52:37 UTC [ConfigureOSModule] Generate init os script
14:52:37 UTC success: [kubesphere-worker-1]
14:52:37 UTC success: [kubesphere-master-1]
14:52:37 UTC [ConfigureOSModule] Exec init os script
14:52:38 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:52:38 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
14:52:38 UTC success: [kubesphere-master-1]
14:52:38 UTC success: [kubesphere-worker-1]
14:52:38 UTC [ConfigureOSModule] configure the ntp server for each node
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:52:38 UTC success: [kubesphere-master-1]
14:52:38 UTC [InstallContainerModule] Sync containerd binaries
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC [InstallContainerModule] Sync crictl binaries
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC [InstallContainerModule] Generate containerd service
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC [InstallContainerModule] Generate containerd config
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC [InstallContainerModule] Generate crictl config
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC [InstallContainerModule] Enable containerd
14:52:38 UTC skipped: [kubesphere-worker-1]
14:52:38 UTC skipped: [kubesphere-master-1]
14:52:38 UTC [PullModule] Start to pull images on all nodes
14:52:38 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
14:52:38 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
14:52:40 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
14:52:40 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
14:52:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
14:52:41 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
14:52:43 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
14:52:43 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:52:44 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
14:52:44 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
14:52:46 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
14:52:46 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
14:52:47 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
14:52:47 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
14:52:49 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:52:49 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
14:52:50 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
14:52:52 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
14:52:53 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
14:52:55 UTC success: [kubesphere-worker-1]
14:52:55 UTC success: [kubesphere-master-1]
14:52:55 UTC [ETCDPreCheckModule] Get etcd status
14:52:55 UTC stdout: [kubesphere-master-1]
ETCD_NAME=etcd-kubesphere-master-1
14:52:55 UTC success: [kubesphere-master-1]
14:52:55 UTC [CertsModule] Fetch etcd certs
14:52:55 UTC success: [kubesphere-master-1]
14:52:55 UTC [CertsModule] Generate etcd Certs
14:52:55 UTC success: [LocalHost]
14:52:55 UTC [CertsModule] Synchronize certs file
14:52:55 UTC success: [kubesphere-master-1]
14:52:55 UTC [CertsModule] Synchronize certs file to master
14:52:55 UTC skipped: [kubesphere-master-1]
14:52:55 UTC [InstallETCDBinaryModule] Install etcd using binary
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [InstallETCDBinaryModule] Generate etcd service
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [InstallETCDBinaryModule] Generate access address
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Health check on exist etcd
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
14:52:56 UTC skipped: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Join etcd member
14:52:56 UTC skipped: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Health check on new etcd
14:52:56 UTC skipped: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Check etcd member
14:52:56 UTC skipped: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDConfigureModule] Health check on all etcd
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDBackupModule] Backup etcd data regularly
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDBackupModule] Generate backup ETCD service
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDBackupModule] Generate backup ETCD timer
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [ETCDBackupModule] Enable backup etcd service
14:52:56 UTC success: [kubesphere-master-1]
14:52:56 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
14:53:00 UTC success: [kubesphere-master-1]
14:53:00 UTC success: [kubesphere-worker-1]
14:53:00 UTC [InstallKubeBinariesModule] Change kubelet mode
14:53:00 UTC success: [kubesphere-worker-1]
14:53:00 UTC success: [kubesphere-master-1]
14:53:00 UTC [InstallKubeBinariesModule] Generate kubelet service
14:53:00 UTC success: [kubesphere-master-1]
14:53:00 UTC success: [kubesphere-worker-1]
14:53:00 UTC [InstallKubeBinariesModule] Enable kubelet service
14:53:01 UTC success: [kubesphere-master-1]
14:53:01 UTC success: [kubesphere-worker-1]
14:53:01 UTC [InstallKubeBinariesModule] Generate kubelet env
14:53:01 UTC success: [kubesphere-worker-1]
14:53:01 UTC success: [kubesphere-master-1]
14:53:01 UTC [InitKubernetesModule] Generate kubeadm config
14:53:01 UTC success: [kubesphere-master-1]
14:53:01 UTC [InitKubernetesModule] Generate audit policy
14:53:01 UTC skipped: [kubesphere-master-1]
14:53:01 UTC [InitKubernetesModule] Generate audit webhook
14:53:01 UTC skipped: [kubesphere-master-1]
14:53:01 UTC [InitKubernetesModule] Init cluster using kubeadm
14:53:16 UTC stdout: [kubesphere-master-1]
W0913 14:53:01.150232    8587 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 9.503816 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: 0nlk31.qbefmdkn935clrhf
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token 0nlk31.qbefmdkn935clrhf \
	--discovery-token-ca-cert-hash sha256:2704154c1ce8ae85bfed2420762b9cd781811db9b4c8b0fde7a6536314a3f81a \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token 0nlk31.qbefmdkn935clrhf \
	--discovery-token-ca-cert-hash sha256:2704154c1ce8ae85bfed2420762b9cd781811db9b4c8b0fde7a6536314a3f81a
14:53:16 UTC success: [kubesphere-master-1]
14:53:16 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
14:53:16 UTC success: [kubesphere-master-1]
14:53:16 UTC [InitKubernetesModule] Remove master taint
14:53:16 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
14:53:16 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
14:53:16 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
14:53:16 UTC success: [kubesphere-master-1]
14:53:16 UTC [ClusterDNSModule] Generate coredns service
14:53:16 UTC skipped: [kubesphere-master-1]
14:53:16 UTC [ClusterDNSModule] Override coredns service
14:53:16 UTC skipped: [kubesphere-master-1]
14:53:16 UTC [ClusterDNSModule] Generate nodelocaldns
14:53:16 UTC success: [kubesphere-master-1]
14:53:16 UTC [ClusterDNSModule] Deploy nodelocaldns
14:53:17 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns unchanged
daemonset.apps/nodelocaldns unchanged
14:53:17 UTC success: [kubesphere-master-1]
14:53:17 UTC [ClusterDNSModule] Generate nodelocaldns configmap
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [ClusterDNSModule] Apply nodelocaldns configmap
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [KubernetesStatusModule] Get kubernetes cluster status
14:53:17 UTC stdout: [kubesphere-master-1]
v1.23.10
14:53:17 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
kubesphere-worker-1   v1.23.10   [map[address:192.168.122.181 type:InternalIP] map[address:kubesphere-worker-1 type:Hostname]]
14:53:17 UTC stdout: [kubesphere-master-1]
W0913 14:53:17.496452    9539 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
7e555aacdbcf96da053fa98a343cac15097a9e1cad6ffd743936b2deb87a5b72
14:53:17 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:53:17 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:53:17 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
14:53:17 UTC stdout: [kubesphere-master-1]
ld52fy.dfbdd1g684m5cbjf
14:53:17 UTC success: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Generate kubeadm config
14:53:17 UTC skipped: [kubesphere-worker-1]
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Generate audit policy
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Generate audit webhook
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Join control-plane node
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Join worker node
14:53:17 UTC skipped: [kubesphere-worker-1]
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Remove master taint
14:53:17 UTC skipped: [kubesphere-master-1]
14:53:17 UTC [JoinNodesModule] Add worker label to all nodes
14:53:17 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 not labeled
14:53:17 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 not labeled
14:53:17 UTC success: [kubesphere-master-1]
14:53:17 UTC [DeployNetworkPluginModule] Generate calico
14:53:17 UTC success: [kubesphere-master-1]
14:53:17 UTC [DeployNetworkPluginModule] Deploy calico
14:53:18 UTC stdout: [kubesphere-master-1]
configmap/calico-config unchanged
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org configured
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org configured
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged
clusterrole.rbac.authorization.k8s.io/calico-node unchanged
clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged
daemonset.apps/calico-node configured
serviceaccount/calico-node unchanged
deployment.apps/calico-kube-controllers unchanged
serviceaccount/calico-kube-controllers unchanged
poddisruptionbudget.policy/calico-kube-controllers configured
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [ConfigureKubernetesModule] Configure kubernetes
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [ChownModule] Chown user $HOME/.kube dir
14:53:18 UTC success: [kubesphere-worker-1]
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [AutoRenewCertsModule] Generate k8s certs renew script
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [AutoRenewCertsModule] Generate k8s certs renew service
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [AutoRenewCertsModule] Enable k8s certs renew service
14:53:18 UTC success: [kubesphere-master-1]
14:53:18 UTC [SaveKubeConfigModule] Save kube config as a configmap
14:53:18 UTC success: [LocalHost]
14:53:18 UTC [AddonsModule] Install addons
14:53:18 UTC success: [LocalHost]
14:53:18 UTC Pipeline[CreateClusterPipeline] execute successfully
15:02:28 UTC [GreetingsModule] Greetings
15:02:28 UTC message: [kubesphere-worker-1]
Greetings, KubeKey!
15:02:29 UTC message: [kubesphere-master-1]
Greetings, KubeKey!
15:02:29 UTC success: [kubesphere-worker-1]
15:02:29 UTC success: [kubesphere-master-1]
15:02:29 UTC [NodePreCheckModule] A pre-check on nodes
15:02:29 UTC success: [kubesphere-master-1]
15:02:29 UTC success: [kubesphere-worker-1]
15:02:29 UTC [ConfirmModule] Display confirmation form
15:02:29 UTC success: [LocalHost]
15:02:29 UTC [NodeBinariesModule] Download installation binaries
15:02:29 UTC message: [localhost]
downloading amd64 kubeadm v1.23.10 ...
15:02:29 UTC message: [localhost]
kubeadm is existed
15:02:29 UTC message: [localhost]
downloading amd64 kubelet v1.23.10 ...
15:02:29 UTC message: [localhost]
kubelet is existed
15:02:29 UTC message: [localhost]
downloading amd64 kubectl v1.23.10 ...
15:02:29 UTC message: [localhost]
kubectl is existed
15:02:29 UTC message: [localhost]
downloading amd64 helm v3.9.0 ...
15:02:29 UTC message: [localhost]
helm is existed
15:02:29 UTC message: [localhost]
downloading amd64 kubecni v1.2.0 ...
15:02:29 UTC message: [localhost]
kubecni is existed
15:02:29 UTC message: [localhost]
downloading amd64 crictl v1.24.0 ...
15:02:29 UTC message: [localhost]
crictl is existed
15:02:29 UTC message: [localhost]
downloading amd64 etcd v3.4.13 ...
15:02:29 UTC message: [localhost]
etcd is existed
15:02:29 UTC message: [localhost]
downloading amd64 containerd 1.6.4 ...
15:02:30 UTC message: [localhost]
containerd is existed
15:02:30 UTC message: [localhost]
downloading amd64 runc v1.1.1 ...
15:02:30 UTC message: [localhost]
runc is existed
15:02:30 UTC message: [localhost]
downloading amd64 calicoctl v3.23.2 ...
15:02:30 UTC message: [localhost]
calicoctl is existed
15:02:30 UTC success: [LocalHost]
15:02:30 UTC [ConfigureOSModule] Get OS release
15:02:30 UTC success: [kubesphere-master-1]
15:02:30 UTC success: [kubesphere-worker-1]
15:02:30 UTC [ConfigureOSModule] Prepare to init OS
15:02:30 UTC success: [kubesphere-worker-1]
15:02:30 UTC success: [kubesphere-master-1]
15:02:30 UTC [ConfigureOSModule] Generate init os script
15:02:30 UTC success: [kubesphere-master-1]
15:02:30 UTC success: [kubesphere-worker-1]
15:02:30 UTC [ConfigureOSModule] Exec init os script
15:02:31 UTC stdout: [kubesphere-master-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
15:02:31 UTC stdout: [kubesphere-worker-1]
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.rp_filter = 1
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-arptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_local_reserved_ports = 30000-32767
net.core.netdev_max_backlog = 65535
net.core.rmem_max = 33554432
net.core.wmem_max = 33554432
net.core.somaxconn = 32768
net.ipv4.tcp_max_syn_backlog = 1048576
net.ipv4.neigh.default.gc_thresh1 = 512
net.ipv4.neigh.default.gc_thresh2 = 2048
net.ipv4.neigh.default.gc_thresh3 = 4096
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_max_tw_buckets = 1048576
net.ipv4.tcp_max_orphans = 65535
net.ipv4.udp_rmem_min = 131072
net.ipv4.udp_wmem_min = 131072
net.ipv4.conf.all.arp_accept = 1
net.ipv4.conf.default.arp_accept = 1
net.ipv4.conf.all.arp_ignore = 1
net.ipv4.conf.default.arp_ignore = 1
vm.max_map_count = 262144
vm.swappiness = 0
vm.overcommit_memory = 0
fs.inotify.max_user_instances = 524288
fs.inotify.max_user_watches = 524288
fs.pipe-max-size = 4194304
fs.aio-max-nr = 262144
kernel.pid_max = 65535
kernel.watchdog_thresh = 5
kernel.hung_task_timeout_secs = 5
15:02:31 UTC success: [kubesphere-master-1]
15:02:31 UTC success: [kubesphere-worker-1]
15:02:31 UTC [ConfigureOSModule] configure the ntp server for each node
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC [KubernetesStatusModule] Get kubernetes cluster status
15:02:31 UTC success: [kubesphere-master-1]
15:02:31 UTC [InstallContainerModule] Sync containerd binaries
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC [InstallContainerModule] Sync crictl binaries
15:02:31 UTC success: [kubesphere-master-1]
15:02:31 UTC success: [kubesphere-worker-1]
15:02:31 UTC [InstallContainerModule] Generate containerd service
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC [InstallContainerModule] Generate containerd config
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC [InstallContainerModule] Generate crictl config
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC [InstallContainerModule] Enable containerd
15:02:31 UTC skipped: [kubesphere-master-1]
15:02:31 UTC skipped: [kubesphere-worker-1]
15:02:31 UTC [PullModule] Start to pull images on all nodes
15:02:31 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/pause:3.6
15:02:31 UTC message: [kubesphere-master-1]
downloading image: kubesphere/pause:3.6
15:02:33 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/kube-proxy:v1.23.10
15:02:33 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-apiserver:v1.23.10
15:02:34 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-controller-manager:v1.23.10
15:02:35 UTC message: [kubesphere-worker-1]
downloading image: coredns/coredns:1.8.6
15:02:36 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-scheduler:v1.23.10
15:02:36 UTC message: [kubesphere-worker-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
15:02:38 UTC message: [kubesphere-master-1]
downloading image: kubesphere/kube-proxy:v1.23.10
15:02:38 UTC message: [kubesphere-worker-1]
downloading image: calico/kube-controllers:v3.23.2
15:02:39 UTC message: [kubesphere-master-1]
downloading image: coredns/coredns:1.8.6
15:02:39 UTC message: [kubesphere-worker-1]
downloading image: calico/cni:v3.23.2
15:02:41 UTC message: [kubesphere-master-1]
downloading image: kubesphere/k8s-dns-node-cache:1.15.12
15:02:41 UTC message: [kubesphere-worker-1]
downloading image: calico/node:v3.23.2
15:02:42 UTC message: [kubesphere-master-1]
downloading image: calico/kube-controllers:v3.23.2
15:02:42 UTC message: [kubesphere-worker-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
15:02:44 UTC message: [kubesphere-master-1]
downloading image: calico/cni:v3.23.2
15:02:45 UTC message: [kubesphere-master-1]
downloading image: calico/node:v3.23.2
15:02:47 UTC message: [kubesphere-master-1]
downloading image: calico/pod2daemon-flexvol:v3.23.2
15:02:48 UTC success: [kubesphere-worker-1]
15:02:48 UTC success: [kubesphere-master-1]
15:02:48 UTC [ETCDPreCheckModule] Get etcd status
15:02:48 UTC success: [kubesphere-master-1]
15:02:48 UTC [CertsModule] Fetch etcd certs
15:02:48 UTC success: [kubesphere-master-1]
15:02:48 UTC [CertsModule] Generate etcd Certs
15:02:48 UTC success: [LocalHost]
15:02:48 UTC [CertsModule] Synchronize certs file
15:02:48 UTC success: [kubesphere-master-1]
15:02:48 UTC [CertsModule] Synchronize certs file to master
15:02:48 UTC skipped: [kubesphere-master-1]
15:02:48 UTC [InstallETCDBinaryModule] Install etcd using binary
15:02:49 UTC success: [kubesphere-master-1]
15:02:49 UTC [InstallETCDBinaryModule] Generate etcd service
15:02:49 UTC success: [kubesphere-master-1]
15:02:49 UTC [InstallETCDBinaryModule] Generate access address
15:02:49 UTC success: [kubesphere-master-1]
15:02:49 UTC [ETCDConfigureModule] Health check on exist etcd
15:02:49 UTC skipped: [kubesphere-master-1]
15:02:49 UTC [ETCDConfigureModule] Generate etcd.env config on new etcd
15:02:49 UTC success: [kubesphere-master-1]
15:02:49 UTC [ETCDConfigureModule] Refresh etcd.env config on all etcd
15:02:49 UTC success: [kubesphere-master-1]
15:02:49 UTC [ETCDConfigureModule] Restart etcd
15:02:53 UTC stdout: [kubesphere-master-1]
Created symlink /etc/systemd/system/multi-user.target.wants/etcd.service → /etc/systemd/system/etcd.service.
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDConfigureModule] Health check on all etcd
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDConfigureModule] Refresh etcd.env config to exist mode on all etcd
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDConfigureModule] Health check on all etcd
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDBackupModule] Backup etcd data regularly
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDBackupModule] Generate backup ETCD service
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDBackupModule] Generate backup ETCD timer
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [ETCDBackupModule] Enable backup etcd service
15:02:53 UTC success: [kubesphere-master-1]
15:02:53 UTC [InstallKubeBinariesModule] Synchronize kubernetes binaries
15:02:57 UTC success: [kubesphere-worker-1]
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC [InstallKubeBinariesModule] Change kubelet mode
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC success: [kubesphere-worker-1]
15:02:57 UTC [InstallKubeBinariesModule] Generate kubelet service
15:02:57 UTC success: [kubesphere-worker-1]
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC [InstallKubeBinariesModule] Enable kubelet service
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC success: [kubesphere-worker-1]
15:02:57 UTC [InstallKubeBinariesModule] Generate kubelet env
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC success: [kubesphere-worker-1]
15:02:57 UTC [InitKubernetesModule] Generate kubeadm config
15:02:57 UTC success: [kubesphere-master-1]
15:02:57 UTC [InitKubernetesModule] Generate audit policy
15:02:57 UTC skipped: [kubesphere-master-1]
15:02:57 UTC [InitKubernetesModule] Generate audit webhook
15:02:57 UTC skipped: [kubesphere-master-1]
15:02:57 UTC [InitKubernetesModule] Init cluster using kubeadm
15:03:12 UTC stdout: [kubesphere-master-1]
W0913 15:02:57.977396    3626 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[init] Using Kubernetes version: v1.23.10
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local kubesphere-master-1 kubesphere-master-1.cluster.local kubesphere-worker-1 kubesphere-worker-1.cluster.local lb.kubesphere.local localhost] and IPs [10.233.0.1 192.168.122.151 127.0.0.1 192.168.122.181]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] External etcd mode: Skipping etcd/ca certificate authority generation
[certs] External etcd mode: Skipping etcd/server certificate generation
[certs] External etcd mode: Skipping etcd/peer certificate generation
[certs] External etcd mode: Skipping etcd/healthcheck-client certificate generation
[certs] External etcd mode: Skipping apiserver-etcd-client certificate generation
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 11.006063 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config-1.23" in namespace kube-system with the configuration for the kubelets in the cluster
NOTE: The "kubelet-config-1.23" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just "kubelet-config". Kubeadm upgrade will handle this transition transparently.
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node kubesphere-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
[bootstrap-token] Using token: chsydu.zpknayjponjrzq4e
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join lb.kubesphere.local:6443 --token chsydu.zpknayjponjrzq4e \
	--discovery-token-ca-cert-hash sha256:ab1ffb11003339683754d50a5a55997713920350323d93e8525affaa7757a182 \
	--control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join lb.kubesphere.local:6443 --token chsydu.zpknayjponjrzq4e \
	--discovery-token-ca-cert-hash sha256:ab1ffb11003339683754d50a5a55997713920350323d93e8525affaa7757a182
15:03:12 UTC success: [kubesphere-master-1]
15:03:12 UTC [InitKubernetesModule] Copy admin.conf to ~/.kube/config
15:03:12 UTC success: [kubesphere-master-1]
15:03:12 UTC [InitKubernetesModule] Remove master taint
15:03:14 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 untainted
15:03:14 UTC stdout: [kubesphere-master-1]
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found
15:03:14 UTC [WARN] Failed to exec command: sudo -E /bin/bash -c "/usr/local/bin/kubectl taint nodes kubesphere-master-1 node-role.kubernetes.io/control-plane=:NoSchedule-" 
error: taint "node-role.kubernetes.io/control-plane:NoSchedule" not found: Process exited with status 1
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Generate coredns service
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Override coredns service
15:03:14 UTC stdout: [kubesphere-master-1]
service "kube-dns" deleted
15:03:14 UTC stdout: [kubesphere-master-1]
service/coredns created
Warning: resource clusterroles/system:coredns is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
clusterrole.rbac.authorization.k8s.io/system:coredns configured
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Generate nodelocaldns
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Deploy nodelocaldns
15:03:14 UTC stdout: [kubesphere-master-1]
serviceaccount/nodelocaldns created
daemonset.apps/nodelocaldns created
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Generate nodelocaldns configmap
15:03:14 UTC success: [kubesphere-master-1]
15:03:14 UTC [ClusterDNSModule] Apply nodelocaldns configmap
15:03:15 UTC stdout: [kubesphere-master-1]
configmap/nodelocaldns created
15:03:15 UTC success: [kubesphere-master-1]
15:03:15 UTC [KubernetesStatusModule] Get kubernetes cluster status
15:03:15 UTC stdout: [kubesphere-master-1]
v1.23.10
15:03:15 UTC stdout: [kubesphere-master-1]
kubesphere-master-1   v1.23.10   [map[address:192.168.122.151 type:InternalIP] map[address:kubesphere-master-1 type:Hostname]]
15:03:15 UTC stdout: [kubesphere-master-1]
W0913 15:03:15.117570    4303 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
94d473f70bd871f92a5503d11ec884e820e4b80dcbb45f2a9c5e11cfc514b871
15:03:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
15:03:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
15:03:15 UTC stdout: [kubesphere-master-1]
secret/kubeadm-certs patched
15:03:15 UTC stdout: [kubesphere-master-1]
fz7bg7.vxtr4nmd1ovg51ot
15:03:15 UTC success: [kubesphere-master-1]
15:03:15 UTC [JoinNodesModule] Generate kubeadm config
15:03:15 UTC skipped: [kubesphere-master-1]
15:03:15 UTC success: [kubesphere-worker-1]
15:03:15 UTC [JoinNodesModule] Generate audit policy
15:03:15 UTC skipped: [kubesphere-master-1]
15:03:15 UTC [JoinNodesModule] Generate audit webhook
15:03:15 UTC skipped: [kubesphere-master-1]
15:03:15 UTC [JoinNodesModule] Join control-plane node
15:03:15 UTC skipped: [kubesphere-master-1]
15:03:15 UTC [JoinNodesModule] Join worker node
15:03:33 UTC stdout: [kubesphere-worker-1]
[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
W0913 15:03:28.297212    3174 utils.go:69] The recommended value for "clusterDNS" in "KubeletConfiguration" is: [10.233.0.10]; the provided value is: [169.254.25.10]
W0913 15:03:28.299835    3174 utils.go:69] The recommended value for "resolvConf" in "KubeletConfiguration" is: /run/systemd/resolve/resolv.conf; the provided value is: /run/systemd/resolve/resolv.conf
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
15:03:33 UTC skipped: [kubesphere-master-1]
15:03:33 UTC success: [kubesphere-worker-1]
15:03:33 UTC [JoinNodesModule] Copy admin.conf to ~/.kube/config
15:03:33 UTC skipped: [kubesphere-master-1]
15:03:33 UTC [JoinNodesModule] Remove master taint
15:03:33 UTC skipped: [kubesphere-master-1]
15:03:33 UTC [JoinNodesModule] Add worker label to all nodes
15:03:34 UTC stdout: [kubesphere-master-1]
node/kubesphere-master-1 labeled
15:03:34 UTC stdout: [kubesphere-master-1]
node/kubesphere-worker-1 labeled
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [DeployNetworkPluginModule] Generate calico
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [DeployNetworkPluginModule] Deploy calico
15:03:34 UTC stdout: [kubesphere-master-1]
configmap/calico-config created
customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/caliconodestatuses.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/ipreservations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/kubecontrollersconfigurations.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created
customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created
clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created
clusterrole.rbac.authorization.k8s.io/calico-node created
clusterrolebinding.rbac.authorization.k8s.io/calico-node created
daemonset.apps/calico-node created
serviceaccount/calico-node created
deployment.apps/calico-kube-controllers created
serviceaccount/calico-kube-controllers created
poddisruptionbudget.policy/calico-kube-controllers created
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [ConfigureKubernetesModule] Configure kubernetes
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [ChownModule] Chown user $HOME/.kube dir
15:03:34 UTC success: [kubesphere-worker-1]
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [AutoRenewCertsModule] Generate k8s certs renew script
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [AutoRenewCertsModule] Generate k8s certs renew service
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [AutoRenewCertsModule] Generate k8s certs renew timer
15:03:34 UTC success: [kubesphere-master-1]
15:03:34 UTC [AutoRenewCertsModule] Enable k8s certs renew service
15:03:35 UTC success: [kubesphere-master-1]
15:03:35 UTC [SaveKubeConfigModule] Save kube config as a configmap
15:03:35 UTC success: [LocalHost]
15:03:35 UTC [AddonsModule] Install addons
15:03:35 UTC success: [LocalHost]
15:03:35 UTC Pipeline[CreateClusterPipeline] execute successfully
